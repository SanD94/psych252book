[
["modeling-data.html", "Chapter 9 Modeling data 9.1 Load packages and set plotting theme 9.2 Modeling data 9.3 Hypothesis testing: “One-sample t-test” 9.4 Building a sampling distribution of PRE 9.5 Additional resources 9.6 Session info", " Chapter 9 Modeling data 9.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set(theme_classic() + #set the theme theme(text = element_text(size = 20))) #set the default text size 9.2 Modeling data 9.2.1 Simplicity vs. accuracy trade-off # make example reproducible set.seed(1) n_samples = 20 # sample size n_parameters = 2 # number of parameters in the polynomial regression # generate data df.data = tibble(x = runif(n_samples, min = 0, max = 10), y = 10 + 3 * x + 3 * x^2 + rnorm(n_samples, sd = 20)) # plot a fit to the data ggplot(data = df.data, mapping = aes(x = x, y = y)) + geom_point(size = 3) + # geom_hline(yintercept = mean(df.data$y), color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, degree = n_parameters, raw = TRUE)) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) Figure 9.1: Tradeoff between fit and model simplicity. # make example reproducible set.seed(1) # n_samples = 20 n_samples = 3 df.pre = tibble(x = runif(n_samples, min = 0, max = 10), y = 2 * x + rnorm(n_samples, sd = 1)) # plot a fit to the data ggplot(data = df.pre, mapping = aes(x = x, y = y)) + geom_point(size = 3) + # geom_hline(yintercept = mean(df.pre$y), color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, 1, raw = TRUE)) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) Figure 9.2: Figure that I used to illustrate that fitting more data points with fewer parameter is more impressive. 9.2.2 Error definitions and best estimators Let’s start with some simple data: df.data = tibble(observation = 1:5, value = c(1, 3, 5, 9, 14)) And plot the data ggplot(data = df.data, mapping = aes(x = &quot;1&quot;, y = value)) + geom_point(size = 3) + scale_y_continuous(breaks = seq(0, 16, 2), limits = c(0, 16)) + theme(panel.grid.major.y = element_line(color = &quot;gray80&quot;, linetype = 2), axis.title.x = element_blank(), axis.text.x = element_blank(), text = element_text(size = 24)) This is what the sum of absolute errors looks like for a given value_predicted. value_predicted = 7 df.data = df.data %&gt;% mutate(prediction = value_predicted, error_absolute = abs(prediction - value)) ggplot(data = df.data, mapping = aes(x = observation, y = value)) + geom_segment(mapping = aes(x = observation, xend = observation, y = value_predicted, yend = value ), color = &quot;blue&quot;, size = 1) + geom_line(data = tibble(x = c(1, 5), y = value_predicted), mapping = aes(x = x, y = y), size = 1, color = &quot;green&quot;) + geom_point(size = 4) + annotate(x = 1, y = 15.5, geom = &quot;text&quot;, label = str_c(&quot;Prediction = &quot;, value_predicted), size = 8, hjust = 0, vjust = 1, color = &quot;green&quot;) + annotate(x = 1, y = 13.5, geom = &quot;text&quot;, label = str_c(&quot;Sum of absolute errors = &quot;, sum(df.data$error_absolute)), size = 8, hjust = 0, vjust = 1, color = &quot;blue&quot;) + annotate(x = 5, y = value_predicted, geom = &quot;text&quot;, label = parse(text = str_c(&quot;{hat(Y)&quot;,&quot;==b[0]}==&quot;, value_predicted)), hjust = -0.1, size = 8) + scale_x_continuous(breaks = df.data$observation, labels = parse(text = str_c(&#39;e[&#39;,df.data$observation,&#39;]&#39;, &quot;==&quot;, df.data$error_absolute)), limits = c(1, 6)) + scale_y_continuous(breaks = seq(0, 16, 2), limits = c(0, 16)) + theme(panel.grid.major.y = element_line(color = &quot;gray80&quot;, linetype = 2), axis.title.x = element_blank(), text = element_text(size = 24)) Figure 9.3: Sum of absolute errors. Play around with the code below to see how using (1) the sum of absolute errors, or (2) the sum of squared errors affects what estimate minimizes the error. value_predicted = seq(0, 50, 0.1) # value_predicted = seq(0, 10, 1) df.data = tibble(observation = 1:5, value = c(1, 3, 5, 9, 140)) # function that calculates the sum absolute error fun.sum_absolute_error = function(prediction){ x = df.data$value sum_absolute_error = sum(abs(x-prediction)) return(sum_absolute_error) } # function that calculates the sum squared error fun.sum_squared_error = function(prediction){ x = df.data$value sum_squared_error = sum((x-prediction)^2) return(sum_squared_error) } df.model = tibble( estimate = value_predicted, sum_absolute_error = map_dbl(value_predicted, fun.sum_absolute_error), sum_squared_error = map_dbl(value_predicted, fun.sum_squared_error) ) ggplot(data = df.model, mapping = aes(x = estimate, # y = sum_absolute_error)) + y = sum_squared_error)) + geom_line(size = 1) + # labs(y = &quot;Sum absolute error&quot;) labs(y = &quot;Sum of squared errors&quot;) Error definition Best estimator Count of errors Mode = most frequent value Sum of absolute errors Median = middle observation of all values Sum of squared errors Mean = average of all values mu = 0 sigma = 1 mean = mu median = mu mode = mu ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1) + geom_segment(aes(x = median, xend = median, y = dnorm(median), yend = 0), color = &quot;green&quot;, size = 2) + geom_segment(aes(x = mode, xend = mode, y = dnorm(mode), yend = 0), color = &quot;red&quot;, size = 2) + geom_segment(aes(x = mean, xend = mean, y = dnorm(mean), yend = 0), color = &quot;blue&quot;, size = 2) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.4: Mean, median, and mode on the normal distribution. rate = 1 mean = rate median = rate * log(2) mode = 0 ggplot(data = tibble(x = c(-0.1, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dexp&quot;, size = 1) + geom_segment(aes(x = median, xend = median, y = dexp(median), yend = 0), color = &quot;green&quot;, size = 2) + geom_segment(aes(x = mode, xend = mode, y = dexp(mode), yend = 0), color = &quot;red&quot;, size = 2) + geom_segment(aes(x = mean, xend = mean, y = dexp(mean), yend = 0), color = &quot;blue&quot;, size = 2) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = 0:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.5: Mean, median, and mode on the exponential distribution. 9.2.3 Sampling distributions for median and mean # make example reproducible set.seed(1) sample_size = 40 # size of each sample sample_n = 1000 # number of samples # draw sample fun.draw_sample = function(sample_size, distribution){ x = 50 + rnorm(sample_size) return(x) } # generate many samples samples = replicate(n = sample_n, fun.draw_sample(sample_size, df.population)) # set up a data frame with samples df.sampling_distribution = matrix(samples, ncol = sample_n) %&gt;% as_tibble(.name_repair = ~ str_c(1:sample_n)) %&gt;% pivot_longer(cols = everything(), names_to = &quot;sample&quot;, values_to = &quot;number&quot;) %&gt;% mutate(sample = as.numeric(sample)) %&gt;% group_by(sample) %&gt;% mutate(draw = 1:n()) %&gt;% select(sample, draw, number) %&gt;% ungroup() # turn the data frame into long format and calculate the mean and median of each sample df.sampling_distribution_summaries = df.sampling_distribution %&gt;% group_by(sample) %&gt;% summarize(mean = mean(number), median = median(number)) %&gt;% ungroup() %&gt;% pivot_longer(cols = -sample, names_to = &quot;index&quot;, values_to = &quot;value&quot;) And plot it: # plot a histogram of the means with density overlaid ggplot(data = df.sampling_distribution_summaries, mapping = aes(x = value, color = index)) + stat_density(bw = 0.1, size = 2, geom = &quot;line&quot;) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.01))) 9.3 Hypothesis testing: “One-sample t-test” df.internet = read_table2(file = &quot;data/internet_access.txt&quot;) %&gt;% clean_names() ## Parsed with column specification: ## cols( ## State = col_character(), ## Internet = col_double(), ## College = col_double(), ## Auto = col_double(), ## Density = col_double() ## ) df.internet %&gt;% mutate(i = 1:n()) %&gt;% select(i, internet, everything()) %&gt;% head(10) %&gt;% kable(digits = 1) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) i internet state college auto density 1 79.0 AK 28.0 1.2 1.2 2 63.5 AL 23.5 1.3 94.4 3 60.9 AR 20.6 1.7 56.0 4 73.9 AZ 27.4 1.3 56.3 5 77.9 CA 31.0 0.8 239.1 6 79.4 CO 37.8 1.0 48.5 7 77.5 CT 37.2 1.0 738.1 8 74.5 DE 29.8 1.1 460.8 9 74.3 FL 27.2 1.2 350.6 10 72.2 GA 28.3 1.1 168.4 # parameters per model pa = 1 pc = 0 df.model = df.internet %&gt;% select(internet, state) %&gt;% mutate(i = 1:n(), compact_b = 75, augmented_b = mean(internet), compact_se = (internet-compact_b)^2, augmented_se = (internet-augmented_b)^2) %&gt;% select(i, state, internet, contains(&quot;compact&quot;), contains(&quot;augmented&quot;)) df.model %&gt;% summarize(augmented_sse = sum(augmented_se), compact_sse = sum(compact_se), pre = 1 - augmented_sse/compact_sse, f = (pre/(pa-pc))/((1-pre)/(nrow(df.model)-pa)), p_value = 1-pf(f, pa-pc, nrow(df.model)-1), mean = mean(internet), sd = sd(internet)) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) augmented_sse compact_sse pre f p_value mean sd 1355.028 1595.71 0.1508305 8.703441 0.0048592 72.806 5.258673 df1 = 1 df2 = 49 ggplot(data = tibble(x = c(0, 10)), mapping = aes(x = x)) + stat_function(fun = &quot;df&quot;, geom = &quot;area&quot;, fill = &quot;red&quot;, alpha = 0.5, args = list(df1 = df1, df2 = df2), size = 1, xlim = c(qf(0.95, df1 = df1, df2 = df2), 10)) + stat_function(fun = ~ df(x = ., df1 = df1, df2 = df2), size = 0.5) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) + labs(y = &quot;Density&quot;, x = &quot;Proportional reduction in error&quot;) Figure 9.6: F-distribution We’ve implemented a one sample t-test (compare the p-value here to the one I computed above using PRE and the F statistic). t.test(df.internet$internet, mu = 75) ## ## One Sample t-test ## ## data: df.internet$internet ## t = -2.9502, df = 49, p-value = 0.004859 ## alternative hypothesis: true mean is not equal to 75 ## 95 percent confidence interval: ## 71.3115 74.3005 ## sample estimates: ## mean of x ## 72.806 9.4 Building a sampling distribution of PRE Here is the general procedure for building a sampling distribution of the proportional reduction in error (PRE). In this instance, I compare the following two models Model C (compact): \\(Y_i = 75 + \\epsilon_i\\) Model A (augmented): \\(Y_i = \\overline Y + \\epsilon_i\\) whereby I assume that \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)\\). For this example, I assume that I know the population distribution. I first draw a sample from that distribution, and then calculate PRE. # make example reproducible set.seed(1) # set the sample size sample_size = 50 # draw sample from the population distribution (I&#39;ve fixed sigma -- the standard deviation # of the population distribution to be 5) df.sample = tibble(observation = 1:sample_size, value = 75 + rnorm(sample_size, mean = 0, sd = 5)) # calculate SSE for each model, and then PRE based on that df.summary = df.sample %&gt;% mutate(compact = 75, augmented = mean(value)) %&gt;% summarize(sse_compact = sum((value - compact)^2), sse_augmented = sum((value - augmented)^2), pre = 1 - (sse_augmented/sse_compact)) To generate the sampling distribution, I assume that the null hypothesis is true, and then take a look at what values for PRE we could expect by chance for our given sample size. # simulation parameters n_samples = 1000 sample_size = 50 mu = 75 # true mean of the distribution sigma = 5 # true standard deviation of the errors # function to draw samples from the population distribution fun.draw_sample = function(sample_size, mu, sigma){ sample = mu + rnorm(sample_size, mean = 0, sd = sigma) return(sample) } # draw samples samples = n_samples %&gt;% replicate(fun.draw_sample(sample_size, mu, sigma)) %&gt;% t() # transpose the resulting matrix (i.e. flip rows and columns) # put samples in data frame and compute PRE df.samples = samples %&gt;% as_tibble(.name_repair = ~ 1:ncol(samples)) %&gt;% mutate(sample = 1:n()) %&gt;% pivot_longer(cols = -sample, names_to = &quot;index&quot;, values_to = &quot;value&quot;) %&gt;% mutate(compact = mu) %&gt;% group_by(sample) %&gt;% mutate(augmented = mean(value)) %&gt;% summarize(sse_compact = sum((value - compact)^2), sse_augmented = sum((value - augmented)^2), pre = 1 - sse_augmented/sse_compact) # plot the sampling distribution for PRE ggplot(data = df.samples, mapping = aes(x = pre)) + stat_density(geom = &quot;line&quot;) + labs(x = &quot;Proportional reduction in error&quot;) # calculate the p-value for our sample df.samples %&gt;% summarize(p_value = sum(pre &gt;= df.summary$pre)/n()) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.394 Some code I wrote to show a subset of the samples. samples %&gt;% as_tibble(.name_repair = &quot;unique&quot;) %&gt;% mutate(sample = 1:n()) %&gt;% pivot_longer(cols = -sample, names_to = &quot;index&quot;, values_to = &quot;value&quot;) %&gt;% mutate(compact = mu) %&gt;% group_by(sample) %&gt;% mutate(augmented = mean(value)) %&gt;% ungroup() %&gt;% mutate(index = str_extract(index, pattern = &quot;\\\\-*\\\\d+\\\\.*\\\\d*&quot;), index = as.numeric(index)) %&gt;% filter(index &lt; 6) %&gt;% arrange(sample, index) %&gt;% head(15) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sample index value compact augmented 1 1 76.99 75 75.59 1 2 71.94 75 75.59 1 3 76.71 75 75.59 1 4 69.35 75 75.59 1 5 82.17 75 75.59 2 1 71.90 75 74.24 2 2 75.21 75 74.24 2 3 70.45 75 74.24 2 4 75.79 75 74.24 2 5 71.73 75 74.24 3 1 77.25 75 75.38 3 2 74.91 75 75.38 3 3 73.41 75 75.38 3 4 70.35 75 75.38 3 5 67.56 75 75.38 9.5 Additional resources 9.5.1 Reading Judd, C. M., McClelland, G. H., &amp; Ryan, C. S. (2011). Data analysis: A model comparison approach. Routledge. –&gt; Chapters 1–4 9.5.2 Datacamp Foundations of Inference 9.6 Session info ## R version 3.6.2 (2019-12-12) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Mojave 10.14.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.3 ## [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.3.0 janitor_1.2.0 kableExtra_1.1.0 knitr_1.26 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.11 haven_2.2.0 lattice_0.20-38 ## [5] snakecase_0.11.0 colorspace_1.4-1 vctrs_0.2.1 generics_0.0.2 ## [9] htmltools_0.4.0 viridisLite_0.3.0 yaml_2.2.0 utf8_1.1.4 ## [13] rlang_0.4.2 pillar_1.4.3 withr_2.1.2 glue_1.3.1 ## [17] DBI_1.1.0 dbplyr_1.4.2 readxl_1.3.1 modelr_0.1.5 ## [21] lifecycle_0.1.0 cellranger_1.1.0 munsell_0.5.0 gtable_0.3.0 ## [25] rvest_0.3.5 evaluate_0.14 labeling_0.3 fansi_0.4.0 ## [29] highr_0.8 broom_0.5.3 Rcpp_1.0.3 scales_1.1.0 ## [33] backports_1.1.5 webshot_0.5.2 jsonlite_1.6 farver_2.0.1 ## [37] fs_1.3.1 hms_0.5.2 digest_0.6.23 stringi_1.4.3 ## [41] bookdown_0.16 grid_3.6.2 cli_2.0.0 tools_3.6.2 ## [45] magrittr_1.5 lazyeval_0.2.2 crayon_1.3.4 pkgconfig_2.0.3 ## [49] zeallot_0.1.0 xml2_1.2.2 reprex_0.3.0 lubridate_1.7.4 ## [53] assertthat_0.2.1 rmarkdown_2.0 httr_1.4.1 rstudioapi_0.10 ## [57] R6_2.4.1 nlme_3.1-142 compiler_3.6.2 "]
]
