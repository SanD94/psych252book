[
["index.html", "Psych 252: Statistical Methods for Behavioral and Social Sciences Preface", " Psych 252: Statistical Methods for Behavioral and Social Sciences Tobias Gerstenberg 2019-03-19 Preface This book contains the course notes for Psych 252. The book is not intended to be self-explanatory and instead should be used in combination with the course lectures. If you have any questions about the notes, please feel free to contact me at: gerstenberg@stanford.edu "],
["introduction.html", "Chapter 1 Introduction 1.1 Thanks 1.2 List of R packages used in this book", " Chapter 1 Introduction 1.1 Thanks Various people have helped in the process of putting together these materials (either knowingly, or unknowingly). Big thanks go to: Alexandra Chouldechova Ben Baumer Benoit Monin Datacamp David Lagnado Ewart Thomas Henrik Singmann Julian Jara-Ettinger Kevin Smith Maarten Speekenbrink Matthew Kay Matthew Salganik Mika Braginsky Mike Frank Mine Çetinkaya-Rundel Patrick Mair Peter Cushner Mohanty Richard McElreath Russ Poldrack Stephen Dewitt Tom Hardwicke Tristan Mahr Special thanks also to my teaching assistants: Andrew Lampinen Mona Rosenke Shao-Fang (Pam) Wang 1.2 List of R packages used in this book # RMarkdown library(&quot;knitr&quot;) # markdown things library(&quot;kableExtra&quot;) # for nicely formatted tables # Datasets library(&quot;gapminder&quot;) # data available from Gapminder.org library(&quot;NHANES&quot;) # data set library(&quot;titanic&quot;) # titanic dataset # Data manipulation library(&quot;arrangements&quot;) # fast generators and iterators for permutations, combinations and partitions library(&quot;magrittr&quot;) # for wrangling library(&quot;tidyverse&quot;) # everything else # Visualization library(&quot;patchwork&quot;) # making figure panels library(&quot;cowplot&quot;) # making figure panels library(&quot;ggpol&quot;) # for making fancy boxplots library(&quot;ggridges&quot;) # for making joyplots library(&quot;gganimate&quot;) # for making animations library(&quot;GGally&quot;) # for pairs plot library(&quot;ggrepel&quot;) # for labels in ggplots library(&quot;corrr&quot;) # for calculating correlations between many variables library(&quot;corrplot&quot;) # for plotting correlations library(&quot;DiagrammeR&quot;) # for drawing diagrams # Modeling library(&quot;afex&quot;) # also for running ANOVAs library(&quot;lme4&quot;) # mixed effects models library(&quot;emmeans&quot;) # comparing estimated marginal means library(&quot;broom&quot;) # getting tidy model summaries library(&quot;broom.mixed&quot;) # getting tidy mixed model summaries library(&quot;janitor&quot;) # cleaning variable names library(&quot;car&quot;) # for running ANOVAs library(&quot;rstanarm&quot;) # for Bayesian models library(&quot;greta&quot;) # Bayesian models library(&quot;tidybayes&quot;) # tidying up results from Bayesian models library(&quot;boot&quot;) # bootstrapping library(&quot;modelr&quot;) # cross-validation and bootstrapping library(&quot;mediation&quot;) # for mediation and moderation analysis library(&quot;multilevel&quot;) # Sobel test library(&quot;extraDistr&quot;) # additional probability distributions library(&quot;effects&quot;) # for showing effects in linear, generalized linear, and other models library(&quot;brms&quot;) # Bayesian regression # Misc library(&quot;tictoc&quot;) # timing things library(&quot;MASS&quot;) # various useful functions (e.g. bootstrapped confidence intervals) library(&quot;lsr&quot;) # for computing effect size measures library(&quot;extrafont&quot;) # additional fonts library(&quot;pwr&quot;) # for power calculations "],
["visualization-1.html", "Chapter 2 Visualization 1 2.1 Learning objectives 2.2 Load packages 2.3 Why visualize data? 2.4 Setting up RStudio 2.5 Getting help 2.6 Data visualization using ggplot2 2.7 Additional resources", " Chapter 2 Visualization 1 In this lecture, we will take a look at how to visualize data using the powerful ggplot2 package. We will use ggplot2 a lot throughout the rest of the course! 2.1 Learning objectives Get familiar with the RStudio interface. Take a look at some suboptimal plots, and think about how to make them better. Understand the general philosophy behind ggplot2 – a grammar of graphics. Understand the mapping from data to geoms in ggplot2. Create informative figures using grouping and facets. 2.2 Load packages Let’s first load the packages that we need for this chapter. library(&quot;knitr&quot;) # for rendering the RMarkdown file library(&quot;tidyverse&quot;) # for plotting (and many more cool things we&#39;ll discover later) The tidyverse is a collection of packages that includes ggplot2. 2.3 Why visualize data? The greatest value of a picture is when it forces us to notice what we never expected to see. — John Tukey There is no single statistical tool that is as powerful as a well‐chosen graph. (Chambers et al. 1983) …make both calculations and graphs. Both sorts of output should be studied; each will contribute to understanding. (Anscombe 1973) Figure 2.1: Anscombe’s quartet. Anscombe’s quartet in Figure ?? (left side) illustrates the importance of visualizing data. Even though the datasets I-IV have the same summary statistics (mean, standard deviation, correlation), they are importantly different from each other. On the right side, we have four data sets with the same summary statistics that are very similar to each other. Figure 2.2: The Pearson’s \\(r\\) correlation coefficient is the same for all of these datasets. Source: Data Visualization – A practical introduction by Kieran Healy All the datasets in Figure 2.2 share the same correlation coefficient. However, again, they are very different from each other. Figure 2.3: The Datasaurus Dozen. While different in appearance, each dataset has the same summary statistics to two decimal places (mean, standard deviation, and Pearson’s correlation). The data sets in Figure 2.3 all share the same summary statistics. Clearly, the data sets are not the same though. Tip: Always plot the data first! Here is the paper from which I took Figure 2.1 and 2.3. It explains how the figures were generated and shows more examples for how summary statistics and some kinds of plots are insufficient to get a good sense for what’s going on in the data. Figure 2.4: Animation showing different data sets that all share the same summary statistics. 2.3.1 How not to visualize data Below are some examples of visualizations that could be improved. How would you make them better? Figure 2.5: Example of a bad plot. Source: Data Visualization – A practical introduction by Kieran Healy Figure 2.6: Another bad plot. Source: Google image search for ‘bad graphs’ Figure 2.7: And another one. Source: Bad graph wall of shame Figure 2.8: And another one. Source: Bad graph wall of shame Figure 2.9: And another one. Source: Bad graph wall of shame Figure 2.10: The last one for now. Source: Bad graph wall of shame 2.3.2 How to make it better In this class, we you will learn how to use ggplot2 to make nice figures. The ggplot2 library provides a unified framework for making plots – it defines a grammar of graphics according to which we construct figures step by step. Instead of learning rigid rules for what makes for a good figure, you will learn how to make figures yourself, play around with things, and get a feeling for what works best. 2.4 Setting up RStudio Figure 2.11: General preferences. Make sure that: Restore .RData into workspace at startup is unselected Save workspace to .RData on exit is set to Never Figure 2.12: Code window preferences. Make sure that: Soft-wrap R source files is selected This way you don’t have to scroll horizontally. At the same time, avoid writing long single lines of code. For example, instead of writing code like so: ggplot(data = diamonds, aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, width = 0.85) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1.5) + labs(title = &quot;Price as a function of quality of cut&quot;, subtitle = &quot;Note: The price is in US dollars&quot;, tag = &quot;A&quot;, x = &quot;Quality of the cut&quot;, y = &quot;Price&quot;) You may want to write it this way instead: ggplot(data = diamonds, aes(x = cut, y = price)) + # display the means stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, width = 0.85) + # display the error bars stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1.5) + # change labels labs(title = &quot;Price as a function of quality of cut&quot;, subtitle = &quot;Note: The price is in US dollars&quot;, # we might want to change this later tag = &quot;A&quot;, x = &quot;Quality of the cut&quot;, y = &quot;Price&quot;) This makes it much easier to see what’s going on, and you can easily add comments to individual lines of code. RStudio makes it easy to write nice code. It figures out where to put the next line of code when you press ENTER. And if things ever get messy, just select the code of interest and hit cmd+i to re-indent the code. Here are some more resources with tips for how to write nice code in R: Advanced R style guide 2.5 Getting help There are three simple ways to get help in R. You can either put a ? in front of the function you’d like to learn more about, or use the help() function. ?print help(&quot;print&quot;) Tip: To see the help file, hover over a function (or dataset) with the mouse (or select the text) and then press F1. I recommend using F1 to get to help files – it’s the fastest way! R help files can sometimes look a little cryptic. Most R help files have the following sections (copied from here): Title: A one-sentence overview of the function. Description: An introduction to the high-level objectives of the function, typically about one paragraph long. Usage: A description of the syntax of the function (in other words, how the function is called). This is where you find all the arguments that you can supply to the function, as well as any default values of these arguments. Arguments: A description of each argument. Usually this includes a specification of the class (for example, character, numeric, list, and so on). This section is an important one to understand, because arguments are frequently a cause of errors in R. Details: Extended details about how the function works, provides longer descriptions of the various ways to call the function (if applicable), and a longer discussion of the arguments. Value: A description of the class of the value returned by the function. See also: Links to other relevant functions. In most of the R editors, you can click these links to read the Help files for these functions. Examples: Worked examples of real R code that you can paste into your console and run. Here is the help file for the print() function: Figure 2.13: Help file for the print() function. 2.6 Data visualization using ggplot2 We will use the ggplot2 package to visualize data. By the end of next class, you’ll be able to make a figure like this: Figure 2.14: What a nice figure! Now let’s figure out how to get there. 2.6.1 Setting up a plot Let’s first get some data. df.diamonds = diamonds The diamonds dataset comes with the ggplot2 package. We can get a description of the dataset by running the following command: ?diamonds Above, we assigned the diamonds dataset to the variable df.diamonds so that we can see it in the data explorer. Let’s take a look at the full dataset by clicking on it in the explorer. Tip: You can view a data frame by highlighting the text in the editor (or simply moving the mouse above the text), and then press F2. The df.diamonds data frame contains information about almost 60,000 diamonds, including their price, carat value, size, etc. Let’s use visualization to get a better sense for this dataset. We start by setting up the plot. To do so, we pass a data frame to the function ggplot() in the following way. ggplot(data = df.diamonds) This, by itself, won’t do anything yet. We also need to specify what to plot. Let’s take a look at how much diamonds of different color cost. The help file says that diamonds labeled D have the best color, and diamonds labeled J the worst color. Let’s make a bar plot that shows the average price of diamonds for different colors. We do so via specifying a mapping from the data to the plot aesthetics with the function aes(). We need to tell aes() what we would like to display on the x-axis, and the y-axis of the plot. ggplot(data = df.diamonds, mapping = aes(x = color, y = price)) Here, we specified that we want to plot color on the x-axis, and price on the y-axis. As you can see, ggplot2 has already figured out how to label the axes. However, we still need to specify how to plot it. Let’s make a bar graph: ggplot(data = df.diamonds, mapping = aes(x = color, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;) Neat! Three lines of code produce an almost-publication-ready plot (to be published in the Proceedings of Unnecessary Diamonds)! Note how we used a + at the end of the first line of code to specify that there will be more. This is a very powerful idea underlying ggplot2. We can start simple and keep adding things to the plot step by step. We used the stat_summary() function to define what we want to plot (the “mean”), and how (as a “bar” chart). Let’s take a closer look at that function. help(stat_summary) Not the the easiest help file … We supplied two arguments to the function, fun.y = and geom =. The fun.y argument specifies what function we’d like to apply to the data for each value of x. Here, we said that we would like to take the mean and we specified that as a string. The geom (= geometric object) argument specifies how we would like to plot the result, namely as a “bar” plot. Instead of showing the “mean”, we could also show the “median” instead. ggplot(data = df.diamonds, mapping = aes(x = color, y = price)) + stat_summary(fun.y = &quot;median&quot;, geom = &quot;bar&quot;) And instead of making a bar plot, we could plot some points. ggplot(df.diamonds, aes(x = color, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) Tip: Take a look here to see what other geoms ggplot2 supports. Somewhat surprisingly, diamonds with the best color (D) are not the most expensive ones. What’s going on here? We’ll need to do some more exploration to figure this out. Note that in the last plot, I removed the data = and mapping = specifiers. These keywords are optional, and as long as we provide the arguments to the function in the correct order, we are ok. So, the following doesn’t work: ggplot(aes(x = color, y = price), df.diamonds) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) While this works: ggplot(mapping = aes(x = color, y = price), data = df.diamonds) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) In general, it’s good practice to include the specifiers – particularly for functions that are not used all the time. If the same function is used multiple times throughout the script, I would suggest to use the specifiers first, and then it’s ok to drop them later. 2.6.2 Setting the default plot theme Before moving one, let’s set a different default theme for our plots. Personally, I’m not a big fan of the gray background and the white grid lines. Also, the default size of the text should be bigger. We can change the default theme using the theme_set() function like so: theme_set( theme_classic() + # set the theme theme(text = element_text(size = 20)) # set the default text size ) From now onwards, all our plots will use what’s specified in theme_classic(), and the default text size will be larger, too. For any individual plot, we can still override these settings. 2.6.3 Scatter plot I don’t know much about diamonds, but I do know that diamonds with a higher carat value tend to be more expensive. color was a discrete variable with seven different values. carat, however, is a continuous variable. We want to see how the price of diamonds differs as a function of the carat value. Since we are interested in the relationship between two continuous variables, plotting a bar graph won’t work. Instead, let’s make a scatter plot. Let’s put the carat value on the x-axis, and the price on the y-axis. ggplot(data = df.diamonds, mapping = aes(x = carat, y = price)) + geom_point() Figure 2.15: Scatterplot. Cool! That looks sensible. Diamonds with a higher carat value tend to have a higher price. Our dataset has 53940 rows. So the plot actually shows 53940 circles even though we can’t see all of them since they overlap. Let’s make some progress on trying to figure out why the diamonds with the better color weren’t the most expensive ones on average. We’ll add some color to the scatter plot in Figure ??. We color each of the points based on the diamond’s color. To do so, we pass another argument to the aesthetics of the plot via aes(). ggplot(data = df.diamonds, mapping = aes(x = carat, y = price, color = color)) + geom_point() Figure 2.16: Scatterplot with color. Aha! Now we’ve got some color. Notice how in Figure ?? ggplot2 added a legend for us, thanks! We’ll see later how to play around with legends. Form just eye-balling the plot, it looks like the diamonds with the best color (D) tended to have a lower carat value, and the ones with the worst color (J), tended to have the highest carat values. So this is why diamonds with better colors are less expensive – these diamonds have a lower carat value overall. There are many other things that we can define in aes(). Take a quick look at the vignette: vignette(&quot;ggplot2-specs&quot;) 2.6.3.1 Practice plot 1 Make a scatter plot that shows the relationship between the variables depth (on the x-axis), and table (on the y-axis). Take a look at the description for the diamonds dataset so you know what these different variables mean. Your plot should look like the one shown in Figure 2.17. # make practice plot 1 here include_graphics(&quot;figures/practice_plot1.png&quot;) Figure 2.17: Practice plot 1. 2.6.4 Line plot What else do we know about the diamonds? We actually know the quality of how they were cut. The cut variable ranges from “Fair” to “Ideal”. First, let’s take a look at the relationship between cut and price. This time, we’ll make a line plot instead of a bar plot (just because we can). ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;) ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? Oops! All we did is that we replaced x = color with x = cut, and geom = \"bar\" with geom = \"line\". However, the plot doesn’t look like expected (i.e. there is no real plot). What happened here? The reason is that the line plot needs to know what points to connect. The error message tells us that each group consists of only one observation. Let’s adjust the group asthetic to fix this. ggplot(data = df.diamonds, mapping = aes(x = cut, y = price, group = 1)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;) By adding the parameter group = 1 to mapping = aes(), we specify that we would like all the levels in x = cut to be treated as coming from the same group. The reason for this is that cut (our x-axis variable) is a factor (and not a numeric variable), so, by default, ggplot2 tries to draw a separate line for each factor level. We’ll learn more about grouping below (and about factors later). Interestingly, there is no simple relationship between the quality of the cut and the price of the diamond. In fact, “Ideal” diamonds tend to be cheapest. 2.6.5 Adding error bars We often don’t just want to show the means but also give a sense for how much the data varies. ggplot2 has some convenient ways of specifying error bars. Let’s take a look at how much price varies as a function of clarity (another variable in our diamonds data frame). ggplot(data = df.diamonds, mapping = aes(x = clarity, y = price)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) + # plot bootstrapped error bars first stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) # add points with means Figure 2.18: Relationship between diamond clarity and price. Error bars indicate 95% bootstrapped confidence intervals. Here we have it. The average price of our diamonds for different levels of clarity together with bootstrapped 95% confidence intervals. How do we know that we have 95% confidence intervals? That’s what mean_cl_boot() computes as a default. Let’s take a look at that function: help(mean_cl_boot) Remember that you can just select the text (or merely put the cursor over the word) and press F1 to see the help. The help file tell us about the function smean.cl.boot in the Hmisc package. The mean_cl_boot() function is a version that works well with ggplot2. We see that this function takes as inputs, the confidence interval conf.int, the number of bootstrap samples B, and some other ones that we don’t care about for now. So let’s make the same plot again with 99.9% confidence intervals, and 2000 bootstrap samples. ggplot(data = df.diamonds, mapping = aes(x = clarity, y = price)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, fun.args = list(conf.int = .999, B = 2000)) + # plot bootstrapped error bars first stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) # add points with means Figure 2.19: Relationship between diamond clarity and price. Error bars indicate 99% bootstrapped confidence intervals. Note how the error bars are larger now in Figure Figure ?? compared to Figure ?? . Note the somewhat peculiar way in which we supplied the parameters to the mean_cl_boot function. The fun.args argument takes in a list of arguments that it then passes on to the function mean_cl_boot. 2.6.5.1 Order matters The order in which we add geoms to a ggplot matters! Generally, we want to plot error bars before the points that represent the means. To illustrate, let’s set the color in which we show the means to “red”. ggplot(df.diamonds, aes(x = clarity, y = price)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;red&quot;) Figure 2.20: This figure looks good. Error bars and means are drawn in the correct order. Figure ?? looks good. # I&#39;ve changed the order in which the means and error bars are drawn. ggplot(df.diamonds, aes(x = clarity, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;red&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) Figure 2.21: This figure looks good. Error bars and means are drawn in the correct order. Figure ?? doesn’t look good. The error bars are on top of the points that represent the means. One cool feature about using stat_summary() is that we did not have to change anything about the data frame that we used to make the plots. We directly used our raw data instead of having to make separate data frames that contain the relevant information (such as the means and the confidence intervals). You may not remember exactly what confidence intervals actually are. Don’t worry! We’ll have a recap later in class. Let’s take a look at two more principles for plotting data that are extremely helpful: groups and facets. But before, another practice plot. 2.6.5.2 Practice plot 2 Make a bar plot that shows the average price of diamonds (on the y-axis) as a function of their clarity (on the x-axis). Also add error bars. Your plot should look like the one shown in Figure ??. # make practice plot 2 here include_graphics(&quot;figures/practice_plot2.png&quot;) Figure 2.22: Practice plot 2. 2.6.6 Grouping data Grouping in ggplot2 is a very powerful idea. It allows us to plot subsets of the data – again without the need to make separate data frames first. Let’s make a plot that shows the relationship between price and color separately for the different qualities of cut. ggplot(data = df.diamonds, mapping = aes(x = color, y = price, group = cut)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;) Well, we got some separate lines here but we don’t know which line corresponds to which cut. Let’s add some color! ggplot(data = df.diamonds, mapping = aes(x = color, y = price, group = cut, color = cut)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;, size = 2) Nice! In addition to adding color, I’ve made the lines a little thicker here by setting the size argument to 2. Grouping is very useful for bar plots. Let’s take a look at how the average price of diamonds looks like taking into account both cut and color (I know – exciting times!). Let’s put the color on the x-axis and then group by the cut. ggplot(data = df.diamonds, mapping = aes(x = color, y = price, group = cut, color = cut)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;) That’s a fail! Several things went wrong here. All the bars are gray and only their outline is colored differently. Instead we want the bars to have a different color. For that we need to specify the fill argument rather than the color argument! But things are worse. The bars currently are shown on top of each other. Instead, we’d like to put them next to each other. Here is how we can do that: ggplot(data = df.diamonds, mapping = aes(x = color, y = price, group = cut, fill = cut)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, position = position_dodge()) + scale_fill_manual(values = c(&quot;lightblue&quot;, &quot;blue&quot;, &quot;orangered&quot;, &quot;red&quot;, &quot;black&quot;)) Neato! We’ve changed the color argument to fill, and have added the position = position_dodge() argument to the stat_summary() call. This argument makes it such that the bars are nicely dodged next to each other. Let’s add some error bars just for kicks. ggplot(data = df.diamonds, mapping = aes(x = color, y = price, group = cut, fill = cut)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, position = position_dodge(width = 0.9), color = &quot;black&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, position = position_dodge(width = 0.9)) Voila! Now with error bars. Note that we’ve added the width = 0.9 argument to position_dodge(). Somehow R was complaining when this was not defined for geom “linerange”. I’ve also added some outline to the bars by including the argument color = \"black\". I think it looks nicer this way. So, still somewhat surprisingly, diamonds with the worst color (J) are more expensive than dimanods with the best color (D), and diamonds with better cuts are not necessarily more expensive. 2.6.6.1 Practice plot 3 Recreate the plot shown in Figure ??. # make practice plot 3 here ggplot(diamonds, aes(x = color, y = price, group = clarity, color = clarity))+ stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1)+ stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;, size = 2) include_graphics(&quot;figures/practice_plot3.png&quot;) Figure 2.23: Practice plot 3. 2.6.7 Making facets Having too much information in a single plot can be overwhelming. The previous plot is already pretty busy. Facets are a nice way of spliting up plots and showing information in separate panels. Let’s take a look at how wide these diamonds tend to be. The width in mm is given in the y column of the diamonds data frame. We’ll make a histogram first. To make a histogram, the only aesthetic we needed to specify is x. ggplot(data = df.diamonds, mapping = aes(x = y)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. That looks bad! Let’s pick a different value for the width of the bins in the histogram. ggplot(data = df.diamonds, mapping = aes(x = y)) + geom_histogram(binwidth = 0.1) Still bad. There seems to be an outlier diamond that happens to be almost 60 mm wide, while most of the rest is much narrower. One option would be to remove the outlier from the data before plotting it. But generally, we don’t want to make new data frames. Instead, let’s just limit what data we want to show in the plot. ggplot(data = df.diamonds, mapping = aes(x = y)) + geom_histogram(binwidth = 0.1) + coord_cartesian(xlim = c(3, 10)) I’ve used the coord_cartesian() function to restrict the range of data to show by passing a minimum and maximum to the xlim argument. This looks better now. Instead of histograms, we can also plot a density fitted to the distribution. ggplot(data = df.diamonds, mapping = aes(x = y)) + geom_density() + coord_cartesian(xlim = c(3, 10)) Looks pretty similar to our histogram above! Just like we can play around with the binwidth of the histogram, we can change the smoothing bandwidth of the kernel that is used to create the histogram. Here is a histogram with a much wider bandwidth: ggplot(data = df.diamonds, mapping = aes(x = y)) + geom_density(bw = 0.5) + coord_cartesian(xlim = c(3, 10)) We’ll learn more about how these densities are determined later in class. I promised that this section was about making facets, right? We’re getting there! Let’s first take a look at how wide diamonds of different color are. We can use grouping to make this happen. ggplot(data = df.diamonds, mapping = aes(x = y, group = color, fill = color)) + geom_density(bw = 0.2, alpha = 0.2) + coord_cartesian(xlim = c(3, 10)) OK! That’s a little tricky to tell apart. Notice that I’ve specified the alpha argument in the geom_density() function so that the densities in the front don’t completely hide the densities in the back. But this plot still looks too busy. Instead of grouping, let’s put the densities for the different colors, in separate panels. That’s what facetting allows you to do. ggplot(data = df.diamonds, mapping = aes(x = y, fill = color)) + geom_density(bw = 0.2) + facet_grid(cols = vars(color)) + coord_cartesian(xlim = c(3, 10)) Now we have the densities next to each other in separate panels. I’ve removed the alpha argument since the densities aren’t overlapping anymore. To make the different panels, I used the facet_grid() function and specified that I want separate columns for the different colors (cols = vars(color)). What’s the deal with vars()? Why couldn’t we just write facet_grid(cols = color) instead? The short answer is: that’s what the function wants. The long answer is: long. (We’ll learn more about this later in the course.) To show the facets in different rows instead of columns we simply replace cols = vars(color) with rows = vars(color). ggplot(data = df.diamonds, mapping = aes(x = y, fill = color)) + geom_density(bw = 0.2) + facet_grid(rows = vars(color)) + coord_cartesian(xlim = c(3, 10)) Several aspects about this plot should be improved: the y-axis text is overlapping having both a legend and separate facet labels is redundant having separate fills is not really necessary here So, what does this plot actually show us? Well, J-colored diamonds tend to be wider than D-colored diamonds. Fascinating! Of course, we could go completely overboard with facets and groups. So let’s do it! Let’s look at how the average price (somewhat more interesting) varies as a function of color, cut, and clarity. We’ll put color on the x-axis, and make separate rows for cut and columns for clarity. ggplot(data = df.diamonds, mapping = aes(y = price, x = color, fill = color)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) + facet_grid(rows = vars(cut), cols = vars(clarity)) ## Warning: Removed 5 rows containing missing values (geom_linerange). Figure 2.24: A figure that is stretching it in terms of information. Figure ?? is stretching it in terms of how much information it presents. But it gives you a sense for how to combine the differnet bits and pieces we’ve learned so far. 2.6.7.1 Practice plot 4 Recreate the plot shown in Figure ??. # make practice plot 4 here ggplot(diamonds, aes(x = color, y = price, fill = cut))+ stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, position = position_dodge(width = 0.9), color = &quot;black&quot;)+ stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, position = position_dodge(width = 0.9), color = &quot;black&quot;)+ facet_grid(rows = vars(clarity)) + theme(axis.text.y = element_text(size = 10)) ## Warning: Removed 5 rows containing missing values (geom_linerange). include_graphics(&quot;figures/practice_plot4.png&quot;) Figure 2.25: Practice plot 4. 2.6.8 Global, local, and setting aes() ggplot2 allows you to specify the plot aesthetics in different ways. ggplot(data = df.diamonds, mapping = aes(x = carat, y = price, color = color)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) Here, I’ve drawn a scatter plot of the relationship between carat and price, and I have added the best-fitting regression lines via the geom_smooth(method = \"lm\") call. (We will learn more about what these regression lines mean later in class.) Because I have defined all the aesthetics at the top level (i.e. directly within the ggplot() function), the aesthetics apply to all the functions afterwards. Aesthetics defined in the ggplot() call are global. In this case, the geom_point() and the geom_smooth() functions. The geom_smooth() function produces separate best-fit regression lines for each different color. But what if we only wanted to show one regression line instead that applies to all the data? Here is one way of doing so: ggplot(data = df.diamonds, mapping = aes(x = carat, y = price)) + geom_point(mapping = aes(color = color)) + geom_smooth(method = &quot;lm&quot;) Here, I’ve moved the color aesthetic into the geom_point() function call. Now, the x and y aesthetics still apply to both the geom_point() and the geom_smooth() function call (they are global), but the color aesthetic applies only to geom_point() (it is local). Alternatively, we can simply overwrite global aesthetics within local function calls. ggplot(data = df.diamonds, aes(x = carat, y = price, color = color)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) Here, I’ve set color = 'black' within the geom_smooth() function, and now only one overall regression line is displayed since the global color aesthetic was overwritten in the local function call. 2.7 Additional resources 2.7.1 Cheatsheets RStudio IDE –&gt; information about RStudio RMarkdown –&gt; information about writing in RMarkdown RMarkdown reference –&gt; RMarkdown reference sheet Data visualization –&gt; general principles of effective graphic design ggplot2 –&gt; specific information about ggplot 2.7.2 Data camp courses Introduction to R RStudio IDE 1 RStudio IDE 2 Communicating with data ggplot2 course 1 ggplot2 course 2 2.7.3 Books and chapters R graphics cookbook –&gt; quick intro to the the most common graphs R for Data Science book Data visualization Graphics for communication Data Visualization – A practical introduction (by Kieran Healy) Look at data Make a plot Show the right numbers Fundamentals of Data Visualization –&gt; very nice resource that goes beyond basic functionality of ggplot and focuses on how to make good figures (e.g. how to choose colors, axes, …) 2.7.4 Misc ggplot2 extensions –&gt; gallery of ggplot2 extension packages ggplot2 gui –&gt; ggplot2 extension package ggplot2 visualizations with code –&gt; gallery of plots with code R Markdown in RStudio introduction R Markdown for class reports knitr in a nutshell styler –&gt; RStudio add-in that re-formats code References "],
["visualization-2.html", "Chapter 3 Visualization 2 3.1 Learning objectives 3.2 Install and load packages, load data, set theme 3.3 Overview of different plot types for different things 3.4 Customizing plots 3.5 Saving plots 3.6 Creating figure panels 3.7 Peeking behind the scenes 3.8 Making animations 3.9 Shiny apps 3.10 Defining snippets 3.11 Additional resources", " Chapter 3 Visualization 2 In this lecture, we will lift our ggplot2 skills to the next level! 3.1 Learning objectives Deciding what plot is appropriate for what kind of data. Customizing plots: Take a sad plot and make it better. Saving plots. Making figure panels. Debugging. Making animations. Defining snippets. 3.2 Install and load packages, load data, set theme Let’s first install the new packages that you might not have yet. Note that the patchwork package is not on CRAN yet (where most of the R packages live), but we can install it directly from the github repository. Now, let’s load the packages that we need for this chapter. library(&quot;knitr&quot;) # for rendering the RMarkdown file library(&quot;patchwork&quot;) # for making figure panels library(&quot;ggpol&quot;) # for making fancy boxplots library(&quot;ggridges&quot;) # for making joyplots library(&quot;gganimate&quot;) # for making animations library(&quot;gapminder&quot;) # data available from Gapminder.org library(&quot;tidyverse&quot;) # for plotting (and many more cool things we&#39;ll discover later) And let’s load the diamonds data again. df.diamonds = diamonds Let’s also set the default theme for the plots again. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 3.3 Overview of different plot types for different things Different plots works best for different kinds of data. Let’s take a look at some. 3.3.1 Proportions 3.3.1.1 Stacked bar charts ggplot(data = df.diamonds, aes(x = cut, y = stat(count), fill = color)) + geom_bar(color = &quot;black&quot;) This bar chart shows for the different cuts (x-axis), the number of diamonds of different color. To get these counts, I’ve used the stat(count) construction. Stacked bar charts give a good general impression of the data. However, it’s difficult to precisely compare different proportions. 3.3.1.2 Pie charts Figure 3.1: Finally a pie chart that makes sense. Pie charts have a bad reputation. And there are indeed a number of problems with pie charts: proportions are difficult to compare don’t look good when there are many categories ggplot(data = df.diamonds, mapping = aes(x = 1, y = stat(count / sum(count)), fill = cut)) + geom_bar() + coord_polar(&quot;y&quot;, start = 0) + theme_void() We can create a pie chart with ggplot2 by changing the coordinate system using coord_polar(). To get the frequency of the different categories, I used the stat() function. If we are interested in comparing proportions and we don’t have too many data points, then tables are a good alternative to showing figures. 3.3.2 Comparisons Often we want to compare the data from many different conditions. And sometimes, it’s also useful to get a sense for what the individual participant data look like. Here is a plot that achieves both. ggplot(data = df.diamonds[1:150, ], mapping = aes(x = color, y = price)) + # individual data points (jittered horizontally) geom_point(alpha = 0.2, color = &quot;blue&quot;, position = position_jitter(width = 0.1, height = 0), size = 2) + # error bars stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, color = &quot;black&quot;, size = 1) + # means stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, fill = &quot;yellow&quot;, color = &quot;black&quot;, stroke = 2, size = 4) Figure 3.2: Price of differently colored diamonds. Red circles are means, black circles are individual data poins, and the error bars are 95% bootstrapped confidence intervals. This plot shows means, bootstrapped confidence intervals, and individual data points. I’ve used two tricks to make the individual data points easier to see. 1. I’ve set the alpha attribute to make the points somewhat transparent. 2. I’ve used the position_jitter() function to jitter the points horizontally. 3. I’ve used shape = 21 for displaying the mean. For this circle shape, we can set a color and fill (see Figure 3.3) Figure 3.3: Different shapes that can be used for plotting. Note that I’m only plotting the first 150 entries of the data here by setting data = df.diamonds[1:150,] in gpplot(). 3.3.2.1 Boxplots Another way to get a sense for the distribution of the data is to use box plots. ggplot(data = df.diamonds[1:500,], mapping = aes(x = color, y = price)) + geom_boxplot() What do boxplots show? Here adapted from help(geom_boxplot()): The boxplots show the median as a horizontal black line. The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles) of the data. The whiskers (= black vertical lines) extend from the top or bottom of the hinge by at most 1.5 * IQR (where IQR is the inter-quartile range, or distance between the first and third quartiles). Data beyond the end of the whiskers are called “outlying” points and are plotted individually. Personally, I’m not a big fan of boxplots. Many data sets are consistent with the same boxplot. Figure 3.4: Box plot distributions. Source: https://www.autodeskresearch.com/publications/samestats Figure 3.4 shows three different distributions that each correspond to the same boxplot. If there is not too much data, I recommend to plot jittered individual data points instead. If you do have a lot of data points, then violin plots can be helpful. (#fig:visualization2-12, boxplot-violin)Boxplot distributions. Source: https://www.autodeskresearch.com/publications/samestats Figure ?? shows the same raw data represented as jittered dots, boxplots, and violin plots. The ggpol packages has a geom_boxjitter() function which displays a boxplot and the jittered data right next to each other. set.seed(1) # used to make the example reproducible ggplot(data = df.diamonds %&gt;% sample_n(1000), mapping = aes(x = color, y = price)) + ggpol::geom_boxjitter(jitter.shape = 1, jitter.color = &quot;black&quot;, jitter.alpha = 0.2, jitter.height = 0, jitter.width = 0.04, outlier.color = NA, errorbar.draw = FALSE)+ stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, color = &quot;black&quot;, fill = &quot;yellow&quot;, size = 4) 3.3.2.2 Violin plots We make violin plots like so: ggplot(data = df.diamonds, mapping = aes(x = color, y = price)) + geom_violin() Violin plots are good for detecting bimodal distributions. They work well when: You have many data points. The data is continuous. Violin plots don’t work well for Likert-scale data (e.g. ratings on a discrete scale from 1 to 7). Here is a simple example: set.seed(1) data = data.frame(rating = sample(x = 1:7, prob = c(0.1, 0.4, 0.1, 0.1, 0.2, 0, 0.1), size = 500, replace = T)) ggplot(data = data, mapping = aes(x = &quot;Likert&quot;, y = rating)) + geom_point(position = position_jitter(width = 0.05, height = 0.1), alpha = 0.05)+ stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, fill = &quot;blue&quot;, size = 5) This represents a vase much better than it represents the data. 3.3.2.3 Joy plots We can also show the distributions along the x-axis using the geom_density_ridges() function from the ggridges package. ggplot(data = df.diamonds, mapping = aes(x = price, y = color)) + ggridges::geom_density_ridges(scale = 1.5) ## Picking joint bandwidth of 535 3.3.2.4 Practice plot Try to make the plot shown in Figure ??. Here are some tips: For the data argument in ggplot() use: df.diamonds[1:1000, ] (this selects the first 1000 rows). Note that the violin plots have different areas that reflect the number of observations. Take a look at geom_violin()’s help file to figure out how to set this. Figure 3.2 will help you with figuring out the other components # make the plot here (#fig:visualization2-18, practice-plot5)Practice plot 5. 3.3.3 Relationships 3.3.3.1 Scatter plots Scatter plots are great for looking at the relationship between two continuous variables. ggplot(data = df.diamonds, mapping = aes(x = carat, y = price, color = color)) + geom_point() 3.3.3.2 Raster plots These are useful for looking how a variable of interest varies as a function of two other variables. For example, when we are trying to fit a model with two parameters, we might be interested to see how well the model does for different combinations of these two parameters. Here, we’ll plot what carat values diamonds of different color and clarity have. ggplot(data = df.diamonds, mapping = aes(x = color, y = clarity, z = carat)) + stat_summary_2d(fun = &quot;mean&quot;, geom = &quot;tile&quot;) Not too bad. Let’s add a few tweaks to make it look nicer. ggplot(data = df.diamonds, mapping = aes(x = color, y = clarity, z = carat)) + stat_summary_2d(fun = &quot;mean&quot;, geom = &quot;tile&quot;, color = &quot;black&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + labs(fill = &quot;carat&quot;) I’ve added some outlines to the tiles by specifying color = \"black\" in geom_tile() and I’ve changed the scale for the fill gradient. I’ve defined the color for the low value to be “white”, and for the high value to be “black.” Finally, I’ve changed the lower and upper limit of the scale via the limits argument. Looks much better now! We see that diamonds with clarity I1 and color J tend to have the highest carat values on average. 3.3.4 Temporal data Line plots are a good choice for temporal data. Here, I’ll use the txhousing data that comes with the ggplot2 package. The dataset contains information about housing sales in Texas. # ignore this part for now (we&#39;ll learn about data wrangling soon) df.plot = txhousing %&gt;% filter(city %in% c(&quot;Dallas&quot;, &quot;Fort Worth&quot;, &quot;San Antonio&quot;, &quot;Houston&quot;)) %&gt;% mutate(city = factor(city, levels = c(&quot;Dallas&quot;, &quot;Houston&quot;, &quot;San Antonio&quot;, &quot;Fort Worth&quot;))) ggplot(data = df.plot, mapping = aes(x = year, y = median, color = city, fill = city)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;ribbon&quot;, alpha = 0.2, linetype = 0) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) Ignore the top part where I’m defining df.plot for now (we’ll look into this in the next class). The other part is fairly straightforward. I’ve used stat_summary() three times: First, to define the confidence interval as a geom = \"ribbon\". Second, to show the lines connecting the means, and third to put the means as data points points on top of the lines. Let’s tweak the figure some more to make it look real good. df.plot = txhousing %&gt;% filter(city %in% c(&quot;Dallas&quot;, &quot;Fort Worth&quot;, &quot;San Antonio&quot;, &quot;Houston&quot;)) %&gt;% mutate(city = factor(city, levels = c(&quot;Dallas&quot;, &quot;Houston&quot;, &quot;San Antonio&quot;, &quot;Fort Worth&quot;))) df.text = df.plot %&gt;% filter(year == max(year)) %&gt;% group_by(city) %&gt;% summarize(year = mean(year) + 0.2, median = mean(median)) ggplot( data = df.plot, mapping = aes(x = year, y = median, color = city, fill = city)) + # draw dashed horizontal lines in the background geom_hline(yintercept = seq(from = 100000, to = 250000, by = 50000), linetype = 2, alpha = 0.2) + # draw ribbon stat_summary(fun.data = mean_cl_boot, geom = &quot;ribbon&quot;, alpha = 0.2, linetype = 0) + # draw lines connecting the means stat_summary(fun.y = &quot;mean&quot;, geom = &quot;line&quot;) + # draw means as points stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) + # add the city names geom_text(data = df.text, mapping = aes(label = city), hjust = 0, size = 5) + # set the y-axis labels scale_y_continuous(breaks = seq(from = 100000, to = 250000, by = 50000), labels = str_c(&quot;$&quot;, seq(from = 100, to = 250, by = 50), &quot;K&quot;)) + # set the x-axis labels scale_x_continuous(breaks = seq(from = 2000, to = 2015, by = 5)) + # set the limits for the coordinates coord_cartesian(xlim = c(1999, 2015), clip = &quot;off&quot;, expand = F) + # set the plot title and axes titles labs(title = &quot;Change of median house sale price in Texas&quot;, x = &quot;Year&quot;, y = &quot;Median house sale price&quot;, fill = &quot;&quot;, color = &quot;&quot;) + theme(title = element_text(size = 16), legend.position = &quot;none&quot;, plot.margin = margin(r = 1, unit = &quot;in&quot;)) 3.4 Customizing plots So far, we’ve seen a number of different ways of plotting data. Now, let’s look into how to customize the plots. For example, we may wanta to change the axis labels, add a title, increase the font size. ggplot2 let’s you customize almost anything. Let’s start simple. ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) This plot shows the average price for diamonds with a different quality of the cut, as well as the bootstrapped confidence intervals. Here are some things we can do to make it look nicer. ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + # change color of the fill, make a little more space between bars by setting their width stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, width = 0.85) + # make error bars thicker stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1.5) + # add a title, subtitle, and changed axis labels labs(title = &quot;Price as a function of quality of cut&quot;, subtitle = &quot;Note: The price is in US dollars&quot;, tag = &quot;A&quot;, x = &quot;Quality of the cut&quot;, y = &quot;Price&quot;) + # adjust what to show on the y-axis scale_y_continuous(breaks = seq(from = 0, to = 4000, by = 2000), labels = seq(from = 0, to = 4000, by = 2000)) + # adjust the range of both axes coord_cartesian(xlim = c(0.25, 5.75), ylim = c(0, 5000), expand = F) + theme( # adjust the text size text = element_text(size = 20), # add some space at top of x-title axis.title.x = element_text(margin = margin(t = 0.2, unit = &quot;inch&quot;)), # add some space t the right of y-title axis.title.y = element_text(margin = margin(r = 0.1, unit = &quot;inch&quot;)), # add some space underneath the subtitle and make it gray plot.subtitle = element_text(margin = margin(b = 0.3, unit = &quot;inch&quot;), color = &quot;gray70&quot;), # make the plot tag bold plot.tag = element_text(face = &quot;bold&quot;), # move the plot tag a little plot.tag.position = c(0.05, 0.99) ) I’ve tweaked quite a few things here (and I’ve added comments to explain what’s happening). Take a quick look at the theme() function to see all the things you can change. 3.4.1 Changing the order of things Sometimes we don’t have a natural ordering of our independent variable. In that case, it’s nice to show the data in order. ggplot(data = df.diamonds, mapping = aes(x = reorder(cut, price), y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, width = 0.85) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1.5) + labs(x = &quot;cut&quot;) The reorder() function helps us to do just that. Now, the results are ordered according to price. To show the results in descending order, I would simply need to write reorder(cut, -price) instead. 3.4.2 Dealing with legends Legends form an important part of many figures. However, it is often better to avoid legends if possible, and directly label the data. This way, the reader doesn’t have to look back and forth between the plot and the legend to understand what’s going on. Here, we’ll look into a few aspects that come up quite often. There are two main functions to manipulate legends with ggplot2 1. theme() (there are a number of arguments starting with legend.) 2. guide_legend() Let’s make a plot with a legend. ggplot(data = df.diamonds, mapping = aes(x = color, y = price, color = clarity)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) Let’s move the legend to the bottom of the plot: ggplot(data = df.diamonds, mapping = aes(x = color, y = price, color = clarity)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;) + theme(legend.position = &quot;bottom&quot;) Let’s change a few more things in the legend using the guides() function: have 3 rows reverse the legend order make the points in the legend larger ggplot(data = df.diamonds, mapping = aes(x = color, y = price, color = clarity)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, size = 2) + theme(legend.position = &quot;bottom&quot;) + guides( color = guide_legend( nrow = 3, # 3 rows reverse = TRUE, # reversed order override.aes = list(size = 6) # point size ) ) 3.4.3 Choosing good colors Color brewer helps with finding colors that are colorblind safe and printfriendly. For more information on how to use color effectively see here. ### Customizing themes For a given project, I often want all of my plots to share certain visual features such as the font type, font size, how the axes are displayed, etc. Instead of defining these for each individual plot, I can set a theme at the beginning of my project so that it applies to all the plots in this file. To do so, I use the theme_set() command: theme_set( theme_classic() + #classic theme theme(text = element_text(size = 20)) #text size ) Here, I’ve just defined that I want to use theme_classic() for all my plots, and that the text size should be 20. For any individual plot, I can still overwrite any of these defaults. 3.5 Saving plots To save plots, use the ggsave() command. Personally, I prefer to save my plots as pdf files. This way, the plot looks good no matter what size you need it to be. This means it’ll look good both in presentations as well as in a paper. You can save the plot in any format that you like. I strongly recommend to use a relative path to specify where the figure should be saved. This way, if you are sharing the project with someone else via Stanford Box, Dropbox, or Github, they will be able to run the code without errors. Here is an example for how to save one of the plots that we’ve created above. p1 = ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) print(p1) p2 = ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) ggsave(filename = &quot;figures/diamond_plot.pdf&quot;, plot = p1, width = 8, height = 6) Here, I’m saving the plot in the figures folder and it’s name is diamond_plot.pdf. I also specify the width and height as the plot in inches (which is the default unit). 3.6 Creating figure panels Sometimes, we want to create a figure with several subfigures, each of which is labeled with a), b), etc. We have already learned how to make separate panels using facet_wrap() or facet_grid(). The R package patchwork makes it very easy to combine multiple plots. Let’s combine a few plots that we’ve made above into one. # first plot p1 = ggplot(data = df.diamonds, mapping = aes(x = y, fill = color)) + geom_density(bw = 0.2, show.legend = F) + facet_grid(cols = vars(color)) + coord_cartesian(xlim = c(3, 10), expand = F) + #setting expand to FALSE removes any padding on x and y axes labs(title = &quot;Width of differently colored diamonds&quot;, tag = &quot;A&quot;) # second plot p2 = ggplot(data = df.diamonds, mapping = aes(x = color, y = clarity, z = carat)) + stat_summary_2d(fun = &quot;mean&quot;, geom = &quot;tile&quot;) + labs(title = &quot;Carat values&quot;, subtitle = &quot;For different color and clarity&quot;, x = &#39;width in mm&#39;, tag = &quot;B&quot;) # third plot p3 = ggplot(data = df.diamonds, mapping = aes(x = cut, y = price)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, width = 0.85) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1.5) + scale_x_discrete(labels = c(&#39;fair&#39;, &#39;good&#39;, &#39;very\\ngood&#39;, &#39;premium&#39;, &#39;ideal&#39;)) + labs( title = &quot;Price as a function of cut&quot;, subtitle = &quot;Note: The price is in US dollars&quot;, tag = &quot;C&quot;, x = &quot;Quality of the cut&quot;, y = &quot;Price&quot;) + coord_cartesian(xlim = c(0.25, 5.75), ylim = c(0, 5000), expand = F) # combine the plots p1 + (p2 + p3) + plot_layout(ncol = 1) &amp; theme_classic() &amp; theme(plot.tag = element_text(face = &quot;bold&quot;, size = 20)) # ggsave(&quot;figures/combined_plot.pdf&quot;, width = 10, height = 6) Not a perfect plot yet, but you get the idea. To combine the plots, we defined that we would like p2 and p3 to be displayed in the same row using the () syntax. And we specified that we only want one column via the plot_layout() function. We also applied the same theme_classic() to all the plots using the &amp; operator, and formatted how the plot tags should be displayed. For more info on how to use patchwork, take a look at the readme on the github page. Other packages that provide additional functionality for combining multiple plots into one are gridExtra and cowplot. You can find more information on how to lay out multiple plots here. An alternative way for making these plots is to use Adobe Illustrator, Powerpoint, or Keynote. However, you want to make changing plots as easy as possible. Adobe Illustrator has a feature that allows you to link to files. This way, if you change the plot, the plot within the illustrator file gets updated automatically as well. If possible, it’s much better to do everything in R though so that your plot can easily be reproduced by someone else. 3.7 Peeking behind the scenes Sometimes it can be helpful for debugging to take a look behind the scenes. Silently, ggplot() computes a data frame based on the information you pass to it. We can take a look at the data frame that’s underlying the plot. p = ggplot(data = df.diamonds, mapping = aes(x = color, y = clarity, z = carat)) + stat_summary_2d(fun = &quot;mean&quot;, geom = &quot;tile&quot;, color = &quot;black&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) print(p) build = ggplot_build(p) df.plot_info = build$data[[1]] dim(df.plot_info) # data frame dimensions ## [1] 56 18 I’ve called the ggplot_build() function on the ggplot2 object that we saved as p. I’ve then printed out the data associated with that plot object. The first thing we note about the data frame is how many entries it has, 56. That’s good. This means there is one value for each of the 7 x 8 grids. The columns tell us what color was used for the fill, the value associated with each row, where each row is being displayed (x and y), etc. If a plot looks weird, it’s worth taking a look behind the scenes. For example, something we thing we could have tried is the following (in fact, this is what I tried first): p = ggplot(data = df.diamonds, mapping = aes(x = color, y = clarity, fill = carat)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) print(p) build = ggplot_build(p) df.plot_info = build$data[[1]] dim(df.plot_info) # data frame dimensions ## [1] 53940 15 Why does this plot look different from the one before? What went wrong here? Notice that the data frame associated with the ggplot2 object has 53940 rows. So instead of plotting means here, we plotted all the individual data points. So what we are seeing here is just the top layer of many, many layers. 3.8 Making animations Animated plots can be a great way to illustrate your data in presentations. The R package gganimate lets you do just that. Here is an example showing how to use it. ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, size = pop, colour = country)) + geom_point(alpha = 0.7, show.legend = FALSE) + geom_text(data = gapminder %&gt;% filter(country %in% c(&quot;United States&quot;, &quot;China&quot;, &quot;India&quot;)), mapping = aes(label = country), color = &quot;black&quot;, vjust = -0.75, show.legend = FALSE) + scale_colour_manual(values = country_colors) + scale_size(range = c(2, 12)) + scale_x_log10(breaks = c(1e3, 1e4, 1e5), labels = c(&quot;1,000&quot;, &quot;10,000&quot;, &quot;100,000&quot;)) + theme_classic() + theme(text = element_text(size = 23)) + # Here come the gganimate specific bits labs(title = &quot;Year: {frame_time}&quot;, x = &quot;GDP per capita&quot;, y = &quot;life expectancy&quot;) + transition_time(year) + ease_aes(&quot;linear&quot;) ## Rendering [---------------------------------------] at 4.8 fps ~ eta: 21s ## Rendering [&gt;--------------------------------------] at 4.2 fps ~ eta: 23s ## Rendering [&gt;--------------------------------------] at 4.3 fps ~ eta: 23s ## Rendering [=&gt;-------------------------------------] at 4.4 fps ~ eta: 22s ## Rendering [=&gt;-------------------------------------] at 4.4 fps ~ eta: 21s ## Rendering [==&gt;------------------------------------] at 4.5 fps ~ eta: 21s ## Rendering [===&gt;-----------------------------------] at 4.5 fps ~ eta: 20s ## Rendering [====&gt;----------------------------------] at 4.5 fps ~ eta: 19s ## Rendering [=====&gt;---------------------------------] at 4.6 fps ~ eta: 19s ## Rendering [=====&gt;---------------------------------] at 4.6 fps ~ eta: 18s ## Rendering [======&gt;--------------------------------] at 4.6 fps ~ eta: 18s ## Rendering [=======&gt;-------------------------------] at 4.6 fps ~ eta: 17s ## Rendering [========&gt;------------------------------] at 4.6 fps ~ eta: 17s ## Rendering [=========&gt;-----------------------------] at 4.6 fps ~ eta: 16s ## Rendering [==========&gt;----------------------------] at 4.6 fps ~ eta: 16s ## Rendering [==========&gt;----------------------------] at 4.6 fps ~ eta: 15s ## Rendering [===========&gt;---------------------------] at 4.6 fps ~ eta: 15s ## Rendering [============&gt;--------------------------] at 4.6 fps ~ eta: 15s ## Rendering [============&gt;--------------------------] at 4.6 fps ~ eta: 14s ## Rendering [=============&gt;-------------------------] at 4.6 fps ~ eta: 14s ## Rendering [==============&gt;------------------------] at 4.6 fps ~ eta: 14s ## Rendering [==============&gt;------------------------] at 4.6 fps ~ eta: 13s ## Rendering [===============&gt;-----------------------] at 4.6 fps ~ eta: 13s ## Rendering [================&gt;----------------------] at 4.6 fps ~ eta: 12s ## Rendering [=================&gt;---------------------] at 4.6 fps ~ eta: 12s ## Rendering [==================&gt;--------------------] at 4.6 fps ~ eta: 11s ## Rendering [===================&gt;-------------------] at 4.6 fps ~ eta: 11s ## Rendering [====================&gt;------------------] at 4.6 fps ~ eta: 10s ## Rendering [====================&gt;------------------] at 4.5 fps ~ eta: 10s ## Rendering [=====================&gt;-----------------] at 4.5 fps ~ eta: 10s ## Rendering [=====================&gt;-----------------] at 4.5 fps ~ eta: 9s ## Rendering [======================&gt;----------------] at 4.5 fps ~ eta: 9s ## Rendering [=======================&gt;---------------] at 4.5 fps ~ eta: 9s ## Rendering [=======================&gt;---------------] at 4.5 fps ~ eta: 8s ## Rendering [========================&gt;--------------] at 4.5 fps ~ eta: 8s ## Rendering [=========================&gt;-------------] at 4.5 fps ~ eta: 8s ## Rendering [=========================&gt;-------------] at 4.5 fps ~ eta: 7s ## Rendering [==========================&gt;------------] at 4.5 fps ~ eta: 7s ## Rendering [===========================&gt;-----------] at 4.5 fps ~ eta: 6s ## Rendering [============================&gt;----------] at 4.5 fps ~ eta: 6s ## Rendering [=============================&gt;---------] at 4.5 fps ~ eta: 5s ## Rendering [==============================&gt;--------] at 4.5 fps ~ eta: 5s ## Rendering [==============================&gt;--------] at 4.5 fps ~ eta: 4s ## Rendering [===============================&gt;-------] at 4.5 fps ~ eta: 4s ## Rendering [================================&gt;------] at 4.5 fps ~ eta: 4s ## Rendering [================================&gt;------] at 4.5 fps ~ eta: 3s ## Rendering [=================================&gt;-----] at 4.5 fps ~ eta: 3s ## Rendering [=================================&gt;-----] at 4.4 fps ~ eta: 3s ## Rendering [==================================&gt;----] at 4.4 fps ~ eta: 3s ## Rendering [==================================&gt;----] at 4.4 fps ~ eta: 2s ## Rendering [===================================&gt;---] at 4.4 fps ~ eta: 2s ## Rendering [===================================&gt;---] at 4.3 fps ~ eta: 2s ## Rendering [====================================&gt;--] at 4.3 fps ~ eta: 1s ## Rendering [=====================================&gt;-] at 4.3 fps ~ eta: 1s ## Rendering [=====================================&gt;-] at 4.3 fps ~ eta: 0s ## Rendering [======================================&gt;] at 4.3 fps ~ eta: 0s ## Rendering [=======================================] at 4.3 fps ~ eta: 0s # anim_save(filename = &quot;figures/life_gdp_animation.gif&quot;) # to save the animation This takes a while to run but it’s worth the wait. The plot shows the relationship between GDP per capita (on the x-axis) and life expectancy (on the y-axis) changes across different years for the countries of different continents. The size of each dot represents the population size of the respective country. And different countries are shown in different colors. This animation is not super useful yet in that we don’t know which continents and countries the different dots represent. I’ve added a label to the United States, China, and India. Note how little is required to define the gganimate-specific information! The {frame_time} variable changes the title for each frame. The transition_time() variable is set to year, and the kind of transition is set as ‘linear’ in ease_aes(). I’ve saved the animation as a gif in the figures folder. We won’t have time to go into more detail here but I encourage you to play around with gganimate. It’s fun, looks cool, and (if done well) makes for a great slide in your next presentation! 3.9 Shiny apps The package shiny makes it relatively easy to create interactive plots that can be hosted online. Here is a gallery with some examples. 3.10 Defining snippets Often, we want to create similar plots over and over again. One way to achieve this is by finding the original plot, copy and pasting it, and changing the bits that need changing. Another more flexible and faster way to do this is by using snippets. Snippets are short pieces of code that Here are some snippets I use: snippet snbar ggplot(data = ${1:data}, mapping = aes(x = ${2:x}, y = ${3:y})) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;) snippet sngg ggplot(data = ${1:data}, mapping = aes(${2:aes})) + ${0} snippet sndf ${1:data} = ${1:data} %&gt;% ${0} To make a bar plot, I now only need to type snbar and then hit TAB to activate the snippet. I can then cycle through the bits in the code that are marked with ${Number:word} by hitting TAB again. In RStudio, you can change and add snippets by going to Tools –&gt; Global Options… –&gt; Code –&gt; Edit Snippets. Make sure to set the tick mark in front of Enable Code Snippets (see Figure ??). ). include_graphics(&quot;figures/snippets.png&quot;) Figure 3.5: Enable code snippets. To edit code snippets faster, run this command from the usethis package. Make sure to install the package first if you don’t have it yet. # install.packages(&quot;usethis&quot;) usethis::edit_rstudio_snippets() This command opens up a separate tab in RStudio called r.snippets so that you can make new snippets and adapt old ones more quickly. Take a look at the snippets that RStudio already comes with. And then, make some new ones! By using snippets you will be able to avoid typing the same code over and over again, and you won’t have to memorize as much, too. 3.11 Additional resources 3.11.1 Cheatsheets shiny –&gt; interactive plots 3.11.2 Data camp courses ggplot2 course 3 shiny 1 shiny 2 3.11.3 Books and chapters R for Data Science book Data visualization Graphics for communication Data Visualization – A practical introduction (by Kieran Healy) Refine your plots 3.11.4 Misc ggplot2 extensions –&gt; gallery of ggplot2 extension packages ggplot2 gui –&gt; ggplot2 extension package ggplot2 visualizations with code –&gt; gallery of plots with code Color brewer –&gt; for finding colors shiny apps examples –&gt; shiny apps examples that focus on statistics teaching (made by students at PennState) "],
["data-wrangling-1.html", "Chapter 4 Data wrangling 1 4.1 Learning objectives 4.2 Install packages 4.3 Load packages 4.4 Some R basics 4.5 Looking at data 4.6 Wrangling data 4.7 Additional resources", " Chapter 4 Data wrangling 1 In this lecture, we will take a look at how to wrangle data using the dplyr package. Again, getting our data into shape is something we’ll need to do throughout the course, so it’s worth spending some time getting a good sense for how this works. The nice thing about R is that (thanks to the tidyverse), both visualization and data wrangling are particularly powerful. 4.1 Learning objectives Review R basics (incl. variable modes, data types, operators, control flow, and functions). Learn how the pipe operator %&gt;% works. See different ways for getting a sense of one’s data. Master key data manipulation verbs from the dplyr package (incl. filter(), rename(), select(), mutate(), and arrange()) 4.2 Install packages install.packages(c(&quot;skimr&quot;, &quot;visdat&quot;, &quot;summarytools&quot;, &quot;DT&quot;)) 4.3 Load packages Let’s first load the packages that we need for this chapter. library(&quot;knitr&quot;) # for rendering the RMarkdown file library(&quot;tidyverse&quot;) # for data wrangling 4.4 Some R basics To test your knowledge of the R basics, I recommend taking the free interactive tutorial on datacamp: Introduction to R. Here, I will just give a very quick overview of some of the basics. 4.4.1 Modes Variables in R can have different modes. Table 4.1 shows the most common ones. Table 4.1: Most commonly used variable modes in R. name example numeric 1, 3, 48 character 'Steve', 'a', '78' logical TRUE, FALSE not available NA For characters you can either use \" or '. R has a number of functions to convert a variable from one mode to another. NA is used for missing values. tmp1 = &quot;1&quot; # we start with a character str(tmp1) ## chr &quot;1&quot; tmp2 = as.numeric(tmp1) # turn it into a numeric str(tmp2) ## num 1 tmp3 = as.factor(tmp2) # turn that into a factor str(tmp3) ## Factor w/ 1 level &quot;1&quot;: 1 tmp4 = as.character(tmp3) # and go full cycle by turning it back into a character str(tmp4) ## chr &quot;1&quot; identical(tmp1, tmp4) # checks whether tmp1 and tmp4 are the same ## [1] TRUE The str() function displays the structure of an R object. Here, it shows us what mode the variable is. 4.4.2 Data types R has a number of different data types. Table 4.2 shows the ones you’re most likely to come across (taken from this source): Table 4.2: Most commonly used data types in R. name description vector list of values with of the same variable mode factor for ordinal variables matrix 2D data structure array same as matrix for higher dimensional data data frame similar to matrix but with column names list flexible type that can contain different other variable types 4.4.2.1 Vectors We build vectors using the concatenate function c(), and we use [] to access one or more elements of a vector. numbers = c(1, 4, 5) # make a vector numbers[2] # access the second element ## [1] 4 numbers[1:2] # access the first two elements ## [1] 1 4 numbers[c(1, 3)] # access the first and last element ## [1] 1 5 In R (unlike in Python for example), 1 refers to the first element of a vector (or list). 4.4.2.2 Matrix We build a matrix using the matrix() function, and we use [] to access its elements. matrix = matrix(data = c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2) matrix # the full matrix ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 matrix[1, 2] # element in row 1, column 2 ## [1] 4 matrix[1, ] # all elements in the first row ## [1] 1 4 matrix[ , 1] # all elements in the first column ## [1] 1 2 3 matrix[-1, ] # a matrix which excludes the first row ## [,1] [,2] ## [1,] 2 5 ## [2,] 3 6 Note how we use an empty placeholder to indicate that we want to select all the values in a row or column, and - to indicate that we want to remove something. 4.4.2.3 Array Arrays work the same was as matrices with data of more than two dimensions. 4.4.2.4 Data frame df = tibble(participant_id = c(1, 2, 3), participant_name = c(&quot;Leia&quot;, &quot;Luke&quot;, &quot;Darth&quot;)) # make the data frame df # the complete data frame ## # A tibble: 3 x 2 ## participant_id participant_name ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 Leia ## 2 2 Luke ## 3 3 Darth df[1, 2] # a single element using numbers ## # A tibble: 1 x 1 ## participant_name ## &lt;chr&gt; ## 1 Leia df$participant_id # all participants ## [1] 1 2 3 df[[&quot;participant_id&quot;]] # same as before but using [[]] instead of $ ## [1] 1 2 3 df$participant_name[2] # name of the second participant ## [1] &quot;Luke&quot; df[[&quot;participant_name&quot;]][2] # same as above ## [1] &quot;Luke&quot; We’ll use data frames a lot. Data frames are like a matrix with column names. Data frames are also more general than matrices in that different columns can have different modes. For example, one column might be a character, another one numeric, and another one a factor. Here we used the tibble() function to create the data frame. A tibble is almost the same as a data frame but it has better defaults for formatting output in the console (more information on tibbles is here). 4.4.2.5 Lists l.mixed = list(number = 1, character = &quot;2&quot;, factor = factor(3), matrix = matrix(1:4, ncol = 2), df = tibble(x = c(1, 2), y = c(3, 4))) l.mixed ## $number ## [1] 1 ## ## $character ## [1] &quot;2&quot; ## ## $factor ## [1] 3 ## Levels: 3 ## ## $matrix ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## $df ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 ## 2 2 4 # three different ways of accessing a list l.mixed$character ## [1] &quot;2&quot; l.mixed[[&#39;character&#39;]] ## [1] &quot;2&quot; l.mixed[[2]] ## [1] &quot;2&quot; Lists are a very flexible data format. You can put almost anything in a list. 4.4.3 Operators Table 4.3 shows the comparison operators that result in logical outputs. Table 4.3: Table of comparison operators that result in boolean (TRUE/FALSE) outputs. symbol name == equal to != not equal to &gt;, &lt; greater/less than &gt;=, &lt;= greater/less than or equal &amp;, |, ! logical operators: and, or, not %in% checks whether an element is in an object 4.4.4 Control flow 4.4.4.1 if-then number = 3 if(number == 1){ print(&quot;The number is 1.&quot;) }else if (number == 2){ print(&quot;The number is 2.&quot;) }else{ print(&quot;The number is neither 1 nor 2.&quot;) } ## [1] &quot;The number is neither 1 nor 2.&quot; As a shorthand version, we can also use the ifelse() function like so: number = 3 ifelse(test = number == 1, yes = &quot;correct&quot;, no = &quot;false&quot;) ## [1] &quot;false&quot; 4.4.4.2 for loop sequence = 1:10 for(i in 1:length(sequence)){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 4.4.4.3 while loop number = 1 while(number &lt;= 10){ print(number) number = number + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 4.4.5 Functions fun.add_two_numbers = function(a, b){ x = a + b return(str_c(&quot;The result is &quot;, x)) } fun.add_two_numbers(1,2) ## [1] &quot;The result is 3&quot; I’ve used the str_c() function here to concatenate the string with the number. (R converts the number x into a string for us.) Note, R functions can only return a single object. However, this object can be a list (which can contain anything). 4.4.5.1 Some often used functions Table 4.4: Some frequently used functions. name description length() length of an object dim() dimensions of an object (e.g. number of rows and columns) rm() remove an object seq() generate a sequence of numbers rep() repeat something n times max() maximum min() minimum which.max() index of the maximum which.min() index of the maximum mean() mean median() median sum() sum var() variance sd() standard deviation 4.4.6 The pipe operator %&gt;% Figure 4.1: Inspiration for the magrittr package name. Figure 4.2: The magrittr package logo. The pipe operator %&gt;% is a special operator introduced in the magrittr package. It is used heavily in the tidyverse. The basic idea is simple: this operator allows us to “pipe” several functions into one long chain that matches the order in which we want to do stuff. Abstractly, the pipe operator does the following: f(x) can be rewritten as x %&gt;% f() For example, in standard R, we would write: x = 1:3 # standard R sum(x) ## [1] 6 With the pipe, we can rewrite this as: x = 1:3 # with the pipe x %&gt;% sum() ## [1] 6 This doesn’t seem super useful yet, but just hold on a little longer. f(x, y) can be rewritten as x %&gt;% f(y) So, we could rewrite the following standard R code … # rounding pi to 6 digits, standard R round(pi, digits = 6) ## [1] 3.141593 … by using the pipe: # rounding pi to 6 digits, standard R pi %&gt;% round(digits = 6) ## [1] 3.141593 Here is another example: a = 3 b = 4 sum(a, b) # standard way ## [1] 7 a %&gt;% sum(b) # the pipe way ## [1] 7 The pipe operator inserts the result of the previous computation as a first element into the next computation. So, a %&gt;% sum(b) is equivalent to sum(a, b). We can also specify to insert the result at a different position via the . operator. For example: a = 1 b = 10 b %&gt;% seq(from = a, to = .) ## [1] 1 2 3 4 5 6 7 8 9 10 Here, I used the . operator to specify that I woud like to insert the result of b where I’ve put the . in the seq() function. f(x, y) can be rewritten as y %&gt;% f(x, .) Still not to thrilled about the pipe? We can keep going though (and I’m sure you’ll be convinced eventually.) h(g(f(x))) can be rewritten as x %&gt;% f() %&gt;% g() %&gt;% h() For example, consider that we want to calculate the root mean squared error (RMSE) between prediction and data. Here is how the RMSE is defined: \\[ \\text{RMSE} = \\sqrt\\frac{\\sum_{i=1}^n(\\hat{y}_i-y_i)^2}{n} \\] where \\(\\hat{y}_i\\) denotes the prediction, and \\(y_i\\) the actually observed value. In base R, we would do the following. data = c(1, 3, 4, 2, 5) prediction = c(1, 2, 2, 1, 4) # calculate root mean squared error rmse = sqrt(mean((prediction-data)^2)) print(rmse) ## [1] 1.183216 Using the pipe operator makes the operation more intuitive: data = c(1, 3, 4, 2, 5) prediction = c(1, 2, 2, 1, 4) # calculate root mean squared error the pipe way rmse = (prediction-data)^2 %&gt;% mean() %&gt;% sqrt() %&gt;% print() ## [1] 1.183216 First, we calculate the squared error, then we take the mean, then the square root, and then print the result. The pipe operator %&gt;% is similar to the + used in ggplot2. It allows us to take step-by-step actions in a way that fits the causal ordering of how we want to do things. Tip: The keyboard shortcut for the pipe operator is: cmd/ctrl + shift + m Definitely learn this one – we’ll use the pipe a lot!! Tip: Code is generally easier to read when the pipe %&gt;% is at the end of a line (just like the + in ggplot2). A key advantage of using the pipe is that you don’t have to save intermediate computations as new variables and this help to keep your environment nice and clean! 4.4.6.1 Practice 1 Let’s practice the pipe operator. # some numbers x = seq(from = 1, to = 5, by = 1) # standard way log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 # the pipe way (write your code underneath) # standard way mean(round(sqrt(x), digits = 2)) ## [1] 1.676 # the pipe way (write your code underneath) 4.5 Looking at data The package dplyr which we loaded as part of the tidyverse, includes a data set with information about starwars characters. Let’s store this as df.starwars. df.starwars = starwars Note: Unlike in other languages (such as Python or Matlab), a . in a variable name has no special meaning and can just be used as part of the name. I’ve used df here to indicate for myself that this variable is a data frame. Before visualizing the data, it’s often useful to take a quick direct look at the data. There are several ways of taking a look at data in R. Personally, I like to look at the data within RStudio’s data viewer. To do so, you can: click on the df.starwars variable in the “Environment” tab type View(df.starwars) in the console move your mouse over (or select) the variable in the editor (or console) and hit F2 I like the F2 route the best as it’s fast and flexible. Sometimes it’s also helpful to look at data in the console instead of the data viewer. Particularly when the data is very large, the data viewer can be sluggish. Here are some useful functions: 4.5.1 head() Without any extra arguments specified, head() shows the top six rows of the data. head(df.starwars) ## # A tibble: 6 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Leia… 150 49 brown light brown 19 female ## 6 Owen… 178 120 brown, gr… light blue 52 male ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; 4.5.2 glimpse() glimpse() is helpful when the data frame has many columns. The data is shown in a transposed way with columns as rows. glimpse(df.starwars) ## Observations: 87 ## Variables: 13 ## $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Darth Vader&quot;, &quot;L… ## $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, … ## $ mass &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.… ## $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brown, grey&quot;, &quot;bro… ## $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;, &quot;light&quot;, &quot;lig… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;brown&quot;, &quot;blue&quot;, &quot;… ## $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, … ## $ gender &lt;chr&gt; &quot;male&quot;, NA, NA, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, N… ## $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatooine&quot;, &quot;Alderaa… ## $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;,… ## $ films &lt;list&gt; [&lt;&quot;Revenge of the Sith&quot;, &quot;Return of the Jedi&quot;, &quot;The … ## $ vehicles &lt;list&gt; [&lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike&quot;&gt;, &lt;&gt;, &lt;&gt;, &lt;… ## $ starships &lt;list&gt; [&lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;, &quot;TIE Advanc… 4.5.3 distinct() distinct() shows all the distinct values for a character or factor column. df.starwars %&gt;% distinct(name) ## # A tibble: 87 x 1 ## name ## &lt;chr&gt; ## 1 Luke Skywalker ## 2 C-3PO ## 3 R2-D2 ## 4 Darth Vader ## 5 Leia Organa ## 6 Owen Lars ## 7 Beru Whitesun lars ## 8 R5-D4 ## 9 Biggs Darklighter ## 10 Obi-Wan Kenobi ## # … with 77 more rows 4.5.4 count() count() shows a count of all the different distinct values in a column. df.starwars %&gt;% count(gender) ## # A tibble: 5 x 2 ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 3 ## 2 female 19 ## 3 hermaphrodite 1 ## 4 male 62 ## 5 none 2 It’s possible to do grouped counts by combining several variables. df.starwars %&gt;% count(species, gender) %&gt;% head(n = 10) ## # A tibble: 10 x 3 ## species gender n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; female 3 ## 2 &lt;NA&gt; male 2 ## 3 Aleena male 1 ## 4 Besalisk male 1 ## 5 Cerean male 1 ## 6 Chagrian male 1 ## 7 Clawdite female 1 ## 8 Droid &lt;NA&gt; 3 ## 9 Droid none 2 ## 10 Dug male 1 4.5.5 datatable() For RMardkown files specifically, we can use the datatable() function from the DT package to get an interactive table widget. df.starwars %&gt;% DT::datatable() 4.5.6 Other tools for taking a quick look at data 4.5.6.1 vis_dat() The vis_dat() function from the visdat package, gives a visual summary that makes it easy to see the variable types and whether there are missing values in the data. visdat::vis_dat(df.starwars) When R loads packages, functions loaded in earlier packages are overwritten by functions of the same name from later packages. This means that the order in which packages are loaded matters. To make sure that a function from the correct package is used, you can use the package_name::function_name() construction. This way, the function_name() from the package_name is used, rather than the same function from a different package. This is why, in general, I recommend to load the tidyverse package last (since it contains a large number of functions that we use a lot). 4.5.6.2 skim() The skim() function from the skimr package provides a nice overview of the data, separated by variable types. # install.packages(&quot;skimr&quot;) skimr::skim(df.starwars) ## Skim summary statistics ## n obs: 87 ## n variables: 13 ## ## ── Variable type:character ──────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## eye_color 0 87 87 3 13 0 15 ## gender 3 84 87 4 13 0 4 ## hair_color 5 82 87 4 13 0 12 ## homeworld 10 77 87 4 14 0 48 ## name 0 87 87 3 21 0 87 ## skin_color 0 87 87 3 19 0 31 ## species 5 82 87 3 14 0 37 ## ## ── Variable type:integer ────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 p75 p100 hist ## height 6 81 87 174.36 34.77 66 167 180 191 264 ▁▁▁▂▇▃▁▁ ## ## ── Variable type:list ───────────────────────────────────────────────────── ## variable missing complete n n_unique min_length median_length ## films 0 87 87 24 1 1 ## starships 0 87 87 17 0 0 ## vehicles 0 87 87 11 0 0 ## max_length ## 7 ## 5 ## 2 ## ## ── Variable type:numeric ────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 p75 p100 ## birth_year 44 43 87 87.57 154.69 8 35 52 72 896 ## mass 28 59 87 97.31 169.46 15 55.6 79 84.5 1358 ## hist ## ▇▁▁▁▁▁▁▁ ## ▇▁▁▁▁▁▁▁ 4.5.6.3 dfSummary() The summarytools package is another great package for taking a look at the data. It renders a nice html output for the data frame including a lot of helpful information. You can find out more about this package here. df.starwars %&gt;% select_if(negate(is.list)) %&gt;% # this removes all list columns (we&#39;ll learn about this later) summarytools::dfSummary() %&gt;% summarytools::view() Note: The summarytools::view() function will not show up here in the html. It generates a summary of the data that is displayed in the Viewer in RStudio. Once we’ve taken a look at the data, the next step would be to visualize relationships between variables of interest. 4.5.7 A quick note on naming things Personally, I like to name things in a (pretty) consistent way so that I have no trouble finding stuff even when I open up a project that I haven’t worked on for a while. I try to use the following naming conventions: Table 4.5: Some naming conventions I adopt to make my life easier. name use df.thing for data frames l.thing for lists fun.thing for functions tmp.thing for temporary variables 4.6 Wrangling data We use the functions in the package dplyr to manipulate our data. 4.6.1 filter() filter() lets us apply logical (and other) operators (see Table 4.3) to subset the data. Here, I’ve filtered out the male characters. df.starwars %&gt;% filter(gender == &#39;male&#39;) ## # A tibble: 62 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 Dart… 202 136 none white yellow 41.9 male ## 3 Owen… 178 120 brown, gr… light blue 52 male ## 4 Bigg… 183 84 black light brown 24 male ## 5 Obi-… 182 77 auburn, w… fair blue-gray 57 male ## 6 Anak… 188 84 blond fair blue 41.9 male ## 7 Wilh… 180 NA auburn, g… fair blue 64 male ## 8 Chew… 228 112 brown unknown blue 200 male ## 9 Han … 180 80 brown fair brown 29 male ## 10 Gree… 173 74 &lt;NA&gt; green black 44 male ## # … with 52 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; We can combine multiple conditions in the same call. Here, I’ve filtered out male characters, whose height is greater than the median height (i.e. they are in the top 50 percentile), and whose mass was not NA. df.starwars %&gt;% filter(gender == &#39;male&#39;, height &gt; median(height, na.rm = T), !is.na(mass)) ## # A tibble: 26 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Dart… 202 136 none white yellow 41.9 male ## 2 Bigg… 183 84 black light brown 24 male ## 3 Obi-… 182 77 auburn, w… fair blue-gray 57 male ## 4 Anak… 188 84 blond fair blue 41.9 male ## 5 Chew… 228 112 brown unknown blue 200 male ## 6 Boba… 183 78.2 black fair brown 31.5 male ## 7 Bossk 190 113 none green red 53 male ## 8 Qui-… 193 89 brown fair blue 92 male ## 9 Nute… 191 90 none mottled g… red NA male ## 10 Jar … 196 66 none orange orange 52 male ## # … with 16 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Many functions like mean(), median(), var(), sd(), sum() have the argument na.rm which is set to FALSE by default. I set the argument to TRUE here (or T for short), which means that the NA values are ignored, and the median() is calculated based on the remaning values. You can use , and &amp; interchangeably in filter(). Make sure to use parentheses when combining several logical operators to indicate which logical operation should be performed first: df.starwars %&gt;% filter((skin_color %in% c(&quot;dark&quot;, &quot;pale&quot;) | gender == &quot;hermaphrodite&quot;) &amp; height &gt; 170) ## # A tibble: 10 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabb… 175 1358 &lt;NA&gt; green-tan… orange 600 herma… ## 2 Land… 177 79 black dark brown 31 male ## 3 Quar… 183 NA black dark brown 62 male ## 4 Bib … 180 NA none pale pink NA male ## 5 Mace… 188 84 none dark brown 72 male ## 6 Ki-A… 198 82 white pale yellow 92 male ## 7 Adi … 184 50 none dark blue NA female ## 8 Saes… 188 NA none pale orange NA male ## 9 Greg… 185 85 black dark brown NA male ## 10 Sly … 178 48 none pale white NA female ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; The starwars characters that have either a \"dark\" or a \"pale\" skin tone, or whose gender is \"hermaphrodite\", and whose height is at least 170 cm. The %in% operator is useful when there are multiple options. Instead of skin_color %in% c(\"dark\", \"pale\"), I could have also written skin_color == \"dark\" | skin_color == \"pale\" but this gets cumbersome as the number of options increases. 4.6.2 rename() rename() renames column names. df.starwars %&gt;% rename(person = name, mass_kg = mass) ## # A tibble: 87 x 13 ## person height mass_kg hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke … 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Darth… 202 136 none white yellow 41.9 male ## 5 Leia … 150 49 brown light brown 19 female ## 6 Owen … 178 120 brown, gr… light blue 52 male ## 7 Beru … 165 75 brown light blue 47 female ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 9 Biggs… 183 84 black light brown 24 male ## 10 Obi-W… 182 77 auburn, w… fair blue-gray 57 male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; The new variable names goes on the LHS of the= sign, and the old name on the RHS. To rename all variables at the same time use set_names(): df.starwars %&gt;% set_names(letters[1:ncol(.)]) # renamed all variables to letters: a, b, ... ## # A tibble: 87 x 13 ## a b c d e f g h i j k l ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; ## 1 Luke … 172 77 blond fair blue 19 male Tato… Human &lt;chr… &lt;chr… ## 2 C-3PO 167 75 &lt;NA&gt; gold yell… 112 &lt;NA&gt; Tato… Droid &lt;chr… &lt;chr… ## 3 R2-D2 96 32 &lt;NA&gt; whit… red 33 &lt;NA&gt; Naboo Droid &lt;chr… &lt;chr… ## 4 Darth… 202 136 none white yell… 41.9 male Tato… Human &lt;chr… &lt;chr… ## 5 Leia … 150 49 brown light brown 19 fema… Alde… Human &lt;chr… &lt;chr… ## 6 Owen … 178 120 brow… light blue 52 male Tato… Human &lt;chr… &lt;chr… ## 7 Beru … 165 75 brown light blue 47 fema… Tato… Human &lt;chr… &lt;chr… ## 8 R5-D4 97 32 &lt;NA&gt; whit… red NA &lt;NA&gt; Tato… Droid &lt;chr… &lt;chr… ## 9 Biggs… 183 84 black light brown 24 male Tato… Human &lt;chr… &lt;chr… ## 10 Obi-W… 182 77 aubu… fair blue… 57 male Stew… Human &lt;chr… &lt;chr… ## # … with 77 more rows, and 1 more variable: m &lt;list&gt; 4.6.3 select() select() allows us to select a subset of the columns in the data frame. df.starwars %&gt;% select(name, height, mass) ## # A tibble: 87 x 3 ## name height mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 ## 2 C-3PO 167 75 ## 3 R2-D2 96 32 ## 4 Darth Vader 202 136 ## 5 Leia Organa 150 49 ## 6 Owen Lars 178 120 ## 7 Beru Whitesun lars 165 75 ## 8 R5-D4 97 32 ## 9 Biggs Darklighter 183 84 ## 10 Obi-Wan Kenobi 182 77 ## # … with 77 more rows We can select multiple columns using the (from:to) syntax: df.starwars %&gt;% select(name:birth_year) # from name to birth_year ## # A tibble: 87 x 7 ## name height mass hair_color skin_color eye_color birth_year ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 blond fair blue 19 ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 ## 4 Darth Vader 202 136 none white yellow 41.9 ## 5 Leia Organa 150 49 brown light brown 19 ## 6 Owen Lars 178 120 brown, grey light blue 52 ## 7 Beru Whitesun… 165 75 brown light blue 47 ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA ## 9 Biggs Darklig… 183 84 black light brown 24 ## 10 Obi-Wan Kenobi 182 77 auburn, whi… fair blue-gray 57 ## # … with 77 more rows Or use a variable for column selection: columns = c(&quot;name&quot;, &quot;height&quot;, &quot;species&quot;) df.starwars %&gt;% select(one_of(columns)) # useful when using a variable for column selection ## # A tibble: 87 x 3 ## name height species ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Luke Skywalker 172 Human ## 2 C-3PO 167 Droid ## 3 R2-D2 96 Droid ## 4 Darth Vader 202 Human ## 5 Leia Organa 150 Human ## 6 Owen Lars 178 Human ## 7 Beru Whitesun lars 165 Human ## 8 R5-D4 97 Droid ## 9 Biggs Darklighter 183 Human ## 10 Obi-Wan Kenobi 182 Human ## # … with 77 more rows We can also deselect (multiple) columns: df.starwars %&gt;% select(-name, -(birth_year:vehicles)) ## # A tibble: 87 x 6 ## height mass hair_color skin_color eye_color starships ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 172 77 blond fair blue &lt;chr [2]&gt; ## 2 167 75 &lt;NA&gt; gold yellow &lt;chr [0]&gt; ## 3 96 32 &lt;NA&gt; white, blue red &lt;chr [0]&gt; ## 4 202 136 none white yellow &lt;chr [1]&gt; ## 5 150 49 brown light brown &lt;chr [0]&gt; ## 6 178 120 brown, grey light blue &lt;chr [0]&gt; ## 7 165 75 brown light blue &lt;chr [0]&gt; ## 8 97 32 &lt;NA&gt; white, red red &lt;chr [0]&gt; ## 9 183 84 black light brown &lt;chr [1]&gt; ## 10 182 77 auburn, white fair blue-gray &lt;chr [5]&gt; ## # … with 77 more rows And select columns by partially matching the column name: df.starwars %&gt;% select(contains(&quot;_&quot;)) # every column that contains the character &quot;_&quot; ## # A tibble: 87 x 4 ## hair_color skin_color eye_color birth_year ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 blond fair blue 19 ## 2 &lt;NA&gt; gold yellow 112 ## 3 &lt;NA&gt; white, blue red 33 ## 4 none white yellow 41.9 ## 5 brown light brown 19 ## 6 brown, grey light blue 52 ## 7 brown light blue 47 ## 8 &lt;NA&gt; white, red red NA ## 9 black light brown 24 ## 10 auburn, white fair blue-gray 57 ## # … with 77 more rows df.starwars %&gt;% select(starts_with(&quot;h&quot;)) # every column that starts with an &quot;h&quot; ## # A tibble: 87 x 3 ## height hair_color homeworld ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 172 blond Tatooine ## 2 167 &lt;NA&gt; Tatooine ## 3 96 &lt;NA&gt; Naboo ## 4 202 none Tatooine ## 5 150 brown Alderaan ## 6 178 brown, grey Tatooine ## 7 165 brown Tatooine ## 8 97 &lt;NA&gt; Tatooine ## 9 183 black Tatooine ## 10 182 auburn, white Stewjon ## # … with 77 more rows We can also use select() to reorder the columns: # useful trick for changing the column order, now eye_color is at the beginning df.starwars %&gt;% select(eye_color, everything()) ## # A tibble: 87 x 13 ## eye_color name height mass hair_color skin_color birth_year gender ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 blue Luke… 172 77 blond fair 19 male ## 2 yellow C-3PO 167 75 &lt;NA&gt; gold 112 &lt;NA&gt; ## 3 red R2-D2 96 32 &lt;NA&gt; white, bl… 33 &lt;NA&gt; ## 4 yellow Dart… 202 136 none white 41.9 male ## 5 brown Leia… 150 49 brown light 19 female ## 6 blue Owen… 178 120 brown, gr… light 52 male ## 7 blue Beru… 165 75 brown light 47 female ## 8 red R5-D4 97 32 &lt;NA&gt; white, red NA &lt;NA&gt; ## 9 brown Bigg… 183 84 black light 24 male ## 10 blue-gray Obi-… 182 77 auburn, w… fair 57 male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Here, I’ve moved the eye_color column to the beginning of the data frame. everything() is a helper function which selects all the columns. df.starwars %&gt;% select(-eye_color, everything(), eye_color) # move eye_color to the end ## # A tibble: 87 x 13 ## name height mass hair_color skin_color birth_year gender homeworld ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair 19 male Tatooine ## 2 C-3PO 167 75 &lt;NA&gt; gold 112 &lt;NA&gt; Tatooine ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… 33 &lt;NA&gt; Naboo ## 4 Dart… 202 136 none white 41.9 male Tatooine ## 5 Leia… 150 49 brown light 19 female Alderaan ## 6 Owen… 178 120 brown, gr… light 52 male Tatooine ## 7 Beru… 165 75 brown light 47 female Tatooine ## 8 R5-D4 97 32 &lt;NA&gt; white, red NA &lt;NA&gt; Tatooine ## 9 Bigg… 183 84 black light 24 male Tatooine ## 10 Obi-… 182 77 auburn, w… fair 57 male Stewjon ## # … with 77 more rows, and 5 more variables: species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt;, eye_color &lt;chr&gt; Here, I’ve moved eye_color to the end. Note that I had to deselect it first. We can select columns based on their data type using select_if(). df.starwars %&gt;% select_if(is.numeric) # just select numeric columns ## # A tibble: 87 x 3 ## height mass birth_year ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 172 77 19 ## 2 167 75 112 ## 3 96 32 33 ## 4 202 136 41.9 ## 5 150 49 19 ## 6 178 120 52 ## 7 165 75 47 ## 8 97 32 NA ## 9 183 84 24 ## 10 182 77 57 ## # … with 77 more rows The following selects all columns that are not numeric: df.starwars %&gt;% select_if(~ !is.numeric(.)) # selects all columns that are not numeric ## # A tibble: 87 x 10 ## name hair_color skin_color eye_color gender homeworld species films ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; ## 1 Luke… blond fair blue male Tatooine Human &lt;chr… ## 2 C-3PO &lt;NA&gt; gold yellow &lt;NA&gt; Tatooine Droid &lt;chr… ## 3 R2-D2 &lt;NA&gt; white, bl… red &lt;NA&gt; Naboo Droid &lt;chr… ## 4 Dart… none white yellow male Tatooine Human &lt;chr… ## 5 Leia… brown light brown female Alderaan Human &lt;chr… ## 6 Owen… brown, gr… light blue male Tatooine Human &lt;chr… ## 7 Beru… brown light blue female Tatooine Human &lt;chr… ## 8 R5-D4 &lt;NA&gt; white, red red &lt;NA&gt; Tatooine Droid &lt;chr… ## 9 Bigg… black light brown male Tatooine Human &lt;chr… ## 10 Obi-… auburn, w… fair blue-gray male Stewjon Human &lt;chr… ## # … with 77 more rows, and 2 more variables: vehicles &lt;list&gt;, ## # starships &lt;list&gt; Note that I used ~ here to indicate that I’m creating an anonymous function to check whether column type is numeric. A one-sided formula (expression beginning with ~) is interpreted as function(x), and wherever x would go in the function is represented by .. So, I could write the same code like so: df.starwars %&gt;% select_if(function(x) !is.numeric(x)) # selects all columns that are not numeric ## # A tibble: 87 x 10 ## name hair_color skin_color eye_color gender homeworld species films ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; ## 1 Luke… blond fair blue male Tatooine Human &lt;chr… ## 2 C-3PO &lt;NA&gt; gold yellow &lt;NA&gt; Tatooine Droid &lt;chr… ## 3 R2-D2 &lt;NA&gt; white, bl… red &lt;NA&gt; Naboo Droid &lt;chr… ## 4 Dart… none white yellow male Tatooine Human &lt;chr… ## 5 Leia… brown light brown female Alderaan Human &lt;chr… ## 6 Owen… brown, gr… light blue male Tatooine Human &lt;chr… ## 7 Beru… brown light blue female Tatooine Human &lt;chr… ## 8 R5-D4 &lt;NA&gt; white, red red &lt;NA&gt; Tatooine Droid &lt;chr… ## 9 Bigg… black light brown male Tatooine Human &lt;chr… ## 10 Obi-… auburn, w… fair blue-gray male Stewjon Human &lt;chr… ## # … with 77 more rows, and 2 more variables: vehicles &lt;list&gt;, ## # starships &lt;list&gt; We can rename some of the columns using select() like so: df.starwars %&gt;% select(person = name, height, mass_kg = mass) ## # A tibble: 87 x 3 ## person height mass_kg ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 ## 2 C-3PO 167 75 ## 3 R2-D2 96 32 ## 4 Darth Vader 202 136 ## 5 Leia Organa 150 49 ## 6 Owen Lars 178 120 ## 7 Beru Whitesun lars 165 75 ## 8 R5-D4 97 32 ## 9 Biggs Darklighter 183 84 ## 10 Obi-Wan Kenobi 182 77 ## # … with 77 more rows For more details, take a look at the help file for select(), and this this great tutorial in which I learned about some of the more advanced ways of using select(). 4.6.4 mutate() mutate() is used to change exisitng columns or make new ones. df.starwars %&gt;% mutate(height = height / 100, # to get height in meters bmi = mass / (height^2)) %&gt;% # bmi = kg / (m^2) select(name, height, mass, bmi) ## # A tibble: 87 x 4 ## name height mass bmi ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 1.72 77 26.0 ## 2 C-3PO 1.67 75 26.9 ## 3 R2-D2 0.96 32 34.7 ## 4 Darth Vader 2.02 136 33.3 ## 5 Leia Organa 1.5 49 21.8 ## 6 Owen Lars 1.78 120 37.9 ## 7 Beru Whitesun lars 1.65 75 27.5 ## 8 R5-D4 0.97 32 34.0 ## 9 Biggs Darklighter 1.83 84 25.1 ## 10 Obi-Wan Kenobi 1.82 77 23.2 ## # … with 77 more rows Here, I’ve calculated the bmi for the different starwars characters. I first mutated the height variable by going from cm to m, and then created the new column “bmi”. A useful helper function for mutate() is ifelse() which is a shorthand for the if-else control flow (Section 4.4.4.1). Here is an example: df.starwars %&gt;% mutate(height_categorical = ifelse(height &gt; median(height, na.rm = T), &#39;tall&#39;, &#39;short&#39;)) %&gt;% select(name, contains(&quot;height&quot;)) ## # A tibble: 87 x 3 ## name height height_categorical ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Luke Skywalker 172 short ## 2 C-3PO 167 short ## 3 R2-D2 96 short ## 4 Darth Vader 202 tall ## 5 Leia Organa 150 short ## 6 Owen Lars 178 short ## 7 Beru Whitesun lars 165 short ## 8 R5-D4 97 short ## 9 Biggs Darklighter 183 tall ## 10 Obi-Wan Kenobi 182 tall ## # … with 77 more rows ifelse() works in the following way: we first specify the condition, then what should be returned if the condition is true, and finally what should be returned otherwise. The more verbose version of the statement above would be: ifelse(test = height &gt; median(height, na.rm = T), yes = 'tall', no = 'short') There are a number of variants of the mutate() function. Let’s take a look at them. 4.6.4.1 mutate_at() With mutate_at(), we can mutate several columns at the same time. df.starwars %&gt;% mutate_at(.vars = vars(height, mass, birth_year), .funs = &quot;scale&quot;) ## # A tibble: 87 x 13 ## name height[,1] mass[,1] hair_color skin_color eye_color birth_year[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Luke… -0.0678 -0.120 blond fair blue -0.443 ## 2 C-3PO -0.212 -0.132 &lt;NA&gt; gold yellow 0.158 ## 3 R2-D2 -2.25 -0.385 &lt;NA&gt; white, bl… red -0.353 ## 4 Dart… 0.795 0.228 none white yellow -0.295 ## 5 Leia… -0.701 -0.285 brown light brown -0.443 ## 6 Owen… 0.105 0.134 brown, gr… light blue -0.230 ## 7 Beru… -0.269 -0.132 brown light blue -0.262 ## 8 R5-D4 -2.22 -0.385 &lt;NA&gt; white, red red NA ## 9 Bigg… 0.249 -0.0786 black light brown -0.411 ## 10 Obi-… 0.220 -0.120 auburn, w… fair blue-gray -0.198 ## # … with 77 more rows, and 6 more variables: gender &lt;chr&gt;, ## # homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, ## # starships &lt;list&gt; In vars() I’ve specified what variables to mutate, I’ve passed the function name \"scale\" to the .funs argument. Here, I’ve z-scored height, mass, and birth_year using the scale() function. Note that I wrote the function without (). The .funs argument expects a list of functions that can be specified by: their name, “mean” the function itself, mean a call to the function with . as a dummy argument, ~ mean(.) (note the ~ before the function call). Within vars(), we can use the same helper functions for selecting columns that we’ve seen above for select(). We can also use names to create new columns: df.starwars %&gt;% mutate_at(vars(height, mass, birth_year), .funs = list(z = &quot;scale&quot;)) %&gt;% select(name, contains(&quot;height&quot;), contains(&quot;mass&quot;), contains(&quot;birth_year&quot;)) ## # A tibble: 87 x 7 ## name height height_z[,1] mass mass_z[,1] birth_year birth_year_z[,1] ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke S… 172 -0.0678 77 -0.120 19 -0.443 ## 2 C-3PO 167 -0.212 75 -0.132 112 0.158 ## 3 R2-D2 96 -2.25 32 -0.385 33 -0.353 ## 4 Darth … 202 0.795 136 0.228 41.9 -0.295 ## 5 Leia O… 150 -0.701 49 -0.285 19 -0.443 ## 6 Owen L… 178 0.105 120 0.134 52 -0.230 ## 7 Beru W… 165 -0.269 75 -0.132 47 -0.262 ## 8 R5-D4 97 -2.22 32 -0.385 NA NA ## 9 Biggs … 183 0.249 84 -0.0786 24 -0.411 ## 10 Obi-Wa… 182 0.220 77 -0.120 57 -0.198 ## # … with 77 more rows As we can see, new columns were created with _z added to the end of the column name. And we can apply several functions at the same time. df.starwars %&gt;% mutate_at(vars(height, mass, birth_year), list(z = &quot;scale&quot;, centered = ~ scale(., scale = FALSE))) %&gt;% select(name, contains(&quot;height&quot;), contains(&quot;mass&quot;), contains(&quot;birth_year&quot;)) ## # A tibble: 87 x 10 ## name height height_z[,1] height_centered… mass mass_z[,1] ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke… 172 -0.0678 -2.36 77 -0.120 ## 2 C-3PO 167 -0.212 -7.36 75 -0.132 ## 3 R2-D2 96 -2.25 -78.4 32 -0.385 ## 4 Dart… 202 0.795 27.6 136 0.228 ## 5 Leia… 150 -0.701 -24.4 49 -0.285 ## 6 Owen… 178 0.105 3.64 120 0.134 ## 7 Beru… 165 -0.269 -9.36 75 -0.132 ## 8 R5-D4 97 -2.22 -77.4 32 -0.385 ## 9 Bigg… 183 0.249 8.64 84 -0.0786 ## 10 Obi-… 182 0.220 7.64 77 -0.120 ## # … with 77 more rows, and 4 more variables: mass_centered[,1] &lt;dbl&gt;, ## # birth_year &lt;dbl&gt;, birth_year_z[,1] &lt;dbl&gt;, ## # birth_year_centered[,1] &lt;dbl&gt; Here, I’ve created z-scored and centered (i.e. only subtracted the mean but didn’t divide by the standard deviation) versions of the height, mass, and birth_year columns in one go. 4.6.4.2 mutate_all() mutate_all() is used to mutate all columns in a data frame. df.starwars %&gt;% select(height, mass) %&gt;% mutate_all(&quot;as.character&quot;) # transform all columns to characters ## # A tibble: 87 x 2 ## height mass ## &lt;chr&gt; &lt;chr&gt; ## 1 172 77 ## 2 167 75 ## 3 96 32 ## 4 202 136 ## 5 150 49 ## 6 178 120 ## 7 165 75 ## 8 97 32 ## 9 183 84 ## 10 182 77 ## # … with 77 more rows Here, I’ve selected some columns first, and then changed the mode to character in each of them. Like we’ve seen with mutate_at(), you can add a name in the mutate_all() function call to make new columns instead of replacing the existing ones. df.starwars %&gt;% select(height, mass) %&gt;% mutate_all(.funs = list(char = &quot;as.character&quot;)) # make new character columns ## # A tibble: 87 x 4 ## height mass height_char mass_char ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 172 77 172 77 ## 2 167 75 167 75 ## 3 96 32 96 32 ## 4 202 136 202 136 ## 5 150 49 150 49 ## 6 178 120 178 120 ## 7 165 75 165 75 ## 8 97 32 97 32 ## 9 183 84 183 84 ## 10 182 77 182 77 ## # … with 77 more rows 4.6.4.3 mutate_if() mutate_if() can sometimes come in handy. For example, the following code changes all the numeric columns to character columns: df.starwars %&gt;% mutate_if(.predicate = is.numeric, .funs = &quot;as.character&quot;) ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Leia… 150 49 brown light brown 19 female ## 6 Owen… 178 120 brown, gr… light blue 52 male ## 7 Beru… 165 75 brown light blue 47 female ## 8 R5-D4 97 32 &lt;NA&gt; white, red red &lt;NA&gt; &lt;NA&gt; ## 9 Bigg… 183 84 black light brown 24 male ## 10 Obi-… 182 77 auburn, w… fair blue-gray 57 male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Or we can round all the numeric columns: df.starwars %&gt;% mutate_if(.predicate = is.numeric, .funs = &quot;round&quot;) ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Dart… 202 136 none white yellow 42 male ## 5 Leia… 150 49 brown light brown 19 female ## 6 Owen… 178 120 brown, gr… light blue 52 male ## 7 Beru… 165 75 brown light blue 47 female ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 9 Bigg… 183 84 black light brown 24 male ## 10 Obi-… 182 77 auburn, w… fair blue-gray 57 male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; 4.6.5 arrange() arrange() allows us to sort the values in a data frame by one or more column entries. df.starwars %&gt;% arrange(hair_color, desc(height)) ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Mon … 150 NA auburn fair blue 48 female ## 2 Wilh… 180 NA auburn, g… fair blue 64 male ## 3 Obi-… 182 77 auburn, w… fair blue-gray 57 male ## 4 Bail… 191 NA black tan brown 67 male ## 5 Greg… 185 85 black dark brown NA male ## 6 Bigg… 183 84 black light brown 24 male ## 7 Boba… 183 78.2 black fair brown 31.5 male ## 8 Quar… 183 NA black dark brown 62 male ## 9 Jang… 183 79 black tan brown 66 male ## 10 Land… 177 79 black dark brown 31 male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; Here, I’ve sorted the data frame first by hair_color, and then by height. I’ve used the desc() function to sort height in descending order. Bail Prestor Organa is the tallest black character in starwars. 4.6.6 Practice 2 Compute the body mass index for male characters who are human. select only the columns you need filter out only the rows you need make the new variable with the body mass index arrange the data frame starting with the highest body mass index # write your code here 4.7 Additional resources 4.7.1 Cheatsheets base R –&gt; summary of how to use base R (we will mostly use the tidyverse but it’s still important to know how to do things in base R) data transformation –&gt; transforming data using dplyr 4.7.2 Data camp courses cleaning data dplyr tidyverse 4.7.3 Books and chapters Chapters 9-15 in “R for Data Science” Chapter 5 in “Data Visualization - A practical introduction” "],
["data-wrangling-2.html", "Chapter 5 Data wrangling 2 5.1 Learning objectives 5.2 Load packages 5.3 Wrangling data (continued) 5.4 Reading in data 5.5 Saving data 5.6 Additional resources", " Chapter 5 Data wrangling 2 In this session, we will continue to learn about wrangling data. Some of the functions that I’ll introduce in this session are a little tricky to master. Like learning a new language, it takes some time to get fluent. However, it’s worth investing the time. 5.1 Learning objectives Learn how to group and summarize data using group_by() and summarize(). Learn how to deal with missing data entries NA. Get familiar with how to reshape data using gather(), spread(), separate() and unite(). Learn the basics of how to join multiple data frames with a focus on left_join(). Master how to read and save data. 5.2 Load packages Let’s first load the packages that we need for this chapter. library(&quot;knitr&quot;) # for rendering the RMarkdown file library(&quot;tidyverse&quot;) # for data wrangling 5.3 Wrangling data (continued) 5.3.1 Summarizing data Let’s first load the starwars data set again: df.starwars = starwars A particularly powerful way of interating with data is by grouping and summarizing it. summarize() returns a single value for each summary that we ask for: df.starwars %&gt;% summarize(height_mean = mean(height, na.rm = T), height_max = max(height, na.rm = T), n = n()) ## # A tibble: 1 x 3 ## height_mean height_max n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 174. 264 87 Here, I computed the mean height, the maximum height, and the total number of observations (using the function n()). Let’s say we wanted to get a quick sense for how tall starwars characters from different species are. To do that, we combine grouping with summarizing: df.starwars %&gt;% group_by(species) %&gt;% summarize(height_mean = mean(height, na.rm = T)) ## # A tibble: 38 x 2 ## species height_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; 160 ## 2 Aleena 79 ## 3 Besalisk 198 ## 4 Cerean 198 ## 5 Chagrian 196 ## 6 Clawdite 168 ## 7 Droid 140 ## 8 Dug 112 ## 9 Ewok 88 ## 10 Geonosian 183 ## # … with 28 more rows I’ve first used group_by() to group our data frame by the different species, and then used summarize() to calculate the mean height of each species. It would also be useful to know how many observations there are in each group. df.starwars %&gt;% group_by(species) %&gt;% summarize(height_mean = mean(height, na.rm = T), group_size = n()) %&gt;% arrange(desc(group_size)) ## # A tibble: 38 x 3 ## species height_mean group_size ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Human 177. 35 ## 2 &lt;NA&gt; 160 5 ## 3 Droid 140 5 ## 4 Gungan 209. 3 ## 5 Kaminoan 221 2 ## 6 Mirialan 168 2 ## 7 Twi&#39;lek 179 2 ## 8 Wookiee 231 2 ## 9 Zabrak 173 2 ## 10 Aleena 79 1 ## # … with 28 more rows Here, I’ve used the n() function to get the number of observations in each group, and then I’ve arranged the data frame according to group size in descending order. Note that n() always yields the number of observations in each group. If we don’t group the data, then we get the overall number of observations in our data frame (i.e. the number of rows). So, Humans are the largest group in our data frame, followed by Droids (who are considerably smaller) and Gungans (who would make for good Basketball players). Sometimes group_by() is also useful without summarizing the data. For example, we often want to z-score (i.e. normalize) data on the level of individual participants. To do so, we first group the data on the level of participants, and then use mutate() to scale the data. Here is an example: # first let&#39;s generate some random data df.summarize = tibble( participant = rep(1:3, each = 5), judgment = sample(0:100, size = 15, replace = TRUE) ) %&gt;% print() ## # A tibble: 15 x 2 ## participant judgment ## &lt;int&gt; &lt;int&gt; ## 1 1 83 ## 2 1 31 ## 3 1 2 ## 4 1 2 ## 5 1 49 ## 6 2 72 ## 7 2 58 ## 8 2 6 ## 9 2 20 ## 10 2 27 ## 11 3 44 ## 12 3 70 ## 13 3 11 ## 14 3 23 ## 15 3 63 df.summarize %&gt;% group_by(participant) %&gt;% # group by participants mutate(judgment_zscored = scale(judgment)) %&gt;% # z-score data on individual participant level ungroup() %&gt;% # ungroup the data frame head(n = 10) # print the top 10 rows ## # A tibble: 10 x 3 ## participant judgment judgment_zscored ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 83 1.45 ## 2 1 31 -0.0702 ## 3 1 2 -0.918 ## 4 1 2 -0.918 ## 5 1 49 0.456 ## 6 2 72 1.29 ## 7 2 58 0.779 ## 8 2 6 -1.11 ## 9 2 20 -0.605 ## 10 2 27 -0.350 First, I’ve generated some random data using the repeat function rep() for making a participant column, and the sample() function to randomly choose values from a range between 0 and 100 with replacement. (We will learn more about these functions later when we look into how to simulate data.) I’ve then grouped the data by participant, and used the scale function to z-score the data. TIP: Don’t forget to ungroup() your data frame. Otherwise, any subsequent operations are applied per group. Sometimes, I want to run operations on each row, rather than per column. For example, let’s say that I wanted each character’s average combined height and mass. Let’s see first what doesn’t work: df.starwars %&gt;% mutate(mean_height_mass = mean(c(height, mass), na.rm = T)) %&gt;% select(name, height, mass, mean_height_mass) ## # A tibble: 87 x 4 ## name height mass mean_height_mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 142. ## 2 C-3PO 167 75 142. ## 3 R2-D2 96 32 142. ## 4 Darth Vader 202 136 142. ## 5 Leia Organa 150 49 142. ## 6 Owen Lars 178 120 142. ## 7 Beru Whitesun lars 165 75 142. ## 8 R5-D4 97 32 142. ## 9 Biggs Darklighter 183 84 142. ## 10 Obi-Wan Kenobi 182 77 142. ## # … with 77 more rows Note that all the values are the same. The value shown here is just the mean of all the values in height and mass. df.starwars %&gt;% select(height, mass) %&gt;% unlist() %&gt;% # turns the data frame into a vector mean(na.rm = T) ## [1] 141.8886 To get the mean by row, we can either spell out the arithmetic df.starwars %&gt;% mutate(mean_height_mass = (height + mass) / 2) %&gt;% # here, I&#39;ve replaced the mean() function select(name, height, mass, mean_height_mass) ## # A tibble: 87 x 4 ## name height mass mean_height_mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 124. ## 2 C-3PO 167 75 121 ## 3 R2-D2 96 32 64 ## 4 Darth Vader 202 136 169 ## 5 Leia Organa 150 49 99.5 ## 6 Owen Lars 178 120 149 ## 7 Beru Whitesun lars 165 75 120 ## 8 R5-D4 97 32 64.5 ## 9 Biggs Darklighter 183 84 134. ## 10 Obi-Wan Kenobi 182 77 130. ## # … with 77 more rows or use the rowwise() helper function which is like group_by() but treats each row like a group: df.starwars %&gt;% rowwise() %&gt;% # now, each row is treated like a separate group mutate(mean_height_mass = mean(c(height, mass), na.rm = T)) %&gt;% ungroup() %&gt;% select(name, height, mass, mean_height_mass) ## # A tibble: 87 x 4 ## name height mass mean_height_mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 124. ## 2 C-3PO 167 75 121 ## 3 R2-D2 96 32 64 ## 4 Darth Vader 202 136 169 ## 5 Leia Organa 150 49 99.5 ## 6 Owen Lars 178 120 149 ## 7 Beru Whitesun lars 165 75 120 ## 8 R5-D4 97 32 64.5 ## 9 Biggs Darklighter 183 84 134. ## 10 Obi-Wan Kenobi 182 77 130. ## # … with 77 more rows 5.3.1.1 Practice 1 Find out what the average height and mass (as well as the standard deviation) is from different species in different homeworlds. Why is the standard deviation NA for many groups? # write your code here Who is the tallest member of each species? What eye color do they have? The top_n() function or the row_number() function (in combination with filter()) will be useful here. # write your code here 5.3.2 Reshaping data We want our data frames to be tidy. What’s tidy? Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. For more information on tidy data frames see the Tidy data chapter in Hadley Wickham’s R for Data Science book. “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham Let’s first generate a data set that is not tidy. # construct data frame df.reshape = tibble( participant = c(1, 2), observation_1 = c(10, 25), observation_2 = c(100, 63), observation_3 = c(24, 45)) %&gt;% print() ## # A tibble: 2 x 4 ## participant observation_1 observation_2 observation_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 100 24 ## 2 2 25 63 45 Here, I’ve generated data from two participants with three observations. This data frame is not tidy since each row contains more than a single observation. Data frames that have one row per participant but many observations are called wide data frames. We can make it tidy using the gather() function. df.reshape.long = df.reshape %&gt;% gather(key = &quot;index&quot;, value = &quot;rating&quot;, -participant) %&gt;% arrange(participant) %&gt;% print() ## # A tibble: 6 x 3 ## participant index rating ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 observation_1 10 ## 2 1 observation_2 100 ## 3 1 observation_3 24 ## 4 2 observation_1 25 ## 5 2 observation_2 63 ## 6 2 observation_3 45 df.reshape.long now contains one observation in each row. Data frames with one row per observation are called long data frames. The gather() function takes four arguments: the data which I’ve passed to it via the pipe %&gt;% a name for the key column which will contain the column names of the original data frame a name for the value column which will contain the values that were spread across different columns in the original data frame a specification for which columns we want to gather – here I’ve specified that we want to gather the values from all columns except the participant column spread() is the counterpart of gather(). We can use it to go from a data frame that is in long format, to a data frame that’s in wide format, like so: df.reshape.wide = df.reshape.long %&gt;% spread(key = index, value = rating) %&gt;% print() ## # A tibble: 2 x 4 ## participant observation_1 observation_2 observation_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 10 100 24 ## 2 2 25 63 45 For my data, I often have a wide data frame that contains demographic information about participants, and a long data frame that contains participants’ responses in the experiment. In Section 5.3.3, we will learn how to combine information from multiple data frames (with potentially different formats). Here is a slightly more advanced example that involves reshaping a data frame. Let’s consider that we have the following data frame to start with: # construct data frame df.reshape2 = tibble( participant = c(1, 2), stimulus_1 = c(&quot;flower&quot;, &quot;car&quot;), observation_1 = c(10, 25), stimulus_2 = c(&quot;house&quot;, &quot;flower&quot;), observation_2 = c(100, 63), stimulus_3 = c(&quot;car&quot;, &quot;house&quot;), observation_3 = c(24, 45) ) %&gt;% print() ## # A tibble: 2 x 7 ## participant stimulus_1 observation_1 stimulus_2 observation_2 stimulus_3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 flower 10 house 100 car ## 2 2 car 25 flower 63 house ## # … with 1 more variable: observation_3 &lt;dbl&gt; Now, the data frame contains in each row, which stimuli a participant saw, and what rating she gave. Each of the two participants saw a picture of a flower, car, and house, and rated how much they liked the picture on a scale from 0 to 100. The order at which the pictures were presented was randomized between participants. I will use a combination of gather(), separate(), and spread() to turn this into a data frame in long format. df.reshape2 %&gt;% gather(key = &quot;index&quot;, value = &quot;value&quot;, -participant) %&gt;% separate(col = index, into = c(&quot;index&quot;, &quot;order&quot;), sep = &quot;_&quot;) %&gt;% spread(key = index, value = value) %&gt;% mutate_at(vars(order, observation), ~ as.numeric(.)) %&gt;% select(participant, order, stimulus, rating = observation) ## # A tibble: 6 x 4 ## participant order stimulus rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 flower 10 ## 2 1 2 house 100 ## 3 1 3 car 24 ## 4 2 1 car 25 ## 5 2 2 flower 63 ## 6 2 3 house 45 Voilà! Getting the desired data frame involved a few new tricks. Let’s take it step by step. First, I use gather() to make a long table. df.reshape2 %&gt;% gather(key = &quot;index&quot;, value = &quot;value&quot;, -participant) ## # A tibble: 12 x 3 ## participant index value ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 stimulus_1 flower ## 2 2 stimulus_1 car ## 3 1 observation_1 10 ## 4 2 observation_1 25 ## 5 1 stimulus_2 house ## 6 2 stimulus_2 flower ## 7 1 observation_2 100 ## 8 2 observation_2 63 ## 9 1 stimulus_3 car ## 10 2 stimulus_3 house ## 11 1 observation_3 24 ## 12 2 observation_3 45 However, I want to have the information about the stimulus and the observation in the same row. That is, I want to see what rating a participant gave to the flower stimulus, for example. To get there, I separate the index column into two separate columns using the separate() function. df.reshape2 %&gt;% gather(key = &quot;index&quot;, value = &quot;value&quot;, -participant) %&gt;% separate(col = index, into = c(&quot;index&quot;, &quot;order&quot;), sep = &quot;_&quot;) ## # A tibble: 12 x 4 ## participant index order value ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 stimulus 1 flower ## 2 2 stimulus 1 car ## 3 1 observation 1 10 ## 4 2 observation 1 25 ## 5 1 stimulus 2 house ## 6 2 stimulus 2 flower ## 7 1 observation 2 100 ## 8 2 observation 2 63 ## 9 1 stimulus 3 car ## 10 2 stimulus 3 house ## 11 1 observation 3 24 ## 12 2 observation 3 45 The separate() function takes four arguments: the data which I’ve passed to it via the pipe %&gt;% the name of the column col which we want to separate the names of the columns into into which we want to separate the original column the separator sep that we want to use to split the columns. Note, like gather() and spread(), there is a partner for separate(), too. It’s called unite() and it allows you to combine several columns into one. Now, I can use the spread() function to make a separate column for each entry in index that contains the values in value. df.reshape2 %&gt;% gather(key = &quot;index&quot;, value = &quot;value&quot;, -participant) %&gt;% separate(index, into = c(&quot;index&quot;, &quot;order&quot;), sep = &quot;_&quot;) %&gt;% spread(index, value) ## # A tibble: 6 x 4 ## participant order observation stimulus ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 1 10 flower ## 2 1 2 100 house ## 3 1 3 24 car ## 4 2 1 25 car ## 5 2 2 63 flower ## 6 2 3 45 house That’s pretty much it. Now, each row contains information about the order in which a stimulus was presented, what the stimulus was, and the judgment that a participant made in this trial. df.reshape2 %&gt;% gather(key = &quot;index&quot;, value = &quot;value&quot;, -participant) %&gt;% separate(index, into = c(&quot;index&quot;, &quot;order&quot;), sep = &quot;_&quot;) %&gt;% spread(index,value) %&gt;% mutate_at(vars(order, observation), ~ as.numeric(.)) %&gt;% select(participant, order, stimulus, rating = observation) ## # A tibble: 6 x 4 ## participant order stimulus rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 flower 10 ## 2 1 2 house 100 ## 3 1 3 car 24 ## 4 2 1 car 25 ## 5 2 2 flower 63 ## 6 2 3 house 45 The rest is familiar. I’ve used mutate_at() to turn order and observation into numeric columns, select() to change the order of the columns (and renamed the observation column to rating along the way), and arrange() to sort the data frame by participant and order. Sometimes, we may have a data frame where data is recorded in a long string. df.reshape3 = tibble( participant = 1:2, judgments = c(&quot;10, 4, 12, 15&quot;, &quot;3, 4&quot;) ) %&gt;% print() ## # A tibble: 2 x 2 ## participant judgments ## &lt;int&gt; &lt;chr&gt; ## 1 1 10, 4, 12, 15 ## 2 2 3, 4 Here, I’ve created a data frame with data from two participants. For whatever reason, we have four judgments from participant 1 and only two judgments from participant 2 (data is often messy in real life, too!). We can use the separate_rows() function to turn this into a tidy data frame in long format. df.reshape3 %&gt;% separate_rows(judgments) ## # A tibble: 6 x 2 ## participant judgments ## &lt;int&gt; &lt;chr&gt; ## 1 1 10 ## 2 1 4 ## 3 1 12 ## 4 1 15 ## 5 2 3 ## 6 2 4 Getting familiar with gather() and spread() takes some time plus trial and error. So don’t be discouraged if you don’t get what you want straight away. Once you’ve mastered these functions, they will make it much easier to get your data frames into shape. After having done some transformations like this, it’s worth checking that nothing went wrong. I often compare a few values in the transformed and original data frame to make sure everything is legit. 5.3.2.1 Practice 2 Load this data frame first. df.practice2 = tibble( participant = 1:10, initial = c(&quot;AR&quot;, &quot;FA&quot;, &quot;IR&quot;, &quot;NC&quot;, &quot;ER&quot;, &quot;PI&quot;, &quot;DH&quot;, &quot;CN&quot;, &quot;WT&quot;, &quot;JD&quot;), judgment_1 = c(12, 13, 1, 14, 5, 6, 12, 41, 100, 33), judgment_2 = c(2, 20, 10, 89, 94, 27, 29, 19, 57, 74), judgment_3 = c(2, 20, 10, 89, 94, 27, 29, 19, 57, 74) ) Make the df.practice2 data framey tidy (by turning into a long format). Compute the z-score of each participants’ judgments (using the scale() function). Calculate the mean and standard deviation of each participants’ z-scored judgments. Notice anything interesting? Think about what z-scoring does … # write your code here 5.3.3 Joining multiple data frames It’s nice to have all the information we need in a single, tidy data frame. We have learned above how to go from a single untidy data frame to a tidy one. However, often our situation to start off with is even worse. The information we need sits in several, messy data frames. For example, we may have one data frame df.stimuli with information about each stimulus, and then have another data frame with participants’ responses df.responses that only contains a stimulus index but no other infromation about the stimuli. set.seed(1) # setting random seed to make this example reproducible # data frame with stimulus information df.stimuli = tibble( index = 1:5, height = c(2, 3, 1, 4, 5), width = c(4, 5, 2, 3, 1), n_dots = c(12, 15, 5, 13, 7), color = c(&quot;green&quot;, &quot;blue&quot;, &quot;white&quot;, &quot;red&quot;, &quot;black&quot;) ) %&gt;% print() ## # A tibble: 5 x 5 ## index height width n_dots color ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 2 4 12 green ## 2 2 3 5 15 blue ## 3 3 1 2 5 white ## 4 4 4 3 13 red ## 5 5 5 1 7 black # data frame with participants&#39; responses df.responses = tibble( participant = rep(1:3, each = 5), index = rep(1:5, 3), response = sample(0:100, size = 15, replace = TRUE) # randomly sample 15 values from 0 to 100 ) %&gt;% print() ## # A tibble: 15 x 3 ## participant index response ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 26 ## 2 1 2 37 ## 3 1 3 57 ## 4 1 4 91 ## 5 1 5 20 ## 6 2 1 90 ## 7 2 2 95 ## 8 2 3 66 ## 9 2 4 63 ## 10 2 5 6 ## 11 3 1 20 ## 12 3 2 17 ## 13 3 3 69 ## 14 3 4 38 ## 15 3 5 77 The df.stimuli data frame contains an index, information about the height, and width, as well as the number of dots, and their color. Let’s imagine that participants had to judge how much they liked each image from a scale of 0 (“not liking this dot pattern at all”) to 100 (“super thrilled about this dot pattern”). Let’s say that I now wanted to know what participants’ average response for the differently colored dot patterns are. Here is how I would do this: df.responses %&gt;% left_join(df.stimuli %&gt;% select(index, color), by = &quot;index&quot;) %&gt;% group_by(color) %&gt;% summarize(response_mean = mean(response)) ## # A tibble: 5 x 2 ## color response_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 black 34.3 ## 2 blue 49.7 ## 3 green 45.3 ## 4 red 64 ## 5 white 64 Let’s take it step by step. The key here is to add the information from the df.stimuli data frame to the df.responses data frame. df.responses %&gt;% left_join(df.stimuli %&gt;% select(index, color), by = &quot;index&quot;) ## # A tibble: 15 x 4 ## participant index response color ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 26 green ## 2 1 2 37 blue ## 3 1 3 57 white ## 4 1 4 91 red ## 5 1 5 20 black ## 6 2 1 90 green ## 7 2 2 95 blue ## 8 2 3 66 white ## 9 2 4 63 red ## 10 2 5 6 black ## 11 3 1 20 green ## 12 3 2 17 blue ## 13 3 3 69 white ## 14 3 4 38 red ## 15 3 5 77 black I’ve joined the df.stimuli table in which I’ve only selected the index and color column, with the df.responses table, and specified the index column as the one by which the tables should be joined. This is the only column that both of the data frames have in common. To specify multiple columns by which we would like to join tables, we specify the by argument as follows: by = c(\"one_column\", \"another_column\"). Sometimes, the tables I want to join don’t have any column names in common. In that case, we can tell the left_join() function which column pair(s) should be used for joining. df.responses %&gt;% rename(stimuli = index) %&gt;% # I&#39;ve renamed the index column to stimuli left_join(df.stimuli %&gt;% select(index, color), by = c(&quot;stimuli&quot; = &quot;index&quot;)) ## # A tibble: 15 x 4 ## participant stimuli response color ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 26 green ## 2 1 2 37 blue ## 3 1 3 57 white ## 4 1 4 91 red ## 5 1 5 20 black ## 6 2 1 90 green ## 7 2 2 95 blue ## 8 2 3 66 white ## 9 2 4 63 red ## 10 2 5 6 black ## 11 3 1 20 green ## 12 3 2 17 blue ## 13 3 3 69 white ## 14 3 4 38 red ## 15 3 5 77 black Here, I’ve first renamed the index column (to create the problem) and then used the by = c(\"stimuli\" = \"index\") construction (to solve the problem). In my experience, it often takes a little bit of playing around to make sure that the data frames were joined as intended. One very good indicator is the row number of the initial data frame, and the joined one. For a left_join(), most of the time, we want the row number of the original data frame (“the one on the left”) and the joined data frame to be the same. If the row number changed, something probably went wrong. Take a look at the join help file to see other operations for combining two or more data frames into one (make sure to look at the one from the dplyr package). 5.3.3.1 Practice 3 Load these three data frames first: set.seed(1) df.judgments = tibble( participant = rep(1:3, each = 5), stimulus = rep(c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), 5), judgment = sample(0:100, size = 15, replace = T) ) df.information = tibble( number = seq(from = 0, to = 100, length.out = 5), color = c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;black&quot;, &quot;white&quot;) ) Create a new data frame called df.join that combines the information from both df.judgments and df.information. Note that column with the colors is called stimulus in df.judgments and color in df.information. At the end, you want a data frame that contains the following columns: participant, stimulus, number, and judgment. # write your code here 5.3.4 Dealing with missing data There are two ways for data to be missing. implicit: data is not present in the table explicit: data is flagged with NA We can check for explicit missing values using the is.na() function like so: tmp.na = c(1, 2, NA, 3) is.na(tmp.na) ## [1] FALSE FALSE TRUE FALSE I’ve first created a vector tmp.na with a missing value at index 3. Calling the is.na() function on this vector yields a logical vector with FALSE for each value that is not missing, and TRUE for each missing value. Let’s say that we have a data frame with missing values and that we want to replace those missing values with something else. Let’s first create a data frame with missing values. df.missing = tibble(x = c(1, 2, NA), y = c(&quot;a&quot;, NA, &quot;b&quot;)) print(df.missing) ## # A tibble: 3 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a ## 2 2 &lt;NA&gt; ## 3 NA b We can use the replace_na() function to replace the missing values with something else. df.missing %&gt;% mutate(x = replace_na(x, replace = 0), y = replace_na(y, replace = &quot;unknown&quot;)) ## # A tibble: 3 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a ## 2 2 unknown ## 3 0 b We can also remove rows with missing values using the drop_na() function. df.missing %&gt;% drop_na() ## # A tibble: 1 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a If we only want to drop values from specific columns, we can specify these columns within the drop_na() function call. So, if we only want to drop rows that have missing values in the x column, we can write: df.missing %&gt;% drop_na(x) ## # A tibble: 2 x 2 ## x y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 a ## 2 2 &lt;NA&gt; To make the distinction between implicit and explicit missing values more concrete, let’s consider the following example (taken from here): df.stocks = tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) There are two missing values in this dataset: The return for the fourth quarter of 2015 is explicitly missing, because the cell where its value should be instead contains NA. The return for the first quarter of 2016 is implicitly missing, because it simply does not appear in the dataset. We can use the complete() function to turn implicit missing values explicit: df.stocks %&gt;% complete(year, qtr) ## # A tibble: 8 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 NA ## 5 2016 1 NA ## 6 2016 2 0.92 ## 7 2016 3 0.17 ## 8 2016 4 2.66 Note how now, the data frame contains an additional row in which year = 2016, qtr = 1 and return = NA even though we didn’t originally specify this. We can also directly tell the complete() function to replace the NA values via passing a list to its fill argument like so: df.stocks %&gt;% complete(year, qtr, fill = list(return = 0)) ## # A tibble: 8 x 3 ## year qtr return ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2015 1 1.88 ## 2 2015 2 0.59 ## 3 2015 3 0.35 ## 4 2015 4 0 ## 5 2016 1 0 ## 6 2016 2 0.92 ## 7 2016 3 0.17 ## 8 2016 4 2.66 This specifies that we would like to replace any NA in the return column with 0. Again, if we had multiple columns with NAs, we could speficy for each column separately how to replace it. 5.4 Reading in data So far, we’ve used data sets that already came with the packages we’ve loaded. In the visualization chapters, we used the diamonds data set from the ggplot2 package, and in the data wrangling chapters, we used the starwars data set from the dplyr package. file type platform description csv general medium-size data frames RData R saving the results of intensive computations xls excel people who use excel json general more complex data structures feather python &amp; R fast interaction between R and python The foreign package helps with importing data that was saved in SPSS, Stata, or Minitab. For data in a json format, I highly recommend the tidyjson package. 5.4.1 csv I’ve stored some data files in the data/ subfolder. Let’s first read a csv (= comma-separated-value) file. df.csv = read_csv(&quot;data/movies.csv&quot;) ## Parsed with column specification: ## cols( ## title = col_character(), ## genre = col_character(), ## director = col_character(), ## year = col_double(), ## duration = col_double(), ## gross = col_double(), ## budget = col_double(), ## cast_facebook_likes = col_double(), ## votes = col_double(), ## reviews = col_double(), ## rating = col_double() ## ) The read_csv() function gives us information about how each column was parsed. Here, we have some columns that are characters (such as title and genre), and some columns that are numeric (such as year and duration). Note that it says double() in the specification but double and numeric are identical. And let’s take a quick peek at the data: df.csv %&gt;% glimpse() ## Observations: 2,961 ## Variables: 11 ## $ title &lt;chr&gt; &quot;Over the Hill to the Poorhouse&quot;, &quot;The Broad… ## $ genre &lt;chr&gt; &quot;Crime&quot;, &quot;Musical&quot;, &quot;Comedy&quot;, &quot;Comedy&quot;, &quot;Com… ## $ director &lt;chr&gt; &quot;Harry F. Millarde&quot;, &quot;Harry Beaumont&quot;, &quot;Lloy… ## $ year &lt;dbl&gt; 1920, 1929, 1933, 1935, 1936, 1937, 1939, 19… ## $ duration &lt;dbl&gt; 110, 100, 89, 81, 87, 83, 102, 226, 88, 144,… ## $ gross &lt;dbl&gt; 3000000, 2808000, 2300000, 3000000, 163245, … ## $ budget &lt;dbl&gt; 100000, 379000, 439000, 609000, 1500000, 200… ## $ cast_facebook_likes &lt;dbl&gt; 4, 109, 995, 824, 352, 229, 2509, 1862, 1178… ## $ votes &lt;dbl&gt; 5, 4546, 7921, 13269, 143086, 133348, 291875… ## $ reviews &lt;dbl&gt; 2, 107, 162, 164, 331, 349, 746, 863, 252, 1… ## $ rating &lt;dbl&gt; 4.8, 6.3, 7.7, 7.8, 8.6, 7.7, 8.1, 8.2, 7.5,… The data frame contains a bunch of movies with information about their genre, director, rating, etc. The readr package (which contains the read_csv() function) has a number of other functions for reading data. Just type read_ in the console below and take a look at the suggestions that autocomplete offers. 5.4.2 RData RData is a data format native to R. Since this format can only be read by R, it’s not a good format for sharing data. However, it’s a useful format that allows us to flexibly save and load R objects. For example, consider that we always start our script by reading in and structuring data, and that this takes quite a while. One thing we can do is to save the output of intermediate steps as an RData object, and then simply load this object (instead of re-running the whole routine every time). We read (or load) an RData file in the following way: load(&quot;data/test.RData&quot;, verbose = TRUE) ## Loading objects: ## df.test I’ve set the verbose = argument to TRUE here so that the load() function tells me what objects it added to the environment. This is useful for checking whether existing objects were overwritten. 5.5 Saving data 5.5.1 csv To save a data frame as a csv file, we simply write: df.test = tibble( x = 1:3, y = c(&quot;test1&quot;, &quot;test2&quot;, &quot;test3&quot;) ) write_csv(df.test, path = &quot;data/test.csv&quot;) Just like for reading in data, the readr package has a number of other functions for saving data. Just type write_ in the console below and take a look at the autocomplete suggestions. 5.5.2 RData To save objects as an RData file, we write: save(df.test, file = &quot;data/test.RData&quot;) We can add multiple objects simply by adding them at the beginning, like so: save(df.test, df.starwars, file = &quot;data/test_starwars.RData&quot;) 5.6 Additional resources 5.6.1 Cheatsheets wrangling data –&gt; wrangling data using dplyr and tidyr importing &amp; saving data –&gt; importing and saving data with readr 5.6.2 Data camp courses Joining tables writing functions importing data 1 importing data 2 5.6.3 Books and chapters Chapters 17-21 in R for Data Science Exploratory data analysis R programming for data science 5.6.4 Tutorials Joining data: Two-table verbs Tutorial by Jenny Bryan tidyexplain: Animations that illustrate how gather(), spread(), left_join(), etc. work "],
["probability-and-causality.html", "Chapter 6 Probability and causality 6.1 Load packages 6.2 Counting 6.3 Flipping a coin many times 6.4 Clue guide to probability 6.5 Probability operations 6.6 Bayesian reasoning example 6.7 Bayesian networks 6.8 Additional resources", " Chapter 6 Probability and causality 6.1 Load packages Let’s first load the packages that we need for this chapter (just use install.packages() to install any packages you don’t have yet). library(&quot;knitr&quot;) # for rendering the RMarkdown file library(&quot;kableExtra&quot;) # for nicely formatted tables library(&quot;arrangements&quot;) # fast generators and iterators for permutations, combinations and partitions library(&quot;DiagrammeR&quot;) # for drawing diagrams library(&quot;tidyverse&quot;) # for data wrangling 6.2 Counting Imagine that there are three balls in an urn. The balls are labeled 1, 2, and 3. Let’s consider a few possible situations. library(&quot;arrangements&quot;) # fast generators and iterators for permutations, combinations and partitions balls = 1:3 # number of balls in urn ndraws = 2 # number of draws # order matters, without replacement permutations(balls, ndraws) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 ## [3,] 2 1 ## [4,] 2 3 ## [5,] 3 1 ## [6,] 3 2 # order matters, with replacement permutations(balls, ndraws, replace = T) ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 2 1 ## [5,] 2 2 ## [6,] 2 3 ## [7,] 3 1 ## [8,] 3 2 ## [9,] 3 3 # order doesn&#39;t matter, with replacement combinations(balls, ndraws, replace = T) ## [,1] [,2] ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 2 2 ## [5,] 2 3 ## [6,] 3 3 # order doesn&#39;t matter, without replacement combinations(balls, ndraws) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 ## [3,] 2 3 I’ve generated the figures below using the DiagrammeR package. It’s a powerful package for drawing diagrams in R. See information on how to use the DiagrammeR package here. Figure 6.1: Drawing two marbles out of an urn with replacement. Figure 6.2: Drawing two marbles out of an urn without replacement. 6.3 Flipping a coin many times # Example taken from here: http://statsthinking21.org/probability.html#empirical-frequency set.seed(1) # set the seed so that the outcome is consistent nsamples = 50000 # how many flips do we want to make? # create some random coin flips using the rbinom() function with # a true probability of 0.5 df.samples = tibble( trial_number = seq(nsamples), outcomes = rbinom(nsamples, 1, 0.5)) %&gt;% mutate(mean_probability = cumsum(outcomes) / seq_along(outcomes)) %&gt;% filter(trial_number &gt;= 10) # start with a minimum sample of 10 flips ggplot(data = df.samples, mapping = aes(x = trial_number, y = mean_probability)) + geom_hline(yintercept = 0.5, color = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_line() + labs( x = &quot;Number of trials&quot;, y = &quot;Estimated probability of heads&quot; )+ theme_classic()+ theme(text = element_text(size = 20)) Figure 6.3: A demonstration of the law of large numbers. 6.4 Clue guide to probability who = c(&quot;ms_scarlet&quot;, &quot;col_mustard&quot;, &quot;mrs_white&quot;, &quot;mr_green&quot;, &quot;mrs_peacock&quot;, &quot;prof_plum&quot;) what = c(&quot;candlestick&quot;, &quot;knife&quot;, &quot;lead_pipe&quot;, &quot;revolver&quot;, &quot;rope&quot;, &quot;wrench&quot;) where = c(&quot;study&quot;, &quot;kitchen&quot;, &quot;conservatory&quot;, &quot;lounge&quot;, &quot;billiard_room&quot;, &quot;hall&quot;, &quot;dining_room&quot;, &quot;ballroom&quot;, &quot;library&quot;) df.clue = expand.grid(who = who, what = what, where = where) %&gt;% as_tibble() df.suspects = df.clue %&gt;% distinct(who) %&gt;% mutate(gender = ifelse( test = who %in% c(&quot;ms_scarlet&quot;, &quot;mrs_white&quot;, &quot;mrs_peacock&quot;), yes = &quot;female&quot;, no = &quot;male&quot;) ) df.suspects %&gt;% arrange(desc(gender)) %&gt;% kable() %&gt;% kable_styling(&quot;striped&quot;, full_width = F) who gender col_mustard male mr_green male prof_plum male ms_scarlet female mrs_white female mrs_peacock female 6.4.1 Conditional probability # conditional probability (via rules of probability) df.suspects %&gt;% summarize(p_prof_plum_given_male = sum(gender == &quot;male&quot; &amp; who == &quot;prof_plum&quot;) / sum(gender == &quot;male&quot;)) ## # A tibble: 1 x 1 ## p_prof_plum_given_male ## &lt;dbl&gt; ## 1 0.333 # conditional probability (via rejection) df.suspects %&gt;% filter(gender == &quot;male&quot;) %&gt;% summarize(p_prof_plum_given_male = sum(who == &quot;prof_plum&quot;) / n()) ## # A tibble: 1 x 1 ## p_prof_plum_given_male ## &lt;dbl&gt; ## 1 0.333 6.4.2 Law of total probability 6.5 Probability operations # Make a deck of cards df.cards = tibble( suit = rep(c(&quot;Clubs&quot;, &quot;Spades&quot;, &quot;Hearts&quot;, &quot;Diamonds&quot;), each = 8), value = rep(c(&quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;Jack&quot;, &quot;Queen&quot;, &quot;King&quot;, &quot;Ace&quot;), 4) ) # conditional probability: p(Hearts | Queen) (via rules of probability) df.cards %&gt;% summarize(p_hearts_given_queen = sum(suit == &quot;Hearts&quot; &amp; value == &quot;Queen&quot;) / sum(value == &quot;Queen&quot;)) ## # A tibble: 1 x 1 ## p_hearts_given_queen ## &lt;dbl&gt; ## 1 0.25 # conditional probability: p(Hearts | Queen) (via rejection) df.cards %&gt;% filter(value == &quot;Queen&quot;) %&gt;% summarize(p_hearts_given_queen = sum(suit == &quot;Hearts&quot;)/n()) ## # A tibble: 1 x 1 ## p_hearts_given_queen ## &lt;dbl&gt; ## 1 0.25 6.6 Bayesian reasoning example 6.7 Bayesian networks 6.7.1 Sprinkler example # cloudy df.cloudy = tibble( `p(C)` = 0.5 ) df.cloudy %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, font_size = 20) p(C) 0.5 # sprinkler given cloudy df.sprinkler_given_cloudy = tibble( C = c(&quot;F&quot;, &quot;T&quot;), `p(S)`= c(0.5, 0.1) ) df.sprinkler_given_cloudy %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, font_size = 20) C p(S) F 0.5 T 0.1 # rain given cloudy df.rain_given_cloudy = tibble( C = c(&quot;F&quot;, &quot;T&quot;), `p(R)`= c(0.2, 0.8) ) df.rain_given_cloudy %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, font_size = 20) C p(R) F 0.2 T 0.8 # wet given sprinkler and rain df.rain_given_sprinkler_and_rain = tibble( S = rep(c(&quot;F&quot;, &quot;T&quot;), 2), R = rep(c(&quot;F&quot;, &quot;T&quot;), each = 2), `p(W)`= c(0, 0.9, 0.9, 0.99) ) df.rain_given_sprinkler_and_rain %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, font_size = 20) S R p(W) F F 0.00 T F 0.90 F T 0.90 T T 0.99 6.8 Additional resources 6.8.1 Cheatsheets Probability cheatsheet 6.8.2 Books and chapters Probability and Statistics with examples using R Learning statistics with R: Chapter 9 Introduction to probability 6.8.3 Misc Statistics 110: Probability; course at Harvard "],
["simulation-1.html", "Chapter 7 Simulation 1 7.1 Load packages and set plotting theme 7.2 Working with distributions 7.3 Bayesian inference with the normal distribution 7.4 Working with samples 7.5 Comparing probability distributions 7.6 Additional resources", " Chapter 7 Simulation 1 7.1 Load packages and set plotting theme library(&quot;knitr&quot;) library(&quot;kableExtra&quot;) library(&quot;MASS&quot;) library(&quot;patchwork&quot;) library(&quot;extrafont&quot;) library(&quot;tidyverse&quot;) theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 7.2 Working with distributions Every distribution that R handles has four functions. There is a root name, for example, the root name for the normal distribution is norm. This root is prefixed by one of the letters here: letter description example d for “density”, the density function (probability function (for discrete variables) or probability density function (for continuous variables)) dnorm() p for “probability”, the cumulative distribution function pnorm() q for “quantile”, the inverse cumulative distribution function qnorm() r for “random”, a random variable having the specified distribution rnorm() For the normal distribution, these functions are dnorm, pnorm, qnorm, and rnorm. For the binomial distribution, these functions are dbinom, pbinom, qbinom, and rbinom. And so forth. You can get more info about the distributions that come with R via running help(Distributions) in your console. If you need a distribution that doesn’t already come with R, then take a look here for many more distributions that can be loaded with different R packages. 7.2.1 Plotting distributions Here’s an easy way to plot distributions in ggplot2 using the stat_function() function. ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;) Note that the data frame I created with tibble() only needs to have the minimum and the maximum value of the x-range that we are interested in. Here, I chose -5 and 5 as the minimum and maximum, respectively. The stat_function() is very flexible. We can define our own functions and plot these like here: # define the breakpoint function fun.breakpoint = function(x, breakpoint){ x[x &lt; breakpoint] = breakpoint return(x) } # plot the function ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;fun.breakpoint&quot;, args = list(breakpoint = 2) ) Here, I defined a breakpoint function. If the value of x is below the breakpoint, y equals the value of the breakpoint. If the value of x is greater than the breakpoint, then y equals x. Note how I used the args = argument in the stat_function() to supply the breakpoint parameter that my fun.breakpoint() wants. Make sure to put these parameters into a list() as shown above. Let’s play around with the parameters of the normal distribution. The normal distribution takes two parameters, the mean and standard deviation. Again, I’m going to use the args = argument to supply these parameters. tmp.mean = 0 tmp.sd = 2 ggplot(data = tibble(x = c(140, 220)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, args = list(mean = tmp.mean, sd = tmp.sd)) # remove all variables with tmp in their name rm(list = ls() %&gt;% str_subset(pattern = &quot;tmp.&quot;)) To keep my environment clean, I’ve named the parameters tmp.mean and tmp.sd and then, at the end of the code chunk, I removed all variables from the environment that have “tmp.” in their name using the ls() function (which prints out all variables in the environment as a vector), and the str_subset() function which filters out only those variables that contain the specified pattern. 7.2.2 Sampling from distributions For each distribution, R provides a way of sampling random number from this distribution. For the normal distribution, we can use the rnorm() function to take random samples. So let’s take some random samples and plot a histogram. # make this example reproducible set.seed(1) # define how many samples to draw tmp.nsamples = 100 # make a data frame with the samples df.plot = tibble( x = rnorm(n = tmp.nsamples, mean = 0, sd = 1) ) # plot the samples using a histogram ggplot(data = df.plot, mapping = aes(x = x)) + geom_histogram(binwidth = 0.2, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = -4:4, labels = -4:4) + coord_cartesian(xlim = c(-4, 4), expand = T) # remove all variables with tmp in their name rm(list = ls() %&gt;% str_subset(pattern = &quot;tmp.&quot;)) Let’s see how many samples it takes to closely approximate the shape of the normal distribution with our histogram of samples. # make this example reproducible set.seed(1) # play around with this value tmp.nsamples = 100 # tmp.nsamples = 10000 tmp.binwidth = 0.2 # make a data frame with the samples df.plot = tibble( x = rnorm(n = tmp.nsamples, mean = 0, sd = 1) ) # adjust the density of the normal distribution based on the samples and binwidth fun.dnorm = function(x, mean, sd, n, binwidth){ dnorm(x = x, mean = mean, sd = sd) * n * binwidth } # plot the samples using a histogram ggplot(data = df.plot, mapping = aes(x = x)) + geom_histogram(binwidth = tmp.binwidth, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + stat_function(fun = &quot;fun.dnorm&quot;, args = list(mean = 0, sd = 1, n = tmp.nsamples, binwidth = tmp.binwidth), xlim = c(min(df.plot$x), max(df.plot$x)), size = 2) + annotate(geom = &quot;text&quot;, label = str_c(&quot;n = &quot;, tmp.nsamples), x = -3.9, y = Inf, hjust = 0, vjust = 1.1, size = 10, family = &quot;Courier New&quot;) + scale_x_continuous(breaks = -4:4, labels = -4:4) + coord_cartesian(xlim = c(-4, 4), expand = F) # remove all variables with tmp in their name rm(list = ls() %&gt;% str_subset(pattern = &quot;tmp.&quot;)) With 10,000 samples, our histogram of samples already closely resembles the theoretical shape of the normal distribution. 7.2.3 Cumulative probability distribution ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;pnorm&quot;, args = list(mean = 0, sd = 1)) Let’s find the cumulative probability of a particular value. tmp.x = 1 tmp.y = pnorm(tmp.x, mean = 0, sd = 1) print(tmp.y %&gt;% round(3)) ## [1] 0.841 # draw the cumulative probability distribution and show the value ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;pnorm&quot;, args = list(mean = 0, sd = 1)) + annotate(geom = &quot;point&quot;, x = tmp.x, y = tmp.y, size = 4, color = &quot;blue&quot;) + geom_segment(mapping = aes(x = tmp.x, xend = tmp.x, y = 0, yend = tmp.y), size = 1, color = &quot;blue&quot;) + geom_segment(mapping = aes(x = -5, xend = tmp.x, y = tmp.y, yend = tmp.y), size = 1, color = &quot;blue&quot;) + scale_x_continuous(breaks = -5:5) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 1.05), expand = F) # remove all variables with tmp in their name rm(list = str_subset(string = ls(), pattern = &quot;tmp.&quot;)) Let’s illustrate what this would look like using a normal density plot. ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, fill = &quot;lightblue&quot;, xlim = c(-5, 1), color = &quot;black&quot;, linetype = 2) + stat_function(fun = &quot;dnorm&quot;, size = 1.5) + coord_cartesian(xlim = c(-5, 5)) + scale_x_continuous(breaks = -5:5) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) 7.2.4 Inverse cumulative distribution ggplot(data = tibble(x = c(0, 1)), mapping = aes(x = x)) + stat_function(fun = &quot;qnorm&quot;, args = list(mean = 0, sd = 1)) And let’s compute the inverse cumulative probability for a particular value. # tmp.x = 0.841 tmp.x = 0.975 tmp.y = qnorm(tmp.x, mean = 0, sd = 1) print(tmp.y %&gt;% round(3)) ## [1] 1.96 # draw the cumulative probability distribution and show the value ggplot(data = tibble(x = c(0, 1)), mapping = aes(x = x)) + stat_function(fun = &quot;qnorm&quot;, args = list(mean = 0, sd = 1)) + annotate(geom = &quot;point&quot;, x = tmp.x, y = tmp.y, size = 4, color = &quot;blue&quot;) + geom_segment(mapping = aes(x = tmp.x, xend = tmp.x, y = -3, yend = tmp.y), size = 1, color = &quot;blue&quot;) + geom_segment(mapping = aes(x = 0, xend = tmp.x, y = tmp.y, yend = tmp.y), size = 1, color = &quot;blue&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = 0.1)) + coord_cartesian(xlim = c(0, 1.05), ylim = c(-3, 3), expand = F) # remove all variables with tmp in their name rm(list = str_subset(string = ls(), pattern = &quot;tmp.&quot;)) 7.2.5 Computing probabilities 7.2.5.1 Via probability distributions Let’s compute the probability of observing a particular value \\(x\\) in a given range. # tmp.lower = -1 # tmp.upper = 1 # tmp.lower = -2 # tmp.upper = 2 # tmp.lower = qnorm(0.001) # tmp.upper = qnorm(0.95) # tmp.lower = qnorm(0.05) # tmp.upper = qnorm(0.999) tmp.lower = qnorm(0.025) tmp.upper = qnorm(0.975) tmp.prob = pnorm(tmp.upper) - pnorm(tmp.lower) ggplot(data = tibble(x = c(-5, 5)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, fill = &quot;lightblue&quot;, xlim = c(tmp.lower, tmp.upper), color = &quot;black&quot;, linetype = 2) + stat_function(fun = &quot;dnorm&quot;, size = 1.5) + annotate(geom = &quot;text&quot;, label = str_c(tmp.prob %&gt;% round(2) * 100, &quot;%&quot;), x = 0, y = 0.2, hjust = 0.5, size = 10 ) + coord_cartesian(xlim = c(-5, 5)) + scale_x_continuous(breaks = -5:5) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) # remove all variables with tmp in their name rm(list = str_subset(string = ls(), pattern = &quot;tmp.&quot;)) We find that 95% of the density in the normal distribution is between -1.96 and 1.96. 7.2.5.2 Via sampling We can also compute the probability of observing certain events using sampling. We first generate samples from the desired probability distribution, and then use these samples to compute our statistic of interest. # let&#39;s compute the probability of observing a value within a certain range tmp.lower = -1.96 tmp.upper = 1.96 # make example reproducible set.seed(1) # generate some samples and store them in a data frame tmp.nsamples = 10000 df.samples = tibble( sample = 1:tmp.nsamples, value = rnorm(n = tmp.nsamples, mean = 0, sd = 1) ) # compute the probability that s sample lies within the range of interest tmp.prob = df.samples %&gt;% filter(value &gt;= tmp.lower, value &lt;= tmp.upper) %&gt;% summarize(prob = n()/tmp.nsamples) # illustrate the result using a histogram ggplot(data = df.samples, mapping = aes(x = value)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + scale_x_continuous(breaks = -4:4, labels = -4:4) + coord_cartesian(xlim = c(-4, 4), expand = F) + geom_vline(xintercept = tmp.lower, size = 1, color = &quot;red&quot;, linetype = 2) + geom_vline(xintercept = tmp.upper, size = 1, color = &quot;red&quot;, linetype = 2) + annotate(geom = &quot;label&quot;, label = str_c(tmp.prob %&gt;% round(3) * 100, &quot;%&quot;), x = 0, y = 200, hjust = 0.5, size = 10) # remove all variables with tmp in their name rm(list = str_subset(string = ls(), pattern = &quot;tmp.&quot;)) 7.3 Bayesian inference with the normal distribution Let’s consider the following scenario. You are helping out at a summer camp. This summer, two different groups of kids go to the same summer camp. The chess kids, and the basketball kids. The chess summer camp is not quite as popular as the basketball summer camp (shocking, I know!). In fact, twice as many children have signed up for the basketball camp. When signing up for the camp, the children were asked for some demographic information including their height in cm. Unsurprisingly, the basketball players tend to be taller on average than the chess players. In fact, the basketball players’ height is approximately normally distributed with a mean of 180cm and a standard deviation of 10cm. For the chess players, the mean height is 170cm with a standard deviation of 8cm. At the camp site, a child walks over to you and asks you where their gym is. You gage that the child is around 175cm tall. Where should you direct the child to? To the basketball gym, or to the chess gym? height = 175 # priors prior_basketball = 2/3 prior_chess = 1/3 # likelihood mean_basketball = 180 sd_basketball = 10 mean_chess = 170 sd_chess = 8 likelihood_basketball = dnorm(height, mean = mean_basketball, sd = sd_basketball) likelihood_chess = dnorm(height, mean = mean_chess, sd = sd_chess) # posterior posterior_basketball = (likelihood_basketball * prior_basketball) / ((likelihood_basketball * prior_basketball) + (likelihood_chess * prior_chess)) posterior_basketball %&gt;% print() ## [1] 0.631886 Let’s do the same thing via sampling. # number of kids tmp.nkids = 10000 # make reproducible set.seed(1) # priors prior_basketball = 2/3 prior_chess = 1/3 # likelihood functions mean_basketball = 180 sd_basketball = 10 mean_chess = 170 sd_chess = 8 # data frame with the kids df.camp = tibble( kid = 1:tmp.nkids, sport = sample(c(&quot;chess&quot;, &quot;basketball&quot;), size = tmp.nkids, replace = T, prob = c(prior_chess, prior_basketball))) %&gt;% rowwise() %&gt;% mutate(height = ifelse(test = sport == &quot;chess&quot;, yes = rnorm(., mean = mean_chess, sd = sd_chess), no = rnorm(., mean = mean_basketball, sd = sd_basketball))) %&gt;% ungroup df.camp %&gt;% print() ## # A tibble: 10,000 x 3 ## kid sport height ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 basketball 165. ## 2 2 basketball 163. ## 3 3 basketball 191. ## 4 4 chess 160. ## 5 5 basketball 183. ## 6 6 chess 164. ## 7 7 chess 169. ## 8 8 basketball 193. ## 9 9 basketball 172. ## 10 10 basketball 177. ## # … with 9,990 more rows Now we have a data frame with kids whose height was randomly sampled depending on which sport they do. I’ve used the sample() function to assign a sport to each kid first using the prob = argument to make sure that a kid is more likely to be assigned the sport “basketball” than “chess”. Note that the solution above is not particularly efficient since it uses the rowwise() function to make sure that a different random value for height is drawn for each row. Running this code will get slow for large samples. A more efficient solution would be the following: # number of kids tmp.nkids = 100000 # make reproducible set.seed(3) df.camp2 = tibble( kid = 1:tmp.nkids, sport = sample(c(&quot;chess&quot;, &quot;basketball&quot;), size = tmp.nkids, replace = T, prob = c(prior_chess, prior_basketball))) %&gt;% arrange(sport) %&gt;% mutate(height = c(rnorm(sum(sport == &quot;basketball&quot;), mean = mean_basketball, sd = sd_basketball), rnorm(sum(sport == &quot;chess&quot;), mean = mean_chess, sd = sd_chess)) ) In this solution, I take advantage of the fact that rnorm() is vectorized. That is, it can produce many random draws in one call. To make this work, I first arrange the data frame, and then draw the correct number of samples from each of the two distributions. This works fast, even if I’m drawing a large number of samples. How can we now use these samples to answer our question of interest? Let’s see what doesn’t work first: tmp.height = 175 df.camp %&gt;% filter(height == tmp.height) %&gt;% count(sport) %&gt;% spread(sport, n) %&gt;% summarize(prob_basketball = basketball/(basketball + chess)) The reason this doesn’t work is because none of our kids is exactly 175cm tall. Instead, we need to filter kids that are within a certain height range. tmp.height = 175 tmp.margin = 1 df.camp %&gt;% filter(between(height, left = tmp.height - tmp.margin, right = tmp.height + tmp.margin)) %&gt;% count(sport) %&gt;% spread(sport, n) %&gt;% summarize(prob_basketball = basketball/(basketball + chess)) ## # A tibble: 1 x 1 ## prob_basketball ## &lt;dbl&gt; ## 1 0.632 Here, I’ve used the between() function which is a shortcut for otherwise writing x &gt;= left &amp; x &lt;= right. You can play around with the margin to see how the result changes. 7.4 Working with samples 7.4.1 Understanding density() First, let’s calculate the density for a set of observations and store them in a data frame. # calculate density observations = c(1, 1.2, 1.5, 2, 3) bandwidth = 0.25 # bandwidth (= sd) of the Gaussian distribution tmp.density = density(observations, kernel = &quot;gaussian&quot;, bw = bandwidth, n = 512) # save density as data frame df.density = tibble( x = tmp.density$x, y = tmp.density$y ) df.density %&gt;% head() %&gt;% kable(digits = 3) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) x y 0.250 0.004 0.257 0.004 0.264 0.005 0.271 0.005 0.277 0.005 0.284 0.006 Now, let’s plot the density. ggplot(data = df.density, aes(x = x, y = y)) + geom_line(size = 2) + geom_point(data = as.tibble(observations), mapping = aes(x = value, y = 0), size = 3) ## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. This density shows the sum of the densities of normal distributions that are centered at the observations with the specified bandwidth. # add densities for the individual normal distributions for (i in 1:length(observations)){ df.density[[str_c(&quot;observation_&quot;,i)]] = dnorm(df.density$x, mean = observations[i], sd = bandwidth) } # sum densities df.density = df.density %&gt;% mutate(sum_norm = rowSums(select(., contains(&quot;observation_&quot;))), y = y * length(observations)) df.density %&gt;% head() %&gt;% kable(digits = 3) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) x y observation_1 observation_2 observation_3 observation_4 observation_5 sum_norm 0.250 0.019 0.018 0.001 0 0 0 0.019 0.257 0.021 0.019 0.001 0 0 0 0.021 0.264 0.023 0.021 0.001 0 0 0 0.022 0.271 0.024 0.023 0.002 0 0 0 0.024 0.277 0.027 0.024 0.002 0 0 0 0.026 0.284 0.029 0.026 0.002 0 0 0 0.028 Now, let’s plot the individual densities as well as the overall density. # add individual Gaussians colors = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;orange&quot;) # original density p = ggplot(data = df.density, aes(x = x, y = y)) + geom_line(size = 2) # individual densities for (i in 1:length(observations)){ p = p + stat_function(fun = &quot;dnorm&quot;, args = list(mean = observations[i], sd = bandwidth), color = colors[i]) } # individual observations p = p + geom_point(data = as.tibble(observations), mapping = aes(x = value, y = 0, color = factor(1:5)), size = 3, show.legend = F) + scale_color_manual(values = colors) # sum of the individual densities p = p + geom_line(data = df.density, aes(x = x, y = sum_norm), size = 1, color = &quot;red&quot;, linetype = 2) p # print the figure Here are the same results when specifying a different bandwidth: # calculate density observations = c(1, 1.2, 1.5, 2, 3) bandwidth = 0.5 # bandwidth (= sd) of the Gaussian distribution tmp.density = density(observations, kernel = &quot;gaussian&quot;, bw = bandwidth, n = 512) # save density as data frame df.density = tibble( x = tmp.density$x, y = tmp.density$y ) # add densities for the individual normal distributions for (i in 1:length(observations)){ df.density[[str_c(&quot;observation_&quot;,i)]] = dnorm(df.density$x, mean = observations[i], sd = bandwidth) } # sum densities df.density = df.density %&gt;% mutate(sum_norm = rowSums(select(., contains(&quot;observation_&quot;))), y = y * length(observations)) # original plot p = ggplot(data = df.density, aes(x = x, y = y)) + geom_line(size = 2) + geom_point(data = as.tibble(observations), mapping = aes(x = value, y = 0), size = 3) # add individual Gaussians for (i in 1:length(observations)){ p = p + stat_function(fun = &quot;dnorm&quot;, args = list(mean = observations[i], sd = bandwidth)) } # add the sum of Gaussians p = p + geom_line(data = df.density, aes(x = x, y = sum_norm), size = 1, color = &quot;red&quot;, linetype = 2) p 7.4.2 The quantile() function The quantile() function allows us to compute different quantiles of a sample. Boxplots are based on the quantiles of a distribution. To better understand this function, let’s compute our own boxplot. tmp.samples = 1000 # make example reproducible set.seed(1) # a sample from the normal distribution df.quantile = tibble( sample = 1:tmp.samples, value = rnorm(n = tmp.samples)) df.quantile %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sample value 1 -0.63 2 0.18 3 -0.84 4 1.60 5 0.33 6 -0.82 7 0.49 8 0.74 9 0.58 10 -0.31 Let’s draw a boxplot using ggplot. ggplot(data = df.quantile, mapping = aes(x = &quot;&quot;, y = value)) + geom_boxplot() Here is a reminder of what boxplots show from the help file of geom_boxplot(): The lower and upper hinges correspond to the first and third quartiles (the 25th and 75th percentiles). This differs slightly from the method used by the boxplot() function, and may be apparent with small samples. See boxplot.stats() for for more information on how hinge positions are calculated for boxplot(). The upper whisker extends from the hinge to the largest value no further than 1.5 * IQR from the hinge (where IQR is the inter-quartile range, or distance between the first and third quartiles). The lower whisker extends from the hinge to the smallest value at most 1.5 * IQR of the hinge. Data beyond the end of the whiskers are called “outlying” points and are plotted individually. So, let’s compute the relevant values using the quantile() function. df.quantile_values = tibble( median = quantile(df.quantile$value, 0.5), quartile_first = quantile(df.quantile$value, 0.25), quartile_third = quantile(df.quantile$value, 0.75), iqr = quartile_third - quartile_first, hinge_upper = quartile_third + 1.5 * iqr, hinge_lower = quartile_first - 1.5 * iqr ) Now, let’s check whether our values are correct by plotting them on top of the boxplot. # original boxplot ggplot(data = df.quantile, mapping = aes(x = 0, y = value)) + geom_boxplot() + geom_segment(x = -0.75, xend = -0.45, y = df.quantile_values$median, yend = df.quantile_values$median, arrow = arrow(type = &quot;closed&quot;, length = unit(0.5, &quot;cm&quot;)) ) + annotate(geom = &quot;text&quot;, label = &quot;median&quot;, x = -0.8, y = df.quantile_values$median, hjust = 1, vjust = 0.5, size = 6) + geom_segment(x = -0.75, xend = -0.45, y = df.quantile_values$quartile_third, yend = df.quantile_values$quartile_third, arrow = arrow(type = &quot;closed&quot;, length = unit(0.5, &quot;cm&quot;)) ) + annotate(geom = &quot;text&quot;, label = &quot;3rd quartile&quot;, x = -0.8, y = df.quantile_values$quartile_third, hjust = 1, vjust = 0.5, size = 6) + geom_segment(x = -0.75, xend = -0.05, y = df.quantile_values$hinge_upper, yend = df.quantile_values$hinge_upper, arrow = arrow(type = &quot;closed&quot;, length = unit(0.5, &quot;cm&quot;)) ) + annotate(geom = &quot;text&quot;, label = &quot;upper hinge&quot;, x = -0.8, y = df.quantile_values$hinge_upper, hjust = 1, vjust = 0.5, size = 6) + coord_cartesian(xlim = c(-1.2, 0.5)) Neat! Now we know how boxplots are made. We can also use the quantile function to create an inverse cumulative probability plot (i.e. the equivalent of what we get from qnorm() for the normal distribution). df.plot = df.quantile$value %&gt;% quantile(probs = seq(0, 1, 0.01)) %&gt;% as_tibble() %&gt;% mutate(x = seq(0, n(), length.out = n())) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead. ## This warning is displayed once per session. ggplot(data = df.plot, mapping = aes(x = x, y = value)) + geom_line() And we can calculate quantiles by hand in the following way: tmp.samples = 1000 # make example reproducible set.seed(1) # a sample from the normal distribution df.quantile = tibble( sample = 1:tmp.samples, value = rnorm(n = tmp.samples)) # compute quantiles by hand df.quantile = df.quantile %&gt;% arrange(value) %&gt;% mutate(rank = row_number(), quantile = rank/tmp.samples) To compute the quantiles by hand, I’ve sorted the data frame, ranked the values, and then computed the quantiles by normalizing the ranks (i.e. dividing by the sample size). Let’s check whether we get roughly the same result with our hand-calculated quantiles as we do from the quantile() function. # by hand df.quantile %&gt;% filter(rank %in% seq(from = 200, to = 800, by = 200)) %&gt;% pull(value) ## [1] -0.8848496 -0.2968686 0.2441649 0.8528150 # using quantile quantile(df.quantile$value, probs = seq(0.2, 0.8, 0.2)) ## 20% 40% 60% 80% ## -0.8815065 -0.2961539 0.2449833 0.8537340 As we can see, the results are very similar. Not identical since the quantile() function uses an efficient algorithm for its calculations (see help(quantile)). 7.5 Comparing probability distributions QQ plots, or quantile-quantile plots, are a good way of visually comparing two distributions. One common usage in statistics is to assess whether a variable is normally distributed. For example, let’s say that we fit a regression model and want to now assess whether the residuals (i.e. the model errors) are normally distributed. (We will learn how to run regressions soon). Let’s first just plot the residuals from the model we fit above. df.residuals = tibble( residual = rnorm(n = 10000, mean = 0, sd = 10) ) params = as.list(MASS::fitdistr(df.residuals$residual, &quot;normal&quot;)$estimate) #fit a normal distribution to the residuals ggplot(data = df.residuals, aes(x = residual))+ stat_density(geom = &quot;line&quot;, aes(color = &quot;green&quot;), size = 1.5)+ stat_function(fun = &quot;dnorm&quot;, args = params, aes(color = &quot;black&quot;), size = 1.5)+ scale_color_manual(values = c(&quot;black&quot;, &quot;green&quot;), labels = c(&quot;theoretical&quot;, &quot;empirical&quot;))+ theme(legend.title = element_blank(), legend.position = c(0.9, 0.9)) Figure 7.1: Empirical distribution of residuals, and theoretical distribution. Here, the empirical distribution of the errors and the theoretical normal distribution with a mean of 0 and a SD of 2 correspond very closely. Let’s take a look at the corresponding QQ plot. ggplot(data = df.residuals, aes(sample = residual)) + geom_abline(intercept = 0, slope = 1, linetype = 2) + geom_qq(distribution = &quot;qnorm&quot;, dparams = params) + coord_cartesian(xlim = c(-40, 40), ylim = c(-40, 40)) Note that the QQ plot is sensitive to the general shape of the distribution. I’ve used the geom_qq() and geom_qq_line() functions that are part of ggplot. By default, these functions assume a normal distribution as the theoretical distribution. This plot is just another way of showing the information in Figure ??. Intuitively, a QQ plot is built in the following way: imagine going with your finger from left to right along the x-axis on Figure ??, and then add a point on the QQ plot which captures the cumulative density for each distribution. Here are some more examples for what these plots would look like when comparing different theoretical distributions to the same empirical distribution. # data frame with parameters saved in a list column df.parameters = tibble( parameters = list( params, list(mean = -10, sd = 10), list(mean = 10, sd = 10), list(mean = 0, sd = 3) ) ) # list container for plots l.plots = list() for (i in 1:nrow(df.parameters)){ p1 = ggplot(data = df.residuals, aes(x = residual)) + stat_density(geom = &quot;line&quot;, color = &quot;green&quot;, size = 1.5) + stat_function(fun = &quot;dnorm&quot;, args = df.parameters$parameters[[i]], color = &quot;black&quot;, size = 1.5) + scale_y_continuous(limits = c(0, 0.15)) p2 = ggplot(data = df.residuals, aes(sample = residual)) + geom_abline(intercept = 0, slope = 1, linetype = 2) + geom_qq(dparams = df.parameters$parameters[[i]]) + geom_qq_line(dparams = df.parameters$parameters[[i]]) + scale_x_continuous(limits = c(-40, 40)) l.plots[[length(l.plots) + 1]] = p1 l.plots[[length(l.plots) + 1]] = p2 } # use patchwork for plotting l.plots[[1]] + l.plots[[2]] + l.plots[[3]] + l.plots[[4]] + l.plots[[5]] + l.plots[[6]] + l.plots[[7]] + l.plots[[8]] + plot_layout(ncol = 4, byrow = F) &amp; theme(text = element_text(size = 16)) ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? # ggsave(&quot;figures/qqplots_normal.pdf&quot;, width = 10, height = 6) The line changes, but it’s still a line. So the QQ plot helps us detect what kind of distribution the data follows. Now, let’s see what happens if distributions don’t have the same shape. #let&#39;s generate some &quot;empirical&quot; data from a beta distribution set.seed(0) df.plot = tibble( residual = rbeta(1000, shape1 = 5, shape2 = 5) ) # data frame with parameters saved in a list column df.parameters = tibble( parameters = list( list(shape1 = 1, shape2 = 5), list(shape1 = 2, shape2 = 5), list(shape1 = 5, shape2 = 2), list(shape1 = 5, shape2 = 1) ) ) # list container for plots l.plots = list() for (i in 1:nrow(df.parameters)){ p1 = ggplot(data = df.plot, aes(x = residual))+ stat_density(geom = &quot;line&quot;, color = &quot;green&quot;, size = 1.5)+ stat_function(fun = &quot;dbeta&quot;, args = df.parameters$parameters[[i]], color = &quot;black&quot;, size = 1.5) + scale_y_continuous(limits = c(0, 3.5)) p2 = ggplot(data = df.plot, aes(sample = residual))+ geom_abline(intercept = 0, slope = 1, linetype = 2)+ geom_qq(distribution = &quot;qbeta&quot;, dparams = df.parameters$parameters[[i]]) + scale_x_continuous(limits = c(0, 1), breaks = seq(.25, .75, .25)) l.plots[[length(l.plots) + 1]] = p1 l.plots[[length(l.plots) + 1]] = p2 } # use patchwork for plotting l.plots[[1]] + l.plots[[2]] + l.plots[[3]] + l.plots[[4]] + l.plots[[5]] + l.plots[[6]] + l.plots[[7]] + l.plots[[8]] + plot_layout(ncol = 4, byrow = F) &amp; theme(text = element_text(size = 16)) ggsave(&quot;figures/qqplots_beta.pdf&quot;, width = 10, height = 6) Figure 7.2: QQ plots indicating different deviations from normality. 7.6 Additional resources 7.6.1 Cheatsheets Probability cheatsheet 7.6.2 Datacamp Foundations of probability in R "],
["simulation-2.html", "Chapter 8 Simulation 2 8.1 Load packages and set plotting theme 8.2 The central limit theorem 8.3 Understanding p-values 8.4 Confidence intervals 8.5 Additional resources", " Chapter 8 Simulation 2 In which we figure out some key statistical concepts through simulation and plotting. On the menu we have: Central limit theorem Sampling distributions p-value Confidence interval 8.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;NHANES&quot;) # data set library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 8.2 The central limit theorem The Central Limit Theorem (CLT) states that the sample mean of a sufficiently large number of independent and identically distributed (i.i.d.) random variables is approximately normally distributed. The larger the sample, the better the approximation. The theorem is a key (“central”) concept in probability theory because it implies that statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. Here are some nice interactive illustrations of the CLT: seeing-theory.brown.edu http://mfviz.com/central-limit/ 8.2.1 Population distribution Let’s first put the information we need for our population distribution in a data frame. # the distribution from which we want to sample (aka the heavy metal distribution) df.population = tibble( numbers = 1:6, probability = c(1/3, 0, 1/6, 1/6, 0, 1/3) ) And then let’s plot it: # plot the distribution ggplot(data = df.population, mapping = aes(x = numbers, y = probability)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;lightblue&quot;, color = &quot;black&quot;) + scale_x_continuous(breaks = df.population$numbers, labels = df.population$numbers, limits = c(0.1, 6.9)) + coord_cartesian(expand = F) Here are the true mean and standard deviation of our population distribution: # mean and standard deviation (see: https://nzmaths.co.nz/category/glossary/standard-deviation-discrete-random-variable) df.population %&gt;% summarize(population_mean = sum(numbers * probability), population_sd = sqrt(sum(numbers^2 * probability) - population_mean^2)) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) population_mean population_sd 3.5 2.06 8.2.2 Distribution of a single sample Let’s draw a single sample of size \\(n = 40\\) from the population distribution and plot it: # make example reproducible set.seed(1) # set the sample size sample_size = 40 # create data frame df.sample = sample(df.population$numbers, size = sample_size, replace = T, prob = df.population$probability) %&gt;% enframe(name = &quot;draw&quot;, value = &quot;number&quot;) # draw a plot of the sample ggplot(data = df.sample, mapping = aes(x = number, y = stat(density))) + geom_histogram(binwidth = 0.5, fill = &quot;lightblue&quot;, color = &quot;black&quot;) + scale_x_continuous(breaks = min(df.sample$number):max(df.sample$number)) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.01))) Here are the sample mean and standard deviation: # print out sample mean and standard deviation df.sample %&gt;% summarize(sample_mean = mean(number), sample_sd = sd(number)) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sample_mean sample_sd 3.73 2.05 8.2.3 The sampling distribution And let’s now create the sampling distribution (making the unrealistic assumption that we know the population distribution). # make example reproducible set.seed(1) # parameters sample_size = 40 # size of each sample sample_n = 10000 # number of samples # define a function that draws samples from a discrete distribution fun.draw_sample = function(sample_size, distribution){ x = sample(distribution$numbers, size = sample_size, replace = T, prob = distribution$probability) return(x) } # generate many samples samples = replicate(n = sample_n, fun.draw_sample(sample_size, df.population)) # set up a data frame with samples df.sampling_distribution = matrix(samples, ncol = sample_n) %&gt;% as_tibble() %&gt;% set_names(str_c(1:ncol(.))) %&gt;% gather(&quot;sample&quot;, &quot;number&quot;) %&gt;% mutate(sample = as.numeric(sample)) %&gt;% group_by(sample) %&gt;% mutate(draw = 1:n()) %&gt;% select(sample, draw, number) %&gt;% ungroup() ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. # turn the data frame into long format and calculate the means of each sample df.sampling_distribution_means = df.sampling_distribution %&gt;% group_by(sample) %&gt;% summarize(mean = mean(number)) %&gt;% ungroup() And plot it: # plot a histogram of the means with density overlaid ggplot(data = df.sampling_distribution_means %&gt;% sample_frac(size = 1, replace = T), mapping = aes(x = mean)) + geom_histogram(aes(y = stat(density)), binwidth = 0.05, fill = &quot;lightblue&quot;, color = &quot;black&quot;) + stat_density(bw = 0.1, size = 2, geom = &quot;line&quot; ) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.01))) That’s the central limit theorem in action! Even though our population distribution was far from normal (and much more heavy-metal like), the means of that distribution are normally distributed. And here are the mean and standard deviation of the sampling distribution: # print out sampling distribution mean and standard deviation df.sampling_distribution_means %&gt;% summarize(sampling_distribution_mean = mean(mean), sampling_distribution_sd = sd(mean)) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sampling_distribution_mean sampling_distribution_sd 3.5 0.33 Here is a data frame that I’ve used for illustrating the idea behind how a sampling distribution is constructed from the population distribution. # data frame for illustration in class df.sampling_distribution %&gt;% filter(sample &lt;= 10, draw &lt;= 4) %&gt;% spread(draw, number) %&gt;% set_names(c(&quot;sample&quot;, str_c(&quot;draw_&quot;, 1:(ncol(.)-1)))) %&gt;% mutate(sample_mean = rowMeans(.[, -1])) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sample draw_1 draw_2 draw_3 draw_4 sample_mean 1 1 6 6 4 4.25 2 3 6 3 6 4.50 3 6 3 6 1 4.00 4 4 6 6 1 4.25 5 1 4 6 3 3.50 6 1 1 6 1 2.25 7 1 6 4 1 3.00 8 1 6 4 6 4.25 9 6 1 6 4 4.25 10 1 1 4 1 1.75 8.2.3.1 Bootstrapping a sampling distribution Of course, in actuality, we never have access to the population distribution. We try to infer characteristics of that distribution (e.g. its mean) from our sample. So using the population distribution to create a sampling distribution is sort of cheating – helpful cheating though since it gives us a sense for the relationship between population, sample, and sampling distribution. It urns out that we can approximate the sampling distribution only using our actual sample. The idea is to take the sample that we drew, and generate new samples from it by drawing with replacement. Essentially, we are treating our original sample like the population from which we are generating random samples to derive the sampling distribution. # make example reproducible set.seed(1) # how many bootstrapped samples shall we draw? n_samples = 1000 # generate a new sample from the original one by sampling with replacement func.bootstrap = function(df){ df %&gt;% sample_frac(size = 1, replace = T) %&gt;% summarize(mean = mean(number)) %&gt;% pull(mean) } # data frame with bootstrapped results df.bootstrap = tibble( bootstrap = 1:n_samples, average = replicate(n = n_samples, func.bootstrap(df.sample)) ) Let’s plot the bootstrapped sampling distribution: # plot the bootstrapped sampling distribution ggplot(data = df.bootstrap, aes(x = average)) + geom_histogram(aes(y = stat(density)), color = &quot;black&quot;, fill = &quot;lightblue&quot;, binwidth = 0.05) + stat_density(geom = &quot;line&quot;, size = 1.5, bw = 0.1) + labs(x = &quot;mean&quot;) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.01))) And let’s calculate the mean and standard deviation: # print out sampling distribution mean and standard deviation df.sampling_distribution_means %&gt;% summarize(bootstrapped_distribution_mean = mean(mean), bootstrapped_distribution_sd = sd(mean)) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) bootstrapped_distribution_mean bootstrapped_distribution_sd 3.5 0.33 Neat, as we can see, the mean and standard deviation of the bootstrapped sampling distribution are very close to the sampling distribution that we generated from the population distribution. 8.2.4 Exploring the CLT 8.2.4.1 Distribution of height In order for the CLT to apply, the following have to hold (approximately): sufficiently large number of variables that affect the outcome the variables are idependent and identically distributed the variables contribute additively, and none of the variables affects the outcome much more strongly than the rest Let’s take a look at a situation where the CLT breaks down. We use survey data collected by the US National Center for Health Statistics (NCHS). The data is from 2009-2012. You can get more information about the NHANES data set by running help(NHANES). Let’s load the data set into our environment first. df.nhanes = NHANES %&gt;% clean_names() %&gt;% distinct(id, .keep_all = T) #drop duplicates Let’s now plot a density of the distribution of womens’ height df.plot = df.nhanes %&gt;% drop_na(height) %&gt;% # remove missing values filter( age &gt;= 18, # only look at adults gender == &quot;female&quot; ) ggplot(data = df.plot, mapping = aes(x = height)) + geom_density(size = 1, fill = &quot;red&quot;, alpha = 0.5, kernel = &quot;gaussian&quot;, bw = 2) + stat_function(fun = &quot;dnorm&quot;, color = &quot;red&quot;, args = list(mean = mean(df.plot$height), sd = sd(df.plot$height)), size = 2) + labs(title = &quot;Women&#39;s height&quot;) + coord_cartesian(expand = F, clip = &quot;off&quot;) Women’s height in the NHANES data set is approximately normally distributed. df.plot = df.nhanes %&gt;% drop_na(height) %&gt;% # remove missing values filter( age &gt;= 18, # only look at adults gender == &quot;male&quot; ) ggplot(data = df.plot, mapping = aes(x = height)) + geom_density(size = 1, fill = &quot;blue&quot;, alpha = 0.5, kernel = &quot;gaussian&quot;, bw = 2) + stat_function(fun = &quot;dnorm&quot;, color = &quot;blue&quot;, args = list(mean = mean(df.plot$height), sd = sd(df.plot$height)), size = 2) + labs(title = &quot;Men&#39;s height&quot;) + coord_cartesian(expand = F, clip = &quot;off&quot;) The same is true for men’s height. df.plot = df.nhanes %&gt;% drop_na(height) %&gt;% # remove missing values filter( age &gt;= 18 # only look at adults ) ggplot(data = df.plot, mapping = aes(x = height))+ geom_density(size = 1, fill = &quot;gray50&quot;, alpha = 0.5, kernel = &quot;gaussian&quot;, bw = 2)+ stat_function(fun = &quot;dnorm&quot;, color = &quot;black&quot;, args = list(mean = mean(df.plot$height), sd = sd(df.plot$height)), size = 2)+ labs(title = &quot;Adults&#39; height&quot;) + coord_cartesian(expand = F, clip = &quot;off&quot;) However, adults’ height is not quite normally distributed. Note that the distribution is too flat in the middle. df.plot = df.nhanes %&gt;% drop_na(height) %&gt;% # remove missing values filter( age &gt;= 18 ) ggplot(data = df.plot, aes(x = height, group = gender, fill = gender))+ geom_density(size = 1, alpha = 0.5, kernel = &quot;gaussian&quot;, bw = 2)+ stat_function(fun = &quot;dnorm&quot;, color = &quot;blue&quot;, args = df.plot %&gt;% filter(gender == &quot;male&quot;) %&gt;% summarise(mean = mean(height), sd = sd(height)) %&gt;% as.list(), size = 2)+ stat_function(fun = &quot;dnorm&quot;, color = &quot;red&quot;, args = df.plot %&gt;% filter(gender == &quot;female&quot;) %&gt;% summarise(mean = mean(height), sd = sd(height)) %&gt;% as.list(), size = 2)+ labs(title = &quot;Adults&#39; height (separated by gender)&quot;)+ theme(legend.position = c(0.9, 0.8)) The fact that adults’ height overall is not normally distributed is because there is a single factor (gender) that accounts for much of the variation. 8.2.4.2 Testing the limits How do sample size and the number of samples affect what the sampling distribution looks like? Here are some simulations. Feel free to play around with: the population distributions to sample from the sample size for each sample the number of samples ggplot(data = tibble(x = c(0, 20)), aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, args = list(mean = 10, sd = 5), size = 1, color = &quot;red&quot;) + stat_function(fun = &quot;dunif&quot;, args = list(min = 0, max = 20), size = 1, color = &quot;green&quot;) + stat_function(fun = &quot;dexp&quot;, args = list(rate = 0.1), size = 1, color = &quot;blue&quot;) + annotate(geom = &quot;text&quot;, label = &quot;normal&quot;, x = 0, y = .03, hjust = 0, color = &quot;red&quot;, size = 6) + annotate(geom = &quot;text&quot;, label = &quot;uniform&quot;, x = 0, y = .055, hjust = 0, color = &quot;green&quot;, size = 6) + annotate(geom = &quot;text&quot;, label = &quot;exponential&quot;, x = 0, y = .105, hjust = 0, color = &quot;blue&quot;, size = 6) # Parameters for the simulation n_samples = c(10, 100, 1000, 10000) sample_size = c(5, 10, 25, 100) distributions = c(&quot;normal&quot;, &quot;uniform&quot;, &quot;exponential&quot;) # take samples (of size n) from specified distribution and calculate the mean fun.sample_mean = function(n, distribution){ if (distribution == &quot;normal&quot;){ tmp = rnorm(n, mean = 10, sd = 5) }else if (distribution == &quot;uniform&quot;){ tmp = runif(n, min = 0, max = 20) }else if (distribution == &quot;exponential&quot;){ tmp = rexp(n, rate = 0.1) } return(mean(tmp)) } df.central_limit = tibble() for (i in 1:length(n_samples)){ for (j in 1:length(sample_size)){ for (k in 1:length(distributions)){ # calculate sample mean sample_mean = replicate(n_samples[i], fun.sample_mean(sample_size[j], distributions[k])) df.tmp = tibble(n_samples = n_samples[i], sample_size = sample_size[j], distribution = distributions[k], mean_value = list(sample_mean)) df.central_limit = rbind(df.central_limit, df.tmp) } } } # transform from list column df.plot = df.central_limit %&gt;% unnest() %&gt;% mutate(sample_size = str_c(&quot;sample size = &quot;, sample_size), sample_size = factor(sample_size, levels = str_c(&quot;sample size = &quot;, c(5, 10, 25, 100))), n_samples = str_c(&quot;n samples = &quot;, n_samples), distribution = factor(distribution, levels = c(&quot;normal&quot;, &quot;uniform&quot;, &quot;exponential&quot;)) ) # densities of sample means ggplot(df.plot, aes(x = mean_value, color = distribution))+ stat_density(geom = &quot;line&quot;, position = &quot;identity&quot;)+ facet_grid(n_samples ~ sample_size, scales = &quot;free&quot;)+ scale_x_continuous(breaks = c(0, 10, 20))+ coord_cartesian(xlim = c(0, 20))+ labs(x = &quot;sample mean&quot;)+ theme( axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10), strip.text.y = element_text(size = 6), strip.text.x = element_text(size = 8), legend.position = &quot;bottom&quot;, panel.background = element_rect(color = &quot;black&quot;) ) No matter where we start, as long as we draw samples that are independent and identically distributed, and these samples combine in an additive way, we end up with a normal distribution (note that this takes considerably longer when we start with an exponential distribution – shown in blue – compared to the other population distributions). 8.3 Understanding p-values The p-value is the probability of finding the observed, or more extreme, results when the null hypothesis (\\(H_0\\)) is true. \\[ \\text{p-value = p(observed or more extreme sample statistic} | H_{0}=\\text{true}) \\] What we are really interested in is the probability of a hypothesis given the data. However, frequentist statistics doesn’t give us this probability – we’ll get to Bayesian statistics later in the course. Instead, we define a null hypothesis, construct a sampling distribution that tells us what we would expect the test statistic of interest to look like if the null hypothesis were true. We reject the null hypothesis in case our observed data would be unlikely if the null hypothesis were true. An intutive way for illustrating (this rather unintuitive procedure) is the permutation test. 8.3.1 Permutation test Let’s start by generating some random data from two different normal distributions (simulating a possible experiment). # make example reproducible set.seed(1) # generate data from two conditions df.permutation = tibble( control = rnorm(25, mean = 5.5, sd = 2), experimental = rnorm(25, mean = 4.5, sd = 1.5) ) %&gt;% gather(&quot;condition&quot;, &quot;performance&quot;) Here is a summary of how each group performed: df.permutation %&gt;% group_by(condition) %&gt;% summarize(mean = mean(performance), sd = sd(performance)) %&gt;% gather(&quot;statistic&quot;, &quot;value&quot;, - condition) %&gt;% spread(condition, value) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) statistic control experimental mean 5.84 4.55 sd 1.90 1.06 Let’s plot the results: ggplot(data = df.permutation, mapping = aes(x = condition, y = performance)) + geom_point(position = position_jitter(height = 0, width = 0.1), alpha = 0.5) + stat_summary(fun.data = mean_cl_boot, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, color = &quot;black&quot;, fill = &quot;white&quot;, size = 4) + scale_y_continuous(breaks = 0:10, labels = 0:10, limits = c(0, 10)) We are interested in the difference in the mean performance between the two groups: # calculate the difference between conditions difference_actual = df.permutation %&gt;% group_by(condition) %&gt;% summarize(mean = mean(performance)) %&gt;% pull(mean) %&gt;% diff() The difference in the mean rating between the control and experimental condition is -1.2889834. Is this difference between conditions statistically significant? What we are asking is: what are the chances that a result like this (or more extreme) could have come about due to chance? Let’s answer the question using simulation. Here is the main idea: imagine that we were very sloppy in how we recorded the data, and now we don’t remember anymore which participants were in the controld condition and which ones were in experimental condition (we still remember though, that we tested 25 participants in each condition). set.seed(0) df.permutation = df.permutation %&gt;% mutate(permutation = sample(condition)) #randomly assign labels df.permutation %&gt;% group_by(permutation) %&gt;% summarize(mean = mean(performance), sd = sd(performance)) %&gt;% ungroup() %&gt;% summarize(diff = diff(mean)) ## # A tibble: 1 x 1 ## diff ## &lt;dbl&gt; ## 1 -0.0223 Here, the difference between the two conditions is 0.0223078. After randomly shuffling the condition labels, this is how the results would look like: ggplot(data = df.permutation, aes(x = permutation, y = performance))+ geom_point(aes(color = condition), position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = mean_cl_boot, geom = &#39;linerange&#39;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &#39;point&#39;, shape = 21, color = &quot;black&quot;, fill = &quot;white&quot;, size = 4) + scale_y_continuous(breaks = 0:10, labels = 0:10, limits = c(0, 10)) The idea is now that, similar to bootstrapping above, we can get a sampling distribution of the difference in the means between the two conditions (assuming that the null hypothesis were true), by randomly shuffling the labels and calculating the difference in means (and doing this many times). What we get is a distribution of the differences we would expect, if there was no effect of condition. set.seed(1) n_permutations = 500 # permutation function func_permutations = function(df){ df %&gt;% mutate(condition = sample(condition)) %&gt;% #we randomly shuffle the condition labels group_by(condition) %&gt;% summarize(mean = mean(performance)) %&gt;% pull(mean) %&gt;% diff() } # data frame with permutation results df.permutations = tibble( permutation = 1:n_permutations, mean_difference = replicate(n = n_permutations, func_permutations(df.permutation)) ) #plot the distribution of the differences ggplot(data = df.permutations, aes(x = mean_difference)) + geom_histogram(aes(y = stat(density)), color = &quot;black&quot;, fill = &quot;lightblue&quot;, binwidth = 0.05) + stat_density(geom = &quot;line&quot;, size = 1.5, bw = 0.2) + geom_vline(xintercept = difference_actual, color = &quot;red&quot;, size = 2) + labs(x = &quot;difference between means&quot;) + scale_x_continuous(breaks = seq(-1.5, 1.5, 0.5), labels = seq(-1.5, 1.5, 0.5), limits = c(-2, 2)) + coord_cartesian(expand = F, clip = &quot;off&quot;) ## Warning: Removed 2 rows containing missing values (geom_bar). And we can then simply calculate the p-value by using some basic data wrangling (i.e. finding the proportion of differences that were as or more extreme than the one we observed). #calculate p-value of our observed result df.permutations %&gt;% summarize(p_value = sum(mean_difference &lt;= difference_actual)/n()) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.006 8.4 Confidence intervals The definition of the confidence interval is the following: “If we were to repeat the experiment over and over, then 95% of the time the confidence intervals contain the true mean.” If we assume normally distributed data (and a large enough sample size), then we can calculate the confidence interval on the estimate of the mean in the following way: \\(\\overline X \\pm Z \\frac{s}{\\sqrt{n}}\\), where \\(Z\\) equals the value of the standard normal distribution for the desired level of confidence. For smaller sample sizes, we can use the \\(t\\)-distribution instead with \\(n-1\\) degrees of freedom. For larger \\(n\\) the \\(t\\)-distribution closely approximates the normal distribution. So let’s run a a simulation to check whether the definition of the confidence interval seems right. We will use our heavy metal distribution from above, take samples from the distribution, calculate the mean and confidende interval, and check how often the true mean of the population (\\(M = 3.5\\)) is contained within the confidence interval. # make example reproducible set.seed(1) # parameters sample_size = 25 # size of each sample sample_n = 20 # number of samples confidence_level = 0.95 # desired level of confidence # define a function that draws samples and calculates means and CIs fun.confidence = function(sample_size, distribution){ df = tibble( values = sample(distribution$numbers, size = sample_size, replace = T, prob = distribution$probability)) %&gt;% summarize(mean = mean(values), sd = sd(values), n = n(), # confidence interval assuming a normal distribution # error = qnorm(1-(1-confidence_level)*2) * sd / sqrt(n), # assuming a t-distribution (more conservative, appropriate for smaller # sample sizes) error = qt(1-(1-confidence_level)/2, df = n-1) * sd / sqrt(n), conf_low = mean - error, conf_high = mean + error) return(df) } # build data frame of confidence intervals df.confidence = tibble() for(i in 1:sample_n){ df.tmp = fun.confidence(sample_size, df.population) df.confidence = df.confidence %&gt;% bind_rows(df.tmp) } # code which CIs contain the true value, and which ones don&#39;t population_mean = 3.5 df.confidence = df.confidence %&gt;% mutate(sample = 1:n(), conf_index = ifelse(conf_low &gt; population_mean | conf_high &lt; population_mean, &#39;outside&#39;, &#39;inside&#39;)) # plot the result ggplot(data = df.confidence, aes(x = sample, y = mean, color = conf_index))+ geom_hline(yintercept = 3.5, color = &quot;red&quot;)+ geom_point()+ geom_linerange(aes(ymin = conf_low, ymax = conf_high))+ coord_flip()+ scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;), labels = c(&quot;inside&quot;, &quot;outside&quot;))+ theme(axis.text.y = element_text(size = 12), legend.position = &quot;none&quot;) So, out of the 20 samples that we drew the 95% confidence interval of 1 sample did not contain the true mean. That makes sense! Feel free to play around with the code above. For example, change the sample size, the number of samples, the confidence level. 8.5 Additional resources 8.5.1 Datacamp Foundations of Inference "],
["modeling-data.html", "Chapter 9 Modeling data 9.1 Load packages and set plotting theme 9.2 Things that came up in class 9.3 Modeling data 9.4 Hypothesis testing: “One-sample t-test” 9.5 Additional resources", " Chapter 9 Modeling data 9.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;magrittr&quot;) # for wrangling library(&quot;patchwork&quot;) # for making figure panels library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 9.2 Things that came up in class 9.2.1 Calculating RMSE using magrittr verbs Here is how we can calculate the root mean squared error using the pipe all the way through. Note that you have to load the magrittr package in order for this to work. data = c(1, 3, 4, 2, 5) prediction = c(1, 2, 2, 1, 4) # calculate root mean squared error the pipe way rmse = prediction %&gt;% subtract(data) %&gt;% raise_to_power(2) %&gt;% mean() %&gt;% sqrt() %&gt;% print() ## [1] 1.183216 9.2.2 Relationship between probability and likelihood margin = 1 point = 0 ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, xlim = c(point - margin, point + margin), fill = &quot;red&quot;, alpha = 0.5) + stat_function(fun = &quot;dnorm&quot;, size = 1) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.1: Probability is the area under the curve of the density point = 0 param_mean = 1 param_sd = 1 ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1, args = list(mean = param_mean, sd = param_sd)) + geom_segment(aes( x = point, y = 0, xend = point, yend = dnorm(point, mean = param_mean, sd = param_sd)), color = &quot;red&quot;, size = 1 ) + geom_segment(aes( x = -3, y = dnorm(point, mean = param_mean, sd = param_sd), xend = point, yend = dnorm(point, mean = param_mean, sd = param_sd)), color = &quot;red&quot;, size = 1) + geom_point(x = point, y = dnorm(point, mean = param_mean, sd = param_sd), shape = 21, fill = &quot;red&quot;, size = 4) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.2: Likelihood is a particular value. point = 1 p1 = ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, xlim = c(-3, point), fill = &quot;red&quot;, alpha = 0.5) + stat_function(fun = &quot;dnorm&quot;, size = 1) + geom_point(x = point, y = dnorm(point), shape = 21, fill = &quot;red&quot;, size = 3) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) p2 = ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;pnorm&quot;, size = 1) + geom_segment(mapping = aes(x = -3, y = pnorm(point), xend = point, yend = pnorm(point)), color = &quot;red&quot;, size = 1) + geom_point(x = point, y = pnorm(point), shape = 21, fill = &quot;red&quot;, size = 3) + labs(y = &quot;cum prob&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(breaks = c(0, 0.5, 1), expand = expand_scale(add = c(0.01, 0.1))) p1 + p2 + plot_layout(ncol = 1) Figure 9.3: Relationship between density and cumulative probability distribution. point = 0 p1 = ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + geom_segment(mapping = aes(x = -3, y = dnorm(point), xend = point, yend = dnorm(point)), color = &quot;red&quot;, size = 1) + stat_function(fun = &quot;dnorm&quot;, size = 1) + geom_point(x = point, y = dnorm(point), shape = 21, fill = &quot;red&quot;, size = 3) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) p2 = ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;pnorm&quot;, size = 1) + geom_abline(slope = dnorm(point), intercept = pnorm(point) - dnorm(point) * point, color = &quot;red&quot;, size = 1) + geom_point(x = point, y = pnorm(point), shape = 21, fill = &quot;red&quot;, size = 3) + labs(y = &quot;cum prob&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(breaks = c(0, 0.5, 1), expand = expand_scale(add = c(0.01, 0.1))) p1 + p2 + plot_layout(ncol = 1) Figure 9.4: The density is the first derivative of the cumulative probability distribution. The likelihood is the value of the slope in the cumulative probability distribution. margin = 0.1 point_blue = -1 point_red = 0 ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, xlim = c(point_red - margin, point_red + margin), fill = &quot;red&quot;, alpha = 0.5) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, xlim = c(point_blue - margin, point_blue + margin), fill = &quot;blue&quot;, alpha = 0.5) + stat_function(fun = &quot;dnorm&quot;, size = 1) + geom_segment(mapping = aes(x = -3, y = dnorm(point_red), xend = point_red, yend = dnorm(point_red)), color = &quot;red&quot;, size = 1) + geom_segment(mapping = aes(x = -3, y = dnorm(point_blue), xend = point_blue, yend = dnorm(point_blue)), color = &quot;blue&quot;, size = 1) + geom_point(x = point_red, y = dnorm(point_red), shape = 21, fill = &quot;red&quot;, size = 4) + geom_point(x = point_blue, y = dnorm(point_blue), shape = 21, fill = &quot;blue&quot;, size = 4) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.5: The relative likelihood of two observations is the same as the relative probability of two areas under the curve as the margin of these areas goes to 0. (pnorm(point_red + margin) - pnorm(point_red - margin)) / (pnorm(point_blue + margin) - pnorm(point_blue - margin)) ## [1] 1.64598 dnorm(point_red) / dnorm(point_blue) ## [1] 1.648721 9.3 Modeling data 9.3.1 Simplicity vs. accuracy trade-off # make example reproducible set.seed(1) n_samples = 20 # sample size n_parameters = 15 # number of parameters in the polynomial regression # generate data df.data = tibble( x = runif(n_samples, min = 0, max = 10), y = 10 + 3 * x + 3 * x^2 + rnorm(n_samples, sd = 20) ) # plot a fit to the data ggplot(data = df.data, mapping = aes(x = x, y = y)) + geom_point(size = 3) + # geom_hline(yintercept = mean(df.data$y), color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, degree = n_parameters, raw = TRUE)) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) Figure 9.6: Tradeoff between fit and model simplicity. # make example reproducible set.seed(1) # n_samples = 20 n_samples = 3 df.pre = tibble( x = runif(n_samples, min = 0, max = 10), y = 2 * x + rnorm(n_samples, sd = 1) ) # plot a fit to the data ggplot(data = df.pre, mapping = aes(x = x, y = y)) + geom_point(size = 3) + # geom_hline(yintercept = mean(df.pre$y), color = &quot;blue&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, 1, raw=TRUE)) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) Figure 9.7: Figure that I used to illustrate that fitting more data points with fewer parameter is more impressive. 9.3.2 Error definitions and best estimators Let’s start with some simple data: df.data = tibble( observation = 1:5, value = c(1, 3, 5, 9, 14) ) And plot the data ggplot(data = df.data, mapping = aes(x = &quot;1&quot;, y = value)) + geom_point(size = 3) + scale_y_continuous(breaks = seq(0, 16, 2), limits = c(0, 16)) + theme(panel.grid.major.y = element_line(color = &quot;gray80&quot;, linetype = 2), axis.title.x = element_blank(), axis.text.x = element_blank(), text = element_text(size = 24)) This is what the sum of absolute errors looks like for a given value_predicted. value_predicted = 7 df.data = df.data %&gt;% mutate(prediction = value_predicted, error_absolute = abs(prediction - value)) ggplot(data = df.data, mapping = aes(x = observation, y = value)) + geom_segment(mapping = aes(x = observation, xend = observation, y = value_predicted, yend = value ), color = &quot;blue&quot;, size = 1) + geom_line(data = tibble(x = c(1, 5), y = value_predicted), mapping = aes(x = x, y = y), size = 1, color = &quot;green&quot;) + geom_point(size = 4) + annotate(x = 1, y = 15.5, geom = &quot;text&quot;, label = str_c(&quot;Prediction = &quot;, value_predicted), size = 8, hjust = 0, vjust = 1, color = &quot;green&quot;) + annotate(x = 1, y = 13.5, geom = &quot;text&quot;, label = str_c(&quot;Sum of absolute errors = &quot;, sum(df.data$error_absolute)), size = 8, hjust = 0, vjust = 1, color = &quot;blue&quot;) + annotate(x = 5, y = value_predicted, geom = &quot;text&quot;, label = parse(text = str_c(&quot;{hat(Y)&quot;,&quot;==b[0]}==&quot;, value_predicted)), hjust = -0.1, size = 8) + scale_x_continuous(breaks = df.data$observation, labels = parse(text = str_c(&#39;e[&#39;,df.data$observation,&#39;]&#39;, &quot;==&quot;, df.data$error_absolute)), limits = c(1, 6)) + scale_y_continuous(breaks = seq(0, 16, 2), limits = c(0, 16)) + theme(panel.grid.major.y = element_line(color = &quot;gray80&quot;, linetype = 2), axis.title.x = element_blank(), text = element_text(size = 24)) Figure 9.8: Sum of absolute errors. Play around with the code below to see how using (1) the sum of absolute errors, or (2) the sum of squared errors affects what estimate minimizes the error. value_predicted = seq(0, 50, 0.1) # value_predicted = seq(0, 10, 1) df.data = tibble( observation = 1:5, value = c(1, 3, 5, 9, 140) ) # function that calculates the sum absolute error fun.sum_absolute_error = function(prediction){ x = df.data$value sum_absolute_error = sum(abs(x-prediction)) return(sum_absolute_error) } # function that calculates the sum squared error fun.sum_squared_error = function(prediction){ x = df.data$value sum_squared_error = sum((x-prediction)^2) return(sum_squared_error) } df.model = tibble( estimate = value_predicted, sum_absolute_error = map_dbl(value_predicted, fun.sum_absolute_error), sum_squared_error = map_dbl(value_predicted, fun.sum_squared_error) ) ggplot(data = df.model, mapping = aes(x = estimate, # y = sum_absolute_error)) + y = sum_squared_error)) + geom_line(size = 1) + # labs(y = &quot;Sum absolute error&quot;) labs(y = &quot;Sum of squared errors&quot;) Error definition Best estimator Count of errors Mode = most frequent value Sum of absolute errors Median = middle observation of all values Sum of squared errors Mean = average of all values mu = 0 sigma = 1 mean = mu median = mu mode = mu ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1) + geom_segment(aes(x = median, xend = median, y = dnorm(median), yend = 0), color = &quot;green&quot;, size = 2) + geom_segment(aes(x = mode, xend = mode, y = dnorm(mode), yend = 0), color = &quot;red&quot;, size = 2) + geom_segment(aes(x = mean, xend = mean, y = dnorm(mean), yend = 0), color = &quot;blue&quot;, size = 2) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = -2:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.9: Mean, median, and mode on the normal distribution. rate = 1 mean = rate median = rate * log(2) mode = 0 ggplot(data = tibble(x = c(-0.1, 3)), mapping = aes(x = x)) + stat_function(fun = &quot;dexp&quot;, size = 1) + geom_segment(aes(x = median, xend = median, y = dexp(median), yend = 0), color = &quot;green&quot;, size = 2) + geom_segment(aes(x = mode, xend = mode, y = dexp(mode), yend = 0), color = &quot;red&quot;, size = 2) + geom_segment(aes(x = mean, xend = mean, y = dexp(mean), yend = 0), color = &quot;blue&quot;, size = 2) + labs(y = &quot;density&quot;) + scale_x_continuous(breaks = 0:2, expand = c(0, 0)) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) Figure 9.10: Mean, median, and mode on the exponential distribution. 9.3.3 Sampling distributions for median and mean # make example reproducible set.seed(1) sample_size = 40 # size of each sample sample_n = 1000 # number of samples # draw sample fun.draw_sample = function(sample_size, distribution){ x = 50 + rnorm(sample_size) return(x) } # generate many samples samples = replicate(n = sample_n, fun.draw_sample(sample_size, df.population)) # set up a data frame with samples df.sampling_distribution = matrix(samples, ncol = sample_n) %&gt;% as_tibble(.name_repair = &quot;minimal&quot;) %&gt;% set_names(str_c(1:ncol(.))) %&gt;% gather(&quot;sample&quot;, &quot;number&quot;) %&gt;% mutate(sample = as.numeric(sample)) %&gt;% group_by(sample) %&gt;% mutate(draw = 1:n()) %&gt;% select(sample, draw, number) %&gt;% ungroup() # turn the data frame into long format and calculate the means of each sample df.sampling_distribution_means = df.sampling_distribution %&gt;% group_by(sample) %&gt;% summarize(mean = mean(number), median = median(number)) %&gt;% ungroup() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -sample) And plot it: # plot a histogram of the means with density overlaid ggplot(data = df.sampling_distribution_means, mapping = aes(x = value, color = index)) + stat_density(bw = 0.1, size = 2, geom = &quot;line&quot;) + scale_y_continuous(expand = expand_scale(mult = c(0, 0.01))) 9.4 Hypothesis testing: “One-sample t-test” df.internet = read_table2(file = &quot;data/internet_access.txt&quot;) %&gt;% clean_names() ## Parsed with column specification: ## cols( ## State = col_character(), ## Internet = col_double(), ## College = col_double(), ## Auto = col_double(), ## Density = col_double() ## ) df.internet %&gt;% mutate(i = 1:n()) %&gt;% select(i, internet, everything()) %&gt;% head(10) %&gt;% kable(digits = 1) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) i internet state college auto density 1 79.0 AK 28.0 1.2 1.2 2 63.5 AL 23.5 1.3 94.4 3 60.9 AR 20.6 1.7 56.0 4 73.9 AZ 27.4 1.3 56.3 5 77.9 CA 31.0 0.8 239.1 6 79.4 CO 37.8 1.0 48.5 7 77.5 CT 37.2 1.0 738.1 8 74.5 DE 29.8 1.1 460.8 9 74.3 FL 27.2 1.2 350.6 10 72.2 GA 28.3 1.1 168.4 # parameters per model pa = 1 pc = 0 df.model = df.internet %&gt;% select(internet, state) %&gt;% mutate(i = 1:n(), compact_b = 75, augmented_b = mean(internet), compact_se = (internet - compact_b)^2, augmented_se = (internet - augmented_b)^2) %&gt;% select(i, state, internet, contains(&quot;compact&quot;), contains(&quot;augmented&quot;)) df.model %&gt;% summarize(augmented_sse = sum(augmented_se), compact_sse = sum(compact_se), pre = 1 - augmented_sse / compact_sse, f = (pre / (pa - pc)) / ((1 - pre) / (nrow(df.model) - pa)), p_value = 1 - pf(f, pa - pc, nrow(df.model) - 1), mean = mean(internet), sd = sd(internet)) %&gt;% kable() %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) augmented_sse compact_sse pre f p_value mean sd 1355.028 1595.71 0.1508305 8.703441 0.0048592 72.806 5.258673 df1 = 1 df2 = 49 ggplot(data = tibble(x = c(0, 10)), mapping = aes(x = x)) + # stat_function(fun = &quot;df&quot;, # geom = &quot;area&quot;, # fill = &quot;red&quot;, # alpha = 0.5, # args = list(df1 = df1, # df2 = df2), # size = 1, # xlim = c(qf(0.95, df1 = df1, df2 = df2), 10) # ) + stat_function(fun = &quot;df&quot;, args = list(df1 = df1, df2 = df2), size = 0.5) + scale_y_continuous(expand = expand_scale(add = c(0.001, 0.1))) + labs(y = &quot;density&quot;) Figure 9.11: The F distribution We’ve implemented a one sample t-test (compare the p-value here to the one I computed above using PRE and the F statistic). t.test(df.internet$internet, mu = 75) ## ## One Sample t-test ## ## data: df.internet$internet ## t = -2.9502, df = 49, p-value = 0.004859 ## alternative hypothesis: true mean is not equal to 75 ## 95 percent confidence interval: ## 71.3115 74.3005 ## sample estimates: ## mean of x ## 72.806 9.5 Additional resources 9.5.1 Reading Judd, C. M., McClelland, G. H., &amp; Ryan, C. S. (2011). Data analysis: A model comparison approach. Routledge. –&gt; Chapters 1–4 9.5.2 Datacamp Foundations of Inference "],
["linear-model-1.html", "Chapter 10 Linear model 1 10.1 Load packages and set plotting theme 10.2 Things that came up in class 10.3 Regression 10.4 Credit example 10.5 Additional resources", " Chapter 10 Linear model 1 10.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. 10.2 Things that came up in class 10.2.1 Building a sampling distribution of PRE Here is the general procedure for building a sampling distribution of the proportinal reduction of error (PRE). In this instance, I compare the following two models Model C (compact): \\(Y_i = 75 + \\epsilon_i\\) Model A (augmented): \\(Y_i = \\overline Y + \\epsilon_i\\) whereby I assume that \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma)\\). For this example, I assume that I know the population distribution. I first draw a sample from that distribution, and then calculate PRE. # make example reproducible set.seed(1) # set the sample size sample_size = 50 # draw sample from the population distribution (I&#39;ve fixed sigma -- the standard deviation # of the population distribution to be 5) df.sample = tibble( observation = 1:sample_size, value = 75 + rnorm(sample_size, mean = 0, sd = 5) ) # calculate SSE for each model, and then PRE based on that df.summary = df.sample %&gt;% mutate(compact = 75, augmented = mean(value)) %&gt;% summarize(sse_compact = sum((value - compact)^2), sse_augmented = sum((value - augmented)^2), pre = 1 - (sse_augmented/sse_compact)) To generate the sampling distribution, I assume that the null hypothesis is true, and then take a look at what values for PRE we could expect by chance for our given sample size. # simulation parameters n_samples = 1000 sample_size = 50 mu = 75 # true mean of the distribution sigma = 5 # true standard deviation of the errors # function to draw samples from the population distribution fun.draw_sample = function(sample_size, sigma){ sample = mu + rnorm(sample_size, mean = 0, sd = sigma) return(sample) } # draw samples samples = n_samples %&gt;% replicate(fun.draw_sample(sample_size, sigma)) %&gt;% t() # transpose the resulting matrix (i.e. flip rows and columns) # put samples in data frame and compute PRE df.samples = samples %&gt;% as_tibble(.name_repair = &quot;unique&quot;) %&gt;% mutate(sample = 1:n()) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -sample) %&gt;% mutate(compact = mu) %&gt;% group_by(sample) %&gt;% mutate(augmented = mean(value)) %&gt;% summarize(sse_compact = sum((value - compact)^2), sse_augmented = sum((value - augmented)^2), pre = 1 - sse_augmented/sse_compact) # plot the sampling distribution for PRE ggplot(data = df.samples, mapping = aes(x = pre)) + stat_density(geom = &quot;line&quot;) # calculate the p-value for our sample df.samples %&gt;% summarize(p_value = sum(pre &gt;= df.summary$pre)/n()) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.394 Some code I wrote to show a subset of the samples. samples %&gt;% as_tibble(.name_repair = &quot;unique&quot;) %&gt;% mutate(sample = 1:n()) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -sample) %&gt;% mutate(compact = mu) %&gt;% group_by(sample) %&gt;% mutate(augmented = mean(value)) %&gt;% ungroup() %&gt;% mutate(index = str_extract(index, pattern = &quot;\\\\-*\\\\d+\\\\.*\\\\d*&quot;), index = as.numeric(index)) %&gt;% filter(index &lt; 6) %&gt;% arrange(sample, index) %&gt;% head(15) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sample index value compact augmented 1 1 76.99 75 75.59 1 2 71.94 75 75.59 1 3 76.71 75 75.59 1 4 69.35 75 75.59 1 5 82.17 75 75.59 2 1 71.90 75 74.24 2 2 75.21 75 74.24 2 3 70.45 75 74.24 2 4 75.79 75 74.24 2 5 71.73 75 74.24 3 1 77.25 75 75.38 3 2 74.91 75 75.38 3 3 73.41 75 75.38 3 4 70.35 75 75.38 3 5 67.56 75 75.38 10.2.2 Correlation # make example reproducible set.seed(1) n_samples = 20 # create correlated data df.correlation = tibble( x = runif(n_samples, min = 0, max = 100), y = x + rnorm(n_samples, sd = 15) ) # plot the data ggplot(data = df.correlation, mapping = aes(x = x, y = y)) + geom_point(size = 2) + labs(x = &quot;chocolate&quot;, y = &quot;happiness&quot;) 10.2.2.1 Variance Variance is the average squared difference between each data point and the mean: \\(Var(Y) = \\frac{\\sum_{i = 1}^n(Y_i - \\overline Y)^2}{n-1}\\) # make example reproducible set.seed(1) # generate random data df.variance = tibble( x = 1:10, y = runif(10, min = 0, max = 1) ) # plot the data ggplot(data = df.variance, mapping = aes(x = x, y = y)) + geom_segment(aes(x = x, xend = x, y = y, yend = mean(df.variance$y))) + geom_point(size = 3) + geom_hline(yintercept = mean(df.variance$y), color = &quot;blue&quot;) + theme(axis.text.x = element_blank(), axis.title.x = element_blank(), axis.ticks.x = element_blank() ) 10.2.2.2 Covariance Covariance is defined in the following way: \\(Cov(X,Y) = \\sum_{i=1}^n\\frac{(X_i-\\overline X)(Y_i-\\overline Y)}{n-1}\\) # make example reproducible set.seed(1) # generate random data df.covariance = tibble( x = runif(20, min = 0, max = 1), y = x + rnorm(x, mean = 0.5, sd = 0.25) ) # plot the data ggplot(df.covariance, aes(x = x, y = y)) + geom_point(size = 3) + theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) Add lines for \\(\\overline X\\) and \\(\\overline Y\\) to the data: ggplot(df.covariance, aes(x = x, y = y)) + geom_hline(yintercept = mean(df.covariance$y), color = &quot;red&quot;, size = 1) + geom_vline(xintercept = mean(df.covariance$x), color = &quot;red&quot;, size = 1) + geom_point(size = 3) + theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) Illustrate how covariance is computed by drawing the distance to \\(\\overline X\\) and \\(\\overline Y\\) for three data points: df.plot = df.covariance %&gt;% mutate(covariance = (x-mean(x)) *( y-mean(y))) %&gt;% arrange(abs(covariance)) %&gt;% mutate(color = NA) mean_xy = c(mean(df.covariance$x), mean(df.covariance$y)) df.plot$color[1] = 1 df.plot$color[10] = 2 df.plot$color[19] = 3 ggplot(df.plot, aes(x = x, y = y, color = as.factor(color))) + geom_segment(data = df.plot %&gt;% filter(color == 1), mapping = aes(x = x, xend = mean_xy[1], y = y, yend = y), size = 1) + geom_segment(data = df.plot %&gt;% filter(color == 1), mapping = aes(x = x, xend = x, y = y, yend = mean_xy[2]), size = 1) + geom_segment(data = df.plot %&gt;% filter(color == 2), mapping = aes(x = x, xend = mean_xy[1], y = y, yend = y), size = 1) + geom_segment(data = df.plot %&gt;% filter(color == 2), mapping = aes(x = x, xend = x, y = y, yend = mean_xy[2]), size = 1) + geom_segment(data = df.plot %&gt;% filter(color == 3), mapping = aes(x = x, xend = mean_xy[1], y = y, yend = y), size = 1) + geom_segment(data = df.plot %&gt;% filter(color == 3), mapping = aes(x = x, xend = x, y = y, yend = mean_xy[2]), size = 1) + geom_hline(yintercept = mean_xy[2], color = &quot;red&quot;, size = 1) + geom_vline(xintercept = mean_xy[1], color = &quot;red&quot;, size = 1) + geom_point(size = 3) + theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), legend.position = &quot;none&quot;) 10.2.2.3 Spearman’s rank order correlation Spearman’s \\(\\rho\\) captures the extent to which the relationship between two variables is monotonic. # create data frame with data points and ranks df.ranking = tibble( x = c(1.2, 2.5, 4.5), y = c(2.2, 1, 3.3), label = str_c(&quot;(&quot;, x, &quot;, &quot;, y, &quot;)&quot;), x_rank = dense_rank(x), y_rank = dense_rank(y), label_rank = str_c(&quot;(&quot;, x_rank, &quot;, &quot;, y_rank, &quot;)&quot;) ) # plot the data (and show their ranks) ggplot(df.ranking, aes(x = x, y = y)) + geom_point(size = 3) + geom_text(aes(label = label), hjust = -0.2, vjust = 0, size = 6) + geom_text(aes(label = label_rank), hjust = -0.4, vjust = 2, size = 6, color = &quot;red&quot;) + coord_cartesian(xlim = c(1, 6), ylim = c(0, 4)) Show that Spearman’s \\(\\rho\\) is equivalent to Pearson’s \\(r\\) applied to ranked data. # data set df.spearman = df.correlation %&gt;% mutate(x_rank = dense_rank(x), y_rank = dense_rank(y)) # correlation df.spearman %&gt;% summarize(r = cor(x, y, method = &quot;pearson&quot;), spearman = cor(x, y, method = &quot;spearman&quot;), r_ranks = cor(x_rank, y_rank)) ## # A tibble: 1 x 3 ## r spearman r_ranks ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.851 0.836 0.836 # plot ggplot(df.spearman, aes(x = x_rank, y = y_rank)) + geom_point(size = 3) + scale_x_continuous(breaks = 1:20) + scale_y_continuous(breaks = 1:20) + theme(axis.text = element_text(size = 10)) # show some of the data and ranks df.spearman %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) x y x_rank y_rank 26.55 49.23 5 10 37.21 43.06 6 7 57.29 47.97 10 8 90.82 57.60 18 11 20.17 37.04 3 6 89.84 89.16 17 19 94.47 94.22 19 20 66.08 80.24 12 16 62.91 75.23 11 14 6.18 15.09 1 2 Comparison between \\(r\\) and \\(\\rho\\) for a given data set: # data set df.example = tibble( x = 1:10, y = c(-10, 2:9, 20) ) %&gt;% mutate(x_rank = dense_rank(x), y_rank = dense_rank(y)) # correlation df.example %&gt;% summarize(r = cor(x, y, method = &quot;pearson&quot;), spearman = cor(x, y, method = &quot;spearman&quot;), r_ranks = cor(x_rank, y_rank)) ## # A tibble: 1 x 3 ## r spearman r_ranks ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.878 1.000 1.000 # plot ggplot(df.example, # aes(x = x_rank, y = y_rank)) + # see the ranked data aes(x = x, y = y)) + # see the original data geom_point(size = 3) + theme(axis.text = element_text(size = 10)) Another example # make example reproducible set.seed(1) # data set df.example2 = tibble( x = c(1, rnorm(8, mean = 5, sd = 1), 10), y = c(-10, rnorm(8, sd = 1), 20) ) %&gt;% mutate(x_rank = dense_rank(x), y_rank = dense_rank(y)) # correlation df.example2 %&gt;% summarize(r = cor(x, y, method = &quot;pearson&quot;), spearman = cor(x, y, method = &quot;spearman&quot;), r_ranks = cor(x_rank, y_rank)) ## # A tibble: 1 x 3 ## r spearman r_ranks ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.919 0.467 0.467 # plot ggplot(df.example2, # aes(x = x_rank, y = y_rank)) + # see the ranked data aes(x = x, y = y)) + # see the original data geom_point(size = 3) + theme(axis.text = element_text(size = 10)) 10.3 Regression # make example reproducible set.seed(1) # set the sample size n_samples = 10 # generate correlated data df.regression = tibble( chocolate = runif(n_samples, min = 0, max = 100), happiness = chocolate * 0.5 + rnorm(n_samples, sd = 15) ) # plot the data ggplot(data = df.regression, aes(x = chocolate, y = happiness)) + geom_point(size = 3) 10.3.1 Define and fit the models Define and fit the compact model (Model C): \\(Y_i = \\beta_0 + \\epsilon_i\\) # fit the compact model lm.compact = lm(happiness ~ 1, data = df.regression) # store the results of the model fit in a data frame df.compact = tidy(lm.compact) # plot the data with model prediction ggplot(data = df.regression, aes(x = chocolate, y = happiness)) + geom_hline(yintercept = df.compact$estimate, color = &quot;blue&quot;, size = 1) + geom_point(size = 3) Define and fit the augmented model (Model A): \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\epsilon_i\\) # fit the augmented model lm.augmented = lm(happiness ~ chocolate, data = df.regression) # store the results of the model fit in a data frame df.augmented = tidy(lm.augmented) # plot the data with model prediction ggplot(data = df.regression, aes(x = chocolate, y = happiness)) + geom_abline(intercept = df.augmented$estimate[1], slope = df.augmented$estimate[2], color = &quot;red&quot;, size = 1) + geom_point(size = 3) 10.3.2 Calculate the sum of squared errors of each model Illustration of the residuals for the compact model: # fit the model lm.compact = lm(happiness ~ 1, data = df.regression) # store the model information df.compact_summary = tidy(lm.compact) # create a data frame that contains the residuals df.compact_model = augment(lm.compact) %&gt;% clean_names() %&gt;% left_join(df.regression) ## Joining, by = &quot;happiness&quot; # plot model prediction with residuals ggplot(data = df.compact_model, aes(x = chocolate, y = happiness)) + geom_hline(yintercept = df.compact_summary$estimate, color = &quot;blue&quot;, size = 1) + geom_segment(aes(xend = chocolate, yend = df.compact_summary$estimate), color = &quot;blue&quot;) + geom_point(size = 3) # calculate the sum of squared errors df.compact_model %&gt;% summarize(SSE = sum(resid^2)) ## # A tibble: 1 x 1 ## SSE ## &lt;dbl&gt; ## 1 5215. Illustration of the residuals for the augmented model: # fit the model lm.augmented = lm(happiness ~ chocolate, data = df.regression) # store the model information df.augmented_summary = tidy(lm.augmented) # create a data frame that contains the residuals df.augmented_model = augment(lm.augmented) %&gt;% clean_names() %&gt;% left_join(df.regression) ## Joining, by = c(&quot;happiness&quot;, &quot;chocolate&quot;) # plot model prediction with residuals ggplot(data = df.augmented_model, aes(x = chocolate, y = happiness)) + geom_abline(intercept = df.augmented_summary$estimate[1], slope = df.augmented_summary$estimate[2], color = &quot;red&quot;, size = 1) + geom_segment(aes(xend = chocolate, yend = fitted), color = &quot;red&quot;) + geom_point(size = 3) # calculate the sum of squared errors df.augmented_model %&gt;% summarize(SSE = sum(resid^2)) ## # A tibble: 1 x 1 ## SSE ## &lt;dbl&gt; ## 1 2397. Calculate the F-test to determine whether PRE is significant. pc = 1 # number of parameters in the compact model pa = 2 # number of parameters in the augmented model n = 10 # number of observations # SSE of the compact model sse_compact = df.compact_model %&gt;% summarize(SSE = sum(resid^2)) # SSE of the augmented model sse_augmented = df.augmented_model %&gt;% summarize(SSE = sum(resid^2)) # Proportional reduction of error pre = as.numeric(1 - (sse_augmented/sse_compact)) # F-statistic f = (pre/(pa-pc))/((1-pre)/(n-pa)) # p-value p_value = 1-pf(f, df1 = pa-pc, df2 = n-pa) print(p_value) ## [1] 0.01542156 F-distribution with a red line indicating the calculated F-statistic. ggplot(data = tibble(x = c(0, 10)), mapping = aes(x = x)) + stat_function(fun = &quot;df&quot;, args = list(df1 = pa-pc, df2 = n-pa), size = 1) + geom_vline(xintercept = f, color = &quot;red&quot;, size = 1) The short version of doing what we did above :) anova(lm.compact, lm.augmented) ## Analysis of Variance Table ## ## Model 1: happiness ~ 1 ## Model 2: happiness ~ chocolate ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9 5215.0 ## 2 8 2396.9 1 2818.1 9.4055 0.01542 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.4 Credit example Let’s load the credit card data: df.credit = read_csv(&quot;data/credit.csv&quot;) %&gt;% rename(index = X1) %&gt;% clean_names() ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## Parsed with column specification: ## cols( ## X1 = col_double(), ## Income = col_double(), ## Limit = col_double(), ## Rating = col_double(), ## Cards = col_double(), ## Age = col_double(), ## Education = col_double(), ## Gender = col_character(), ## Student = col_character(), ## Married = col_character(), ## Ethnicity = col_character(), ## Balance = col_double() ## ) Here is a short description of the variables: variable description income in thousand dollars limit credit limit rating credit rating cards number of credit cards age in years education years of education gender male or female student student or not married married or not ethnicity African American, Asian, Caucasian balance average credit card debt Scatterplot of the relationship between income and balance. ggplot(data = df.credit, mapping = aes(x = income, y = balance)) + geom_point(alpha = 0.3) + coord_cartesian(xlim = c(0, max(df.credit$income))) To make the model intercept interpretable, we can center the predictor variable by subtracting the mean from each value. df.plot = df.credit %&gt;% mutate(income_centered = income - mean(income)) %&gt;% select(balance, income, income_centered) fit = lm(balance ~ 1 + income_centered, data = df.plot) ggplot(data = df.plot, mapping = aes(x = income_centered, y = balance)) + geom_vline(xintercept = 0, linetype = 2, color = &quot;black&quot;) + geom_hline(yintercept = mean(df.plot$balance), color = &quot;red&quot;) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = F) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) # coord_cartesian(xlim = c(0, max(df.plot$income_centered))) Let’s fit the model and take a look at the model summary: fit = lm(balance ~ 1 + income, data = df.credit) fit %&gt;% summary() ## ## Call: ## lm(formula = balance ~ 1 + income, data = df.credit) ## ## Residuals: ## Min 1Q Median 3Q Max ## -803.64 -348.99 -54.42 331.75 1100.25 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 246.5148 33.1993 7.425 6.9e-13 *** ## income 6.0484 0.5794 10.440 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 407.9 on 398 degrees of freedom ## Multiple R-squared: 0.215, Adjusted R-squared: 0.213 ## F-statistic: 109 on 1 and 398 DF, p-value: &lt; 2.2e-16 Here, I double check that I understand how the statistics about the residuals are calculated that the model summary gives me. fit %&gt;% augment() %&gt;% clean_names() %&gt;% summarize( min = min(resid), first_quantile = quantile(resid, 0.25), median = median(resid), third_quantile = quantile(resid, 0.75), max = max(resid), rmse = sqrt(mean(resid^2)) ) ## # A tibble: 1 x 6 ## min first_quantile median third_quantile max rmse ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -804. -349. -54.4 332. 1100. 407. Here is a plot of the residuals. Residual plots are important for checking whether any of the linear model assumptions have been violated. fit %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(aes(x = fitted, y = resid)) + geom_hline(yintercept = 0, color = &quot;blue&quot;) + geom_point(alpha = 0.3) We can use the glance() function from the broom package to print out model statistics. fit %&gt;% glance() %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.21 0.21 407.86 108.99 0 2 -2970.95 5947.89 5959.87 66208745 398 Let’s test whether income is a significant predictor of balance in the credit data set. # fitting the compact model fit_c = lm(formula = balance ~ 1, data = df.credit) # fitting the augmented model fit_a = lm(formula = balance ~ 1 + income, data = df.credit) # run the F test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: balance ~ 1 ## Model 2: balance ~ 1 + income ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 399 84339912 ## 2 398 66208745 1 18131167 108.99 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s print out the paramters of the augmented model with confidence intervals: fit_a %&gt;% tidy(conf.int = T) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) term estimate std.error statistic p.value conf.low conf.high (Intercept) 246.51 33.20 7.43 0 181.25 311.78 income 6.05 0.58 10.44 0 4.91 7.19 We can use augment() with the newdata = argument to get predictions about new data from our fitted model: augment(fit, newdata = tibble(income = 130)) ## # A tibble: 1 x 3 ## income .fitted .se.fit ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 130 1033. 53.2 Here is a plot of the model with confidence interval (that captures our uncertainty in the intercept and slope of the model) and the predicted balance value for an income of 130: ggplot(data = df.credit, mapping = aes(x = income, y = balance)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + annotate(geom = &quot;point&quot;, color = &quot;red&quot;, size = 5, x = 130, y = predict(fit, newdata = tibble(income = 130))) + coord_cartesian(xlim = c(0, max(df.credit$income))) Finally, let’s take a look at how the residuals are distributed. # get the residuals df.plot = fit_a %&gt;% augment() %&gt;% clean_names() # plot a quantile-quantile plot ggplot(df.plot, aes(sample = resid)) + geom_qq_line() + geom_qq() # and a density of the residuals ggplot(df.plot, aes(x = resid)) + stat_density(geom = &quot;line&quot;) Not quite as normally distributed as we would hope. We learn what to do if some of the assumptions of the linear model are violated later in class. 10.5 Additional resources 10.5.1 Datacamp Statistical modeling 1 Statistical modeling 2 Correlation and regression 10.5.2 Misc Spurious correlations "],
["linear-model-2.html", "Chapter 11 Linear model 2 11.1 Load packages and set plotting theme 11.2 Load data sets 11.3 Things that came up in class 11.4 Multiple continuous variables 11.5 One categorical variable 11.6 One continuous and one categorical variable 11.7 Interactions 11.8 Additional resources", " Chapter 11 Linear model 2 11.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;corrr&quot;) # for calculating correlations between many variables library(&quot;corrplot&quot;) # for plotting correlations library(&quot;GGally&quot;) # for running ggpairs() function library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 11.2 Load data sets Let’s load the data sets that we’ll explore in this class: # credit data set df.credit = read_csv(&quot;data/credit.csv&quot;) %&gt;% rename(index = X1) %&gt;% clean_names() # advertising data set df.ads = read_csv(&quot;data/advertising.csv&quot;) %&gt;% clean_names() %&gt;% rename(index = x1) variable description income in thousand dollars limit credit limit rating credit rating cards number of credit cards age in years education years of education gender male or female student student or not married married or not ethnicity African American, Asian, Caucasian balance average credit card debt 11.3 Things that came up in class Can the density at a given point be greater than 1? Yes, since it’s the area under the curve that has to sum to 1. Here is the density plot for a uniform distribution (note that the density is 1 uniformly). # play around with this value to see how the density changes tmp.max = 5 ggplot(data = tibble(x = c(0, tmp.max)), mapping = aes(x = x)) + stat_function(fun = &quot;dunif&quot;, geom = &quot;area&quot;, fill = &quot;lightblue&quot;, size = 1, args = list(min = 0, max = tmp.max)) + stat_function(fun = &quot;dunif&quot;, size = 1, args = list(min = 0, max = tmp.max)) + coord_cartesian(xlim = c(0, tmp.max), ylim = c(0, 6), expand = F) And here is the density plot for a beta distribution: # play around with these parameters tmp.shape1 = 1 tmp.shape2 = 2 ggplot(data = tibble(x = c(0, 1)), mapping = aes(x = x)) + stat_function(fun = &quot;dbeta&quot;, args = list(shape1 = tmp.shape1, shape2 = tmp.shape2), geom = &quot;area&quot;, fill = &quot;lightblue&quot;, size = 1) + stat_function(fun = &quot;dbeta&quot;, args = list(shape1 = tmp.shape1, shape2 = tmp.shape2), size = 1) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 3), expand = F) 11.4 Multiple continuous variables Let’s take a look at a case where we have multiple continuous predictor variables. In this case, we want to make sure that our predictors are not too highly correlated with each other (as this makes the interpration of how much each variable explains the outcome difficult). So we first need to explore the pairwise correlations between variables. 11.4.1 Explore correlations The corrr package is great for exploring correlations between variables. To find out more how corrr works, take a look at this vignette: vignette(topic = &quot;using-corrr&quot;, package = &quot;corrr&quot;) Here is an example that illustrates some of the key functions in the corrr package (using the advertisement data): df.ads %&gt;% select_if(is.numeric) %&gt;% correlate(quiet = T) %&gt;% shave() %&gt;% fashion() ## rowname index tv radio newspaper sales ## 1 index ## 2 tv .02 ## 3 radio -.11 .05 ## 4 newspaper -.15 .06 .35 ## 5 sales -.05 .78 .58 .23 11.4.1.1 Visualize correlations 11.4.1.1.1 Correlations with the dependent variable df.credit %&gt;% select_if(is.numeric) %&gt;% correlate(quiet = T) %&gt;% select(rowname, income) %&gt;% mutate(rowname = reorder(rowname, income)) %&gt;% drop_na() %&gt;% ggplot(aes(x = rowname, y = income, fill = income)) + geom_hline(yintercept = 0) + geom_col(color = &quot;black&quot;, show.legend = F) + scale_fill_gradient2(low = &quot;indianred2&quot;, mid = &quot;white&quot;, high = &quot;skyblue1&quot;, limits = c(-1, 1)) + coord_flip() + theme(axis.title.y = element_blank()) Figure 11.1: Bar plot illustrating how strongly different variables correlate with income. 11.4.1.1.2 All pairwise correlations tmp = df.credit %&gt;% select_if(is.numeric) %&gt;% correlate(diagonal = 0, quiet = T) %&gt;% rearrange() %&gt;% column_to_rownames() %&gt;% as.matrix() %&gt;% corrplot() df.ads %&gt;% select(-index) %&gt;% ggpairs() Figure 11.2: Pairwise correlations with scatter plots, correlation values, and densities on the diagonal. With some customization: df.ads %&gt;% select(-index) %&gt;% ggpairs(lower = list(continuous = wrap(&quot;points&quot;, alpha = 0.3)), upper = list(continuous = wrap(&quot;cor&quot;, size = 8))) + theme(panel.grid.major = element_blank()) Figure 11.3: Pairwise correlations with scatter plots, correlation values, and densities on the diagonal (customized). 11.4.2 Multipe regression Now that we’ve explored the correlations, let’s have a go at the multiple regression. 11.4.2.1 Visualization We’ll first take another look at the pairwise relationships: tmp.x = &quot;tv&quot; # tmp.x = &quot;radio&quot; # tmp.x = &quot;newspaper&quot; # tmp.y = &quot;radio&quot; tmp.y = &quot;radio&quot; # tmp.y = &quot;tv&quot; ggplot(df.ads, aes_string(x = tmp.x, y = tmp.y)) + stat_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, fullrange = T) + geom_point(alpha = 0.3) + annotate(geom = &quot;text&quot;, x = -Inf, y = Inf, hjust = -0.5, vjust = 1.5, label = str_c(&quot;r = &quot;, cor(df.ads[[tmp.x]], df.ads[[tmp.y]]) %&gt;% round(2) %&gt;% # round str_remove(&quot;^0+&quot;) # remove 0 ), size = 8) + theme(text = element_text(size = 30)) TV ads and radio ads aren’t correlated. Yay! 11.4.2.2 Fitting, hypothesis testing, evaluation Let’s see whether adding radio ads is worth it (over and above having TV ads). # fit the models fit_c = lm(sales ~ 1 + tv, data = df.ads) fit_a = lm(sales ~ 1 + tv + radio, data = df.ads) # do the F test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: sales ~ 1 + tv ## Model 2: sales ~ 1 + tv + radio ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 198 2102.53 ## 2 197 556.91 1 1545.6 546.74 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It’s worth it! Let’s evaluate how well the model actually does. We do this by taking a look at the residual plot, and check whether the residuals are normally distributed. tmp.fit = lm(sales ~ 1 + tv + radio, data = df.ads) df.plot = tmp.fit %&gt;% augment() %&gt;% clean_names() # residual plot ggplot(df.plot, aes(x = fitted, y = resid)) + geom_point() # density of residuals ggplot(df.plot, aes(x = resid)) + stat_density(geom = &quot;line&quot;) # QQ plot ggplot(df.plot, aes(sample = resid)) + geom_qq() + geom_qq_line() There is a slight non-linear trend in the residuals. We can also see that the residuals aren’t perfectly normally distributed. We’ll see later what we can do about this … Let’s see how well the model does overall: fit_a %&gt;% glance() %&gt;% kable(digits = 3) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.897 0.896 1.681 859.618 0 3 -386.197 780.394 793.587 556.914 197 As we can see, the model almost explains 90% of the variance. That’s very decent! 11.4.2.3 Visualizing the model fits Here is a way of visualizing how both tv ads and radio ads affect sales: df.plot = lm(sales ~ 1 + tv + radio, data = df.ads) %&gt;% augment() %&gt;% clean_names() df.tidy = lm(sales ~ 1 + tv + radio, data = df.ads) %&gt;% tidy() ggplot(df.plot, aes(x = radio, y = sales, color = tv)) + geom_point() + scale_color_gradient(low = &quot;gray80&quot;, high = &quot;black&quot;) + theme(legend.position = c(0.1, 0.8)) We used color here to encode TV ads (and the x-axis for the radio ads). In addition, we might want to illustrate what relationship between radio ads and sales the model predicts for three distinct values for TV ads. Like so: df.plot = lm(sales ~ 1 + tv + radio, data = df.ads) %&gt;% augment() %&gt;% clean_names() df.tidy = lm(sales ~ 1 + tv + radio, data = df.ads) %&gt;% tidy() ggplot(df.plot, aes(x = radio, y = sales, color = tv)) + geom_point() + geom_abline(intercept = df.tidy$estimate[1] + df.tidy$estimate[2] * 200, slope = df.tidy$estimate[3]) + geom_abline(intercept = df.tidy$estimate[1] + df.tidy$estimate[2] * 100, slope = df.tidy$estimate[3]) + geom_abline(intercept = df.tidy$estimate[1] + df.tidy$estimate[2] * 0, slope = df.tidy$estimate[3]) + scale_color_gradient(low = &quot;gray80&quot;, high = &quot;black&quot;) + theme(legend.position = c(0.1, 0.8)) 11.4.2.4 Interpreting the model fits Fitting the augmented model yields the following estimates for the coefficients in the model: fit_a %&gt;% tidy(conf.int = T) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) term estimate std.error statistic p.value conf.low conf.high (Intercept) 2.92 0.29 9.92 0 2.34 3.50 tv 0.05 0.00 32.91 0 0.04 0.05 radio 0.19 0.01 23.38 0 0.17 0.20 11.4.2.5 Standardizing the predictors One thing we can do to make different predictors more comparable is to standardize them. df.ads = df.ads %&gt;% mutate_at(vars(tv, radio), funs(scaled = scale(.)[,])) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. df.ads %&gt;% select(-newspaper) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) index tv radio sales tv_scaled radio_scaled 1 230.1 37.8 22.1 0.97 0.98 2 44.5 39.3 10.4 -1.19 1.08 3 17.2 45.9 9.3 -1.51 1.52 4 151.5 41.3 18.5 0.05 1.21 5 180.8 10.8 12.9 0.39 -0.84 6 8.7 48.9 7.2 -1.61 1.73 7 57.5 32.8 11.8 -1.04 0.64 8 120.2 19.6 13.2 -0.31 -0.25 9 8.6 2.1 4.8 -1.61 -1.43 10 199.8 2.6 10.6 0.61 -1.39 We can standardize (z-score) variables using the scale() function. # tmp.variable = &quot;tv&quot; tmp.variable = &quot;tv_scaled&quot; ggplot(df.ads, aes_string(x = tmp.variable)) + stat_density(geom = &quot;line&quot;, size = 1) + annotate(geom = &quot;text&quot;, x = median(df.ads[[tmp.variable]]), y = -Inf, label = str_c(&quot;sd = &quot;, sd(df.ads[[tmp.variable]]) %&gt;% round(2)), size = 10, vjust = -1, hjust = 0.5 ) + annotate(geom = &quot;text&quot;, x = median(df.ads[[tmp.variable]]), y = -Inf, label = str_c(&quot;mean = &quot;, mean(df.ads[[tmp.variable]]) %&gt;% round(2)), size = 10, vjust = -3, hjust = 0.5 ) Scaling a variable leaves the distribution intact, but changes the mean to 0 and the SD to 1. 11.5 One categorical variable Let’s compare a compact model that only predicts the mean, with a model that uses the student variable as an additional predictor. # fit the models fit_c = lm(balance ~ 1, data = df.credit) fit_a = lm(balance ~ 1 + student, data = df.credit) # run the F test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: balance ~ 1 ## Model 2: balance ~ 1 + student ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 399 84339912 ## 2 398 78681540 1 5658372 28.622 1.488e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fit_a %&gt;% summary() ## ## Call: ## lm(formula = balance ~ 1 + student, data = df.credit) ## ## Residuals: ## Min 1Q Median 3Q Max ## -876.82 -458.82 -40.87 341.88 1518.63 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 480.37 23.43 20.50 &lt; 2e-16 *** ## studentYes 396.46 74.10 5.35 1.49e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 444.6 on 398 degrees of freedom ## Multiple R-squared: 0.06709, Adjusted R-squared: 0.06475 ## F-statistic: 28.62 on 1 and 398 DF, p-value: 1.488e-07 The summary() shows that it’s worth it: the augmented model explains a signifcant amount of the variance (i.e. it significantly reduces the proportion in error PRE). 11.5.1 Visualization of the model predictions Let’s visualize the model predictions. Here is the compact model: ggplot(df.credit, aes(x = index, y = balance)) + geom_hline(yintercept = mean(df.credit$balance), size = 1) + geom_segment(aes(xend = index, yend = mean(df.credit$balance)), alpha = 0.1) + geom_point(alpha = 0.5) It just predicts the mean (the horizontal black line). The vertical lines from each data point to the mean illustrate the residuals. And here is the augmented model: df.fit = fit_a %&gt;% tidy() %&gt;% mutate(estimate = round(estimate,2)) ggplot(df.credit, aes(x = index, y = balance, color = student)) + geom_hline(yintercept = df.fit$estimate[1], size = 1, color = &quot;#E41A1C&quot;) + geom_hline(yintercept = df.fit$estimate[1] + df.fit$estimate[2], size = 1, color = &quot;#377EB8&quot;) + geom_segment(data = df.credit %&gt;% filter(student == &quot;No&quot;), aes(xend = index, yend = df.fit$estimate[1]), alpha = 0.1, color = &quot;#E41A1C&quot;) + geom_segment(data = df.credit %&gt;% filter(student == &quot;Yes&quot;), aes(xend = index, yend = df.fit$estimate[1] + df.fit$estimate[2]), alpha = 0.1, color = &quot;#377EB8&quot;) + geom_point(alpha = 0.5) + scale_color_brewer(palette = &quot;Set1&quot;) + guides(color = guide_legend(reverse = T)) Note that this model predicts two horizontal lines. One for students, and one for non-students. Let’s make simple plot that shows the means of both groups with bootstrapped confidence intervals. ggplot(data = df.credit, mapping = aes(x = student, y = balance, fill = student)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, show.legend = F) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + scale_fill_brewer(palette = &quot;Set1&quot;) And let’s double check that we also get a signifcant result when we run a t-test instead of our model comparison procedure: t.test(x = df.credit$balance[df.credit$student == &quot;No&quot;], y = df.credit$balance[df.credit$student == &quot;Yes&quot;]) ## ## Welch Two Sample t-test ## ## data: df.credit$balance[df.credit$student == &quot;No&quot;] and df.credit$balance[df.credit$student == &quot;Yes&quot;] ## t = -4.9028, df = 46.241, p-value = 1.205e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -559.2023 -233.7088 ## sample estimates: ## mean of x mean of y ## 480.3694 876.8250 11.5.2 Dummy coding When we put a variable in a linear model that is coded as a character or as a factor, R automatically recodes this variable using dummy coding. It uses level 1 as the reference category for factors, or the value that comes first in the alphabet for characters. df.credit %&gt;% select(income, student) %&gt;% mutate(student_dummy = ifelse(student == &quot;No&quot;, 0, 1))%&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) income student student_dummy 14.89 No 0 106.03 Yes 1 104.59 No 0 148.92 No 0 55.88 No 0 80.18 No 0 21.00 No 0 71.41 No 0 15.12 No 0 71.06 Yes 1 11.5.3 Reporting the results To report the results, we could show a plot like this: df.plot = df.credit ggplot(df.plot, aes(x = student, y = balance)) + geom_point(alpha = 0.1, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, size = 4) And then report the means and standard deviations together with the result of our signifance test: df.credit %&gt;% group_by(student) %&gt;% summarize(mean = mean(balance), sd = sd(balance)) %&gt;% mutate_if(is.numeric, funs(round(., 2))) ## # A tibble: 2 x 3 ## student mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 480. 439. ## 2 Yes 877. 490 11.6 One continuous and one categorical variable Now let’s take a look at a case where we have one continuous and one categorical predictor variable. Let’s first formulate and fit our models: # fit the models fit_c = lm(balance ~ 1 + income, df.credit) fit_a = lm(balance ~ 1 + income + student, df.credit) # run the F test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: balance ~ 1 + income ## Model 2: balance ~ 1 + income + student ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 398 66208745 ## 2 397 60939054 1 5269691 34.331 9.776e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see again that it’s worth it. The augmented model explains significantly more variance than the compact model. 11.6.1 Visualization of the model predictions Let’s visualize the model predictions again. Let’s start with the compact model: df.augment = fit_c %&gt;% augment() %&gt;% clean_names() ggplot(df.augment, aes(x = income, y = balance)) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;black&quot;) + geom_segment(aes(xend = income, yend = fitted), alpha = 0.3) + geom_point(alpha = 0.3) This time, the compact model still predicts just one line (like above) but note that this line is not horizontal anymore. df.tidy = fit_a %&gt;% tidy() %&gt;% mutate(estimate = round(estimate,2)) df.augment = fit_a %&gt;% augment() %&gt;% clean_names() ggplot(df.augment, aes(x = income, y = balance, group = student, color = student)) + geom_segment(data = df.augment %&gt;% filter(student == &quot;No&quot;), aes(xend = income, yend = fitted), color = &quot;#E41A1C&quot;, alpha = 0.3) + geom_segment(data = df.augment %&gt;% filter(student == &quot;Yes&quot;), aes(xend = income, yend = fitted), color = &quot;#377EB8&quot;, alpha = 0.3) + geom_abline(intercept = df.tidy$estimate[1], slope = df.tidy$estimate[2], color = &quot;#E41A1C&quot;, size = 1) + geom_abline(intercept = df.tidy$estimate[1] + df.tidy$estimate[3], slope = df.tidy$estimate[2], color = &quot;#377EB8&quot;, size = 1) + geom_point(alpha = 0.3) + scale_color_brewer(palette = &quot;Set1&quot;) + theme(legend.position = c(0.1, 0.9)) + guides(color = guide_legend(reverse = T)) The augmented model predicts two lines again, each with the same slope (but the intercept differs). 11.7 Interactions Let’s check whether there is an interaction between how income affects balance for students vs. non-students. 11.7.1 Visualization Let’s take a look at the data first. ggplot(data = df.credit, mapping = aes(x = income, y = balance, group = student, color = student)) + geom_smooth(method = &quot;lm&quot;, se = F) + geom_point(alpha = 0.3) + scale_color_brewer(palette = &quot;Set1&quot;) + theme(legend.position = c(0.1, 0.9)) + guides(color = guide_legend(reverse = T)) Note that we just specified here that we want to have a linear model (via geom_smooth(method = \"lm\")). By default, ggplot2 assumes that we want a model that includes interactions. We can see this by the fact that two fitted lines are not parallel. But is the interaction in the model worth it? That is, does a model that includes an interaction explain significantly more variance in the data, than a model that does not have an interaction. 11.7.2 Hypothesis test Let’s check: # fit models fit_c = lm(formula = balance ~ income + student, data = df.credit) fit_a = lm(formula = balance ~ income * student, data = df.credit) # F-test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: balance ~ income + student ## Model 2: balance ~ income * student ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 397 60939054 ## 2 396 60734545 1 204509 1.3334 0.2489 Nope, not worth it! The F-test comes out non-significant. 11.8 Additional resources 11.8.1 Datacamp Statistical modeling 1 Statistical modeling 2 Correlation and regression 11.8.2 Misc Nice review of multiple regression in R "],
["linear-model-3.html", "Chapter 12 Linear model 3 12.1 Load packages and set plotting theme 12.2 Load data sets 12.3 Two-way ANOVA 12.4 Additional resources", " Chapter 12 Linear model 3 12.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;car&quot;) # for running ANOVAs library(&quot;afex&quot;) # also for running ANOVAs library(&quot;emmeans&quot;) # for calculating constrasts library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 12.2 Load data sets df.poker = read_csv(&quot;data/poker.csv&quot;) %&gt;% mutate(skill = factor(skill, levels = 1:2, labels = c(&quot;expert&quot;, &quot;average&quot;)), skill = fct_relevel(skill, &quot;average&quot;, &quot;expert&quot;), hand = factor(hand, levels = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)), limit = factor(limit, levels = 1:2, labels = c(&quot;fixed&quot;, &quot;none&quot;)), participant = 1:n()) %&gt;% select(participant, everything()) Selection of the data: df.poker %&gt;% group_by(skill, hand, limit) %&gt;% filter(row_number() &lt; 3) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) participant skill hand limit balance 1 expert bad fixed 4.00 2 expert bad fixed 5.55 26 expert bad none 5.52 27 expert bad none 8.28 51 expert neutral fixed 11.74 52 expert neutral fixed 10.04 76 expert neutral none 21.55 77 expert neutral none 3.12 101 expert good fixed 10.86 102 expert good fixed 8.68 12.2.1 One-way ANOVA 12.2.1.1 Visualization df.poker %&gt;% ggplot(mapping = aes(x = hand, y = balance, fill = hand)) + geom_point(alpha = 0.2, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, size = 4) + labs(y = &quot;final balance (in Euros)&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + theme(legend.position = &quot;none&quot;) 12.2.1.2 Model fitting We pass the result of the lm() function to anova() to calculate an analysis of variance like so: lm(formula = balance ~ hand, data = df.poker) %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hand 2 2559.4 1279.7 75.703 &lt; 2.2e-16 *** ## Residuals 297 5020.6 16.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.2.1.3 Hypothesis test The F-test reported by the ANOVA compares the fitted model with a compact model that only predicts the grand mean: # fit the models fit_c = lm(formula = balance ~ 1, data = df.poker) fit_a = lm(formula = balance ~ hand, data = df.poker) # compare via F-test anova(fit_c, fit_a) ## Analysis of Variance Table ## ## Model 1: balance ~ 1 ## Model 2: balance ~ hand ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 299 7580.0 ## 2 297 5020.6 2 2559.4 75.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.2.1.4 Visualize the model’s predictions Here is the model prediction of the compact model: set.seed(1) df.plot = df.poker %&gt;% mutate(hand_jitter = 1 + runif(n(), min = -0.25, max = 0.25)) df.augment = fit_c %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(balance, hand, hand_jitter)) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = balance, fill = hand)) + geom_hline(yintercept = mean(df.poker$balance)) + geom_point(alpha = 0.5) + geom_segment(data = df.augment, aes(xend = hand_jitter, yend = fitted), alpha = 0.2) + labs(y = &quot;balance&quot;) + theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.title.x = element_blank()) Note that since we have a categorical variable here, we don’t really have a continuous x-axis. I’ve just jittered the values so it’s easier to show the residuals. And here is the prediction of the augmented model (which predicts different means for each group). set.seed(1) df.plot = df.poker %&gt;% mutate(hand_jitter = hand %&gt;% as.numeric(), hand_jitter = hand_jitter + runif(n(), min = -0.4, max = 0.4)) df.tidy = fit_a %&gt;% tidy() %&gt;% select_if(is.numeric) %&gt;% mutate_all(funs(round, .args = list(digits = 2))) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. df.augment = fit_a %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(hand_jitter)) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = balance, color = hand)) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.tidy$estimate[1], yend = df.tidy$estimate[1] ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.tidy$estimate[1] + df.tidy$estimate[2], yend = df.tidy$estimate[1] + df.tidy$estimate[2] ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.tidy$estimate[1] + df.tidy$estimate[3], yend = df.tidy$estimate[1] + df.tidy$estimate[3] ), color = &quot;green&quot;, size = 1) + geom_segment(data = df.augment, aes(xend = hand_jitter, y = balance, yend = fitted), alpha = 0.3) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) The vertical lines illustrate the residual sum of squares. We can illustrate the model sum of squares like so: set.seed(1) df.plot = df.poker %&gt;% mutate(hand_jitter = hand %&gt;% as.numeric(), hand_jitter = hand_jitter + runif(n(), min = -0.4, max = 0.4)) %&gt;% group_by(hand) %&gt;% mutate(mean_group = mean(balance)) %&gt;% ungroup() %&gt;% mutate(mean_grand = mean(balance)) df.means = df.poker %&gt;% group_by(hand) %&gt;% summarize(mean = mean(balance)) %&gt;% spread(hand, mean) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = mean_group, color = hand)) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.means$bad, yend = df.means$bad ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.means$neutral, yend = df.means$neutral ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.means$good, yend = df.means$good ), color = &quot;green&quot;, size = 1) + geom_segment(aes(xend = hand_jitter, y = mean_group, yend = mean_grand), alpha = 0.3) + geom_hline(yintercept = mean(df.poker$balance), size = 1) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)) + scale_y_continuous(breaks = c(0, 10, 20), labels = c(0, 10, 20), limits = c(0, 25)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) This captures the variance in the data that is accounted for by the hand variable. Just for kicks, let’s calculate our cherished proportion of reduction in error PRE: df.c = fit_c %&gt;% augment() %&gt;% clean_names() %&gt;% summarize(sse = sum(resid^2) %&gt;% round) df.a = fit_a %&gt;% augment() %&gt;% clean_names() %&gt;% summarize(sse = sum(resid^2) %&gt;% round) pre = 1 - df.a$sse/df.c$sse print(pre %&gt;% round(2)) ## [1] 0.34 Note that this is the same as the \\(R^2\\) for the augmented model: fit_a %&gt;% summary() ## ## Call: ## lm(formula = balance ~ hand, data = df.poker) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9264 -2.5902 -0.0115 2.6573 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.9415 0.4111 14.451 &lt; 2e-16 *** ## handneutral 4.4051 0.5815 7.576 4.55e-13 *** ## handgood 7.0849 0.5815 12.185 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.111 on 297 degrees of freedom ## Multiple R-squared: 0.3377, Adjusted R-squared: 0.3332 ## F-statistic: 75.7 on 2 and 297 DF, p-value: &lt; 2.2e-16 12.2.1.5 Dummy coding Let’s check that we understand how dummy-coding works for a variable with more than 2 levels: # dummy code the hand variable df.poker = df.poker %&gt;% mutate(hand_neutral = ifelse(hand == &quot;neutral&quot;, 1, 0), hand_good = ifelse(hand == &quot;good&quot;, 1, 0)) # show the dummy coded variables df.poker %&gt;% select(participant, contains(&quot;hand&quot;), balance) %&gt;% group_by(hand) %&gt;% top_n(3) %&gt;% head(10) %&gt;% kable(digits = 3) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) ## Selecting by balance participant hand hand_neutral hand_good balance 31 bad 0 0 12.22 46 bad 0 0 12.06 50 bad 0 0 16.68 76 neutral 1 0 21.55 87 neutral 1 0 20.89 89 neutral 1 0 25.63 127 good 0 1 26.99 129 good 0 1 21.36 283 good 0 1 22.48 # fit the model fit.tmp = lm(balance ~ 1 + hand_neutral + hand_good, df.poker) # show the model summary fit.tmp %&gt;% summary() ## ## Call: ## lm(formula = balance ~ 1 + hand_neutral + hand_good, data = df.poker) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9264 -2.5902 -0.0115 2.6573 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.9415 0.4111 14.451 &lt; 2e-16 *** ## hand_neutral 4.4051 0.5815 7.576 4.55e-13 *** ## hand_good 7.0849 0.5815 12.185 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.111 on 297 degrees of freedom ## Multiple R-squared: 0.3377, Adjusted R-squared: 0.3332 ## F-statistic: 75.7 on 2 and 297 DF, p-value: &lt; 2.2e-16 Here, I’ve directly put the dummy-coded variables as predictors into the lm(). We get the same model as if we used the hand variable instead. 12.2.1.6 Follow up questions Here are some follow up questions we may ask about the data. Are bad hands different from neutral hands? df.poker %&gt;% filter(hand %in% c(&quot;bad&quot;, &quot;neutral&quot;)) %&gt;% lm(formula = balance ~ hand, data = .) %&gt;% summary() ## ## Call: ## lm(formula = balance ~ hand, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.9566 -2.5078 -0.2365 2.4410 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.9415 0.3816 15.570 &lt; 2e-16 *** ## handneutral 4.4051 0.5397 8.163 3.76e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.816 on 198 degrees of freedom ## Multiple R-squared: 0.2518, Adjusted R-squared: 0.248 ## F-statistic: 66.63 on 1 and 198 DF, p-value: 3.758e-14 Are neutral hands different from good hands? df.poker %&gt;% filter(hand %in% c(&quot;neutral&quot;, &quot;good&quot;)) %&gt;% lm(formula = balance ~ hand, data = .) %&gt;% summary() ## ## Call: ## lm(formula = balance ~ hand, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9264 -2.7141 0.2585 2.7184 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.3466 0.4448 23.26 &lt; 2e-16 *** ## handgood 2.6798 0.6291 4.26 3.16e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.448 on 198 degrees of freedom ## Multiple R-squared: 0.08396, Adjusted R-squared: 0.07933 ## F-statistic: 18.15 on 1 and 198 DF, p-value: 3.158e-05 Doing the same thing by recoding our hand factor and taking “neutral” to be the reference category: df.poker %&gt;% mutate(hand = fct_relevel(hand, &quot;neutral&quot;)) %&gt;% lm(formula = balance ~ hand, data = .) %&gt;% summary() ## ## Call: ## lm(formula = balance ~ hand, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9264 -2.5902 -0.0115 2.6573 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.3466 0.4111 25.165 &lt; 2e-16 *** ## handbad -4.4051 0.5815 -7.576 4.55e-13 *** ## handgood 2.6798 0.5815 4.609 6.02e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.111 on 297 degrees of freedom ## Multiple R-squared: 0.3377, Adjusted R-squared: 0.3332 ## F-statistic: 75.7 on 2 and 297 DF, p-value: &lt; 2.2e-16 12.3 Two-way ANOVA Now let’s take a look at a case where we have multiple categorical predictors. 12.3.1 Visualization Let’s look at the overall effect of skill: ggplot(data = df.poker, mapping = aes(x = skill, y = balance)) + geom_point(position = position_jitter(width = 0.2, height = 0), alpha = 0.2) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, color = &quot;black&quot;, position = position_dodge(0.9)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;black&quot;, position = position_dodge(0.9), aes(shape = skill), size = 3, fill = &quot;black&quot;) + scale_shape_manual(values = c(21, 22)) + guides(shape = F) And now let’s take a look at the means for the full the 3 (hand) x 2 (skill) design: ggplot(data = df.poker, mapping = aes(x = hand, y = balance, group = skill, fill = hand)) + geom_point(position = position_jitterdodge(jitter.width = 0.3, jitter.height = 0, dodge.width = 0.9), alpha = 0.2) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, color = &quot;black&quot;, position = position_dodge(0.9)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, aes(shape = skill), color = &quot;black&quot;, position = position_dodge(0.9), size = 3) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_shape_manual(values = c(21, 22)) + guides(fill = F) 12.3.2 Model fitting For N-way ANOVAs, we need to be careful about what sums of squares we are using. The standard (based on the SPSS output) is to use type III sums of squares. We set this up in the following way: lm(formula = balance ~ hand * skill, data = df.poker, contrasts = list(hand = &quot;contr.sum&quot;, skill = &quot;contr.sum&quot;)) %&gt;% Anova(type = 3) ## Anova Table (Type III tests) ## ## Response: balance ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 28644.7 1 1772.1137 &lt; 2.2e-16 *** ## hand 2559.4 2 79.1692 &lt; 2.2e-16 *** ## skill 39.3 1 2.4344 0.1197776 ## hand:skill 229.0 2 7.0830 0.0009901 *** ## Residuals 4752.3 294 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So, we fit our linear model, but set the contrasts to “contr.sum” (which yields effect coding instead of dummy coding), and then specify the desired type of sums of squares in the Anova() function call. Alternatively, we could use the afex package and specify the ANOVA like so: aov_ez(id = &quot;participant&quot;, dv = &quot;balance&quot;, data = df.poker, between = c(&quot;hand&quot;, &quot;skill&quot;) ) ## Contrasts set to contr.sum for the following variables: hand, skill ## Anova Table (Type 3 tests) ## ## Response: balance ## Effect df MSE F ges p.value ## 1 hand 2, 294 16.16 79.17 *** .35 &lt;.0001 ## 2 skill 1, 294 16.16 2.43 .008 .12 ## 3 hand:skill 2, 294 16.16 7.08 *** .05 .0010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The afex package uses effect coding and type 3 sums of squares by default. 12.3.3 Interpreting interactions Code I’ve used to generate the different plots in the competition: set.seed(1) b0 = 15 nsamples = 30 sd = 5 # simple effect of condition b1 = 10 b2 = 1 b1_2 = 1 # two simple effects # b1 = 5 # b2 = -5 # b1_2 = 0 # interaction effect # b1 = 10 # b2 = 10 # b1_2 = -20 # interaction and simple effect # b1 = 10 # b2 = 0 # b1_2 = -20 # all three # b1 = 2 # b2 = 2 # b1_2 = 10 df.data = tibble( condition = rep(c(0, 1), each = nsamples), treatment = rep(c(0, 1), nsamples), rating = b0 + b1 * condition + b2 * treatment + (b1_2 * condition * treatment) + rnorm(nsamples, sd = sd)) %&gt;% mutate(condition = factor(condition, labels = c(&quot;A&quot;, &quot;B&quot;)), treatment = factor(treatment, labels = c(&quot;1&quot;, &quot;2&quot;))) ggplot(df.data, aes(x = condition, y = rating, group = treatment, fill = treatment)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, position = position_dodge(0.9)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1, position = position_dodge(0.9)) + scale_fill_brewer(palette = &quot;Set1&quot;) And here is one specific example. Let’s generate the data first: # make example reproducible set.seed(1) # set parameters nsamples = 30 b0 = 15 b1 = 10 # simple effect of condition b2 = 0 # simple effect of treatment b1_2 = -20 # interaction effect sd = 5 # generate data df.data = tibble( condition = rep(c(0, 1), each = nsamples), treatment = rep(c(0, 1), nsamples), rating = b0 + b1 * condition + b2 * treatment + (b1_2 * condition * treatment) + rnorm(nsamples, sd = sd)) %&gt;% mutate(condition = factor(condition, labels = c(&quot;A&quot;, &quot;B&quot;)), treatment = factor(treatment, labels = c(&quot;1&quot;, &quot;2&quot;))) Show part of the generated data frame: # show data frame df.data %&gt;% group_by(condition, treatment) %&gt;% filter(row_number() &lt; 3) %&gt;% ungroup() %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) condition treatment rating A 1 11.87 A 2 15.92 A 1 10.82 A 2 22.98 B 1 21.87 B 2 5.92 B 1 20.82 B 2 12.98 Plot the data: # plot data ggplot(df.data, aes(x = condition, y = rating, group = treatment, fill = treatment)) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;bar&quot;, color = &quot;black&quot;, position = position_dodge(0.9)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1, position = position_dodge(0.9)) + scale_fill_brewer(palette = &quot;Set1&quot;) And check whether we can successfully infer the parameters that we used to generate the data: # infer parameters lm(formula = rating ~ 1 + condition + treatment + condition:treatment, data = df.data) %&gt;% summary() ## ## Call: ## lm(formula = rating ~ 1 + condition + treatment + condition:treatment, ## data = df.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.6546 -3.6343 0.7988 3.3514 8.3953 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.244 1.194 13.608 &lt; 2e-16 *** ## conditionB 10.000 1.688 5.924 2.02e-07 *** ## treatment2 -1.662 1.688 -0.985 0.329 ## conditionB:treatment2 -20.000 2.387 -8.378 1.86e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.623 on 56 degrees of freedom ## Multiple R-squared: 0.7473, Adjusted R-squared: 0.7338 ## F-statistic: 55.21 on 3 and 56 DF, p-value: &lt; 2.2e-16 12.4 Additional resources 12.4.1 Datacamp Statistical modeling 1 Statistical modeling 2 Correlation and regression 12.4.2 Misc Explanation of different types of sums of squares "],
["linear-model-4.html", "Chapter 13 Linear model 4 13.1 Load packages and set plotting theme 13.2 Load data sets 13.3 Variance decomposition in one-way ANOVA 13.4 Two-way ANOVA (linear model) 13.5 ANOVA with unbalanced design 13.6 Two-way ANOVA (with interaction) 13.7 Planned contrasts", " Chapter 13 Linear model 4 13.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;afex&quot;) # for running ANOVAs library(&quot;emmeans&quot;) # for calculating constrasts library(&quot;car&quot;) # for calculating ANOVAs library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 13.2 Load data sets Read in the data: df.poker = read_csv(&quot;data/poker.csv&quot;) %&gt;% mutate(skill = factor(skill, levels = 1:2, labels = c(&quot;expert&quot;, &quot;average&quot;)), skill = fct_relevel(skill, &quot;average&quot;, &quot;expert&quot;), hand = factor(hand, levels = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)), limit = factor(limit, levels = 1:2, labels = c(&quot;fixed&quot;, &quot;none&quot;)), participant = 1:n()) %&gt;% select(participant, everything()) ## Parsed with column specification: ## cols( ## skill = col_double(), ## hand = col_double(), ## limit = col_double(), ## balance = col_double() ## ) df.poker.unbalanced = df.poker %&gt;% filter(!participant %in% 1:10) 13.3 Variance decomposition in one-way ANOVA Let’s first run the model fit = lm(formula = balance ~ hand, data = df.poker) fit %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hand 2 2559.4 1279.7 75.703 &lt; 2.2e-16 *** ## Residuals 297 5020.6 16.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.3.1 Calculate sums of squares And then let’s make sure that we understand how the variance is broken down: df.poker %&gt;% mutate(mean_grand = mean(balance)) %&gt;% group_by(hand) %&gt;% mutate(mean_group = mean(balance)) %&gt;% ungroup() %&gt;% summarize(variance_total = sum((balance - mean_grand)^2), variance_model = sum((mean_group - mean_grand)^2), variance_residual = variance_total - variance_model) ## # A tibble: 1 x 3 ## variance_total variance_model variance_residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7580. 2559. 5021. 13.3.2 Visualize model predictions 13.3.2.1 Total variance set.seed(1) fit_c = lm(formula = balance ~ 1, data = df.poker) df.plot = df.poker %&gt;% mutate(hand_jitter = 1 + runif(n(), min = -0.25, max = 0.25)) df.augment = fit_c %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(balance, hand, hand_jitter)) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = balance, fill = hand)) + geom_hline(yintercept = mean(df.poker$balance)) + geom_point(alpha = 0.5) + geom_segment(data = df.augment, aes(xend = hand_jitter, yend = fitted), alpha = 0.2) + labs(y = &quot;balance&quot;) + theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.title.x = element_blank()) 13.3.2.2 Model variance set.seed(1) df.plot = df.poker %&gt;% mutate(hand_jitter = hand %&gt;% as.numeric(), hand_jitter = hand_jitter + runif(n(), min = -0.4, max = 0.4)) %&gt;% group_by(hand) %&gt;% mutate(mean_group = mean(balance)) %&gt;% ungroup() %&gt;% mutate(mean_grand = mean(balance)) df.means = df.poker %&gt;% group_by(hand) %&gt;% summarize(mean = mean(balance)) %&gt;% spread(hand, mean) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = mean_group, color = hand)) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.means$bad, yend = df.means$bad ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.means$neutral, yend = df.means$neutral ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.means$good, yend = df.means$good ), color = &quot;green&quot;, size = 1) + geom_segment(aes(xend = hand_jitter, y = mean_group, yend = mean_grand), alpha = 0.3) + geom_hline(yintercept = mean(df.poker$balance), size = 1) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)) + scale_y_continuous(breaks = c(0, 10, 20), labels = c(0, 10, 20), limits = c(0, 25)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) 13.3.2.3 Residual variance set.seed(1) fit_a = lm(formula = balance ~ hand, data = df.poker) df.plot = df.poker %&gt;% mutate(hand_jitter = hand %&gt;% as.numeric(), hand_jitter = hand_jitter + runif(n(), min = -0.4, max = 0.4)) df.tidy = fit_a %&gt;% tidy() %&gt;% select_if(is.numeric) %&gt;% mutate_all(funs(round, .args = list(digits = 2))) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. df.augment = fit_a %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(hand_jitter)) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = balance, color = hand)) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.tidy$estimate[1], yend = df.tidy$estimate[1] ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.tidy$estimate[1] + df.tidy$estimate[2], yend = df.tidy$estimate[1] + df.tidy$estimate[2] ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.tidy$estimate[1] + df.tidy$estimate[3], yend = df.tidy$estimate[1] + df.tidy$estimate[3] ), color = &quot;green&quot;, size = 1) + geom_segment(data = df.augment, aes(xend = hand_jitter, y = balance, yend = fitted), alpha = 0.3) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) 13.4 Two-way ANOVA (linear model) Let’s fit the model first: fit = lm(formula = balance ~ hand + skill, data = df.poker) fit %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hand 2 2559.4 1279.70 76.0437 &lt;2e-16 *** ## skill 1 39.3 39.35 2.3383 0.1273 ## Residuals 296 4981.2 16.83 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.4.1 Calculate sums of squares df.poker %&gt;% mutate(mean_grand = mean(balance)) %&gt;% group_by(skill) %&gt;% mutate(mean_skill = mean(balance)) %&gt;% group_by(hand) %&gt;% mutate(mean_hand = mean(balance)) %&gt;% ungroup() %&gt;% summarize(variance_total = sum((balance - mean_grand)^2), variance_skill = sum((mean_skill - mean_grand)^2), variance_hand = sum((mean_hand - mean_grand)^2), variance_residual = variance_total - variance_skill - variance_hand) ## # A tibble: 1 x 4 ## variance_total variance_skill variance_hand variance_residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7580. 39.3 2559. 4981. 13.4.2 Visualize model predictions 13.4.2.1 Skill factor set.seed(1) df.plot = df.poker %&gt;% mutate(skill_jitter = skill %&gt;% as.numeric(), skill_jitter = skill_jitter + runif(n(), min = -0.4, max = 0.4)) %&gt;% group_by(skill) %&gt;% mutate(mean_group = mean(balance)) %&gt;% ungroup() %&gt;% mutate(mean_grand = mean(balance)) df.means = df.poker %&gt;% group_by(skill) %&gt;% summarize(mean = mean(balance)) %&gt;% spread(skill, mean) ggplot(data = df.plot, mapping = aes(x = skill_jitter, y = mean_group, color = skill)) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.means$average, yend = df.means$average ), color = &quot;black&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.means$expert, yend = df.means$expert ), color = &quot;gray50&quot;, size = 1) + geom_segment(aes(xend = skill_jitter, y = mean_group, yend = mean_grand), alpha = 0.3) + geom_hline(yintercept = mean(df.poker$balance), size = 1) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;black&quot;, &quot;gray50&quot;)) + scale_x_continuous(breaks = 1:2, labels = c(&quot;average&quot;, &quot;expert&quot;)) + scale_y_continuous(breaks = c(0, 10, 20), labels = c(0, 10, 20), limits = c(0, 25)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) 13.5 ANOVA with unbalanced design For the standard anova() function, the order of the independent predictors matters when the design is unbalanced. # one order lm(formula = balance ~ skill + hand, data = df.poker.unbalanced) %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## skill 1 74.3 74.28 4.2904 0.03922 * ## hand 2 2385.1 1192.57 68.8827 &lt; 2e-16 *** ## Residuals 286 4951.5 17.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # another order lm(formula = balance ~ hand + skill, data = df.poker.unbalanced) %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hand 2 2419.8 1209.92 69.8845 &lt;2e-16 *** ## skill 1 39.6 39.59 2.2867 0.1316 ## Residuals 286 4951.5 17.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For unbalanced designs, we should compute an ANOVA with type 3 sums of squares. # one order lm(formula = balance ~ hand + skill, data = df.poker, contrasts = list(hand = &quot;contr.sum&quot;, skill = &quot;contr.sum&quot;)) %&gt;% Anova(type = &quot;3&quot;) ## Anova Table (Type III tests) ## ## Response: balance ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 28644.7 1 1702.1527 &lt;2e-16 *** ## hand 2559.4 2 76.0437 &lt;2e-16 *** ## skill 39.3 1 2.3383 0.1273 ## Residuals 4981.2 296 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # another order lm(formula = balance ~ skill + hand, data = df.poker, contrasts = list(hand = &quot;contr.sum&quot;, skill = &quot;contr.sum&quot;)) %&gt;% Anova(type = &quot;3&quot;) ## Anova Table (Type III tests) ## ## Response: balance ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 28644.7 1 1702.1527 &lt;2e-16 *** ## skill 39.3 1 2.3383 0.1273 ## hand 2559.4 2 76.0437 &lt;2e-16 *** ## Residuals 4981.2 296 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now, the order of the independent variables doesn’t matter anymore. We can also use the aov_ez() function from the afex package. fit = aov_ez(id = &quot;participant&quot;, dv = &quot;balance&quot;, data = df.poker, between = c(&quot;hand&quot;, &quot;skill&quot;)) ## Contrasts set to contr.sum for the following variables: hand, skill fit$Anova ## Anova Table (Type III tests) ## ## Response: dv ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 28644.7 1 1772.1137 &lt; 2.2e-16 *** ## hand 2559.4 2 79.1692 &lt; 2.2e-16 *** ## skill 39.3 1 2.4344 0.1197776 ## hand:skill 229.0 2 7.0830 0.0009901 *** ## Residuals 4752.3 294 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.6 Two-way ANOVA (with interaction) Let’s firt a two-way ANOVA with the interaction term. fit = lm(formula = balance ~ hand * skill, data = df.poker) fit %&gt;% anova() ## Analysis of Variance Table ## ## Response: balance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hand 2 2559.4 1279.70 79.1692 &lt; 2.2e-16 *** ## skill 1 39.3 39.35 2.4344 0.1197776 ## hand:skill 2 229.0 114.49 7.0830 0.0009901 *** ## Residuals 294 4752.3 16.16 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And let’s compute how the the sums of squares are decomposed: df.poker %&gt;% mutate(mean_grand = mean(balance)) %&gt;% group_by(skill) %&gt;% mutate(mean_skill = mean(balance)) %&gt;% group_by(hand) %&gt;% mutate(mean_hand = mean(balance)) %&gt;% group_by(hand, skill) %&gt;% mutate(mean_hand_skill = mean(balance)) %&gt;% ungroup() %&gt;% summarize(variance_total = sum((balance - mean_grand)^2), variance_skill = sum((mean_skill - mean_grand)^2), variance_hand = sum((mean_hand - mean_grand)^2), variance_hand_skill = sum((mean_hand_skill - mean_skill - mean_hand + mean_grand)^2), variance_residual = variance_total - variance_skill - variance_hand - variance_hand_skill ) ## # A tibble: 1 x 5 ## variance_total variance_skill variance_hand variance_hand_s… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7580. 39.3 2559. 229. ## # … with 1 more variable: variance_residual &lt;dbl&gt; 13.7 Planned contrasts Here is a planned contrast that assumes that there is a linear relationship between the quality of one’s hand, and the final balance. df.poker = df.poker %&gt;% mutate(hand_contrast = factor(hand, levels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;), labels = c(-1, 0, 1)), hand_contrast = hand_contrast %&gt;% as.character() %&gt;% as.numeric()) fit.contrast = lm(formula = balance ~ hand_contrast, data = df.poker) fit.contrast %&gt;% summary() ## ## Call: ## lm(formula = balance ~ hand_contrast, data = df.poker) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.214 -2.684 -0.019 2.444 15.858 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.7715 0.2381 41.03 &lt;2e-16 *** ## hand_contrast 3.5424 0.2917 12.14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.125 on 298 degrees of freedom ## Multiple R-squared: 0.3311, Adjusted R-squared: 0.3289 ## F-statistic: 147.5 on 1 and 298 DF, p-value: &lt; 2.2e-16 Here is a visualization of the model prediction together with the residuals. df.plot = df.poker %&gt;% mutate(hand_jitter = hand %&gt;% as.numeric(), hand_jitter = hand_jitter + runif(n(), min = -0.4, max = 0.4)) df.tidy = fit.contrast %&gt;% tidy() %&gt;% select_if(is.numeric) %&gt;% mutate_all(funs(round, .args = list(digits = 2))) df.augment = fit.contrast %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(hand_jitter)) ggplot(data = df.plot, mapping = aes(x = hand_jitter, y = balance, color = as.factor(hand_contrast))) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.tidy$estimate[1]-df.tidy$estimate[2], yend = df.tidy$estimate[1]-df.tidy$estimate[2] ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.tidy$estimate[1], yend = df.tidy$estimate[1] ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.tidy$estimate[1] + df.tidy$estimate[2], yend = df.tidy$estimate[1] + df.tidy$estimate[2] ), color = &quot;green&quot;, size = 1) + geom_segment(data = df.augment, aes(xend = hand_jitter, y = balance, yend = fitted), alpha = 0.3) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) 13.7.1 Hypothetical data Here is some code to generate a hypothetical developmental data set. # make example reproducible set.seed(1) # means = c(5, 10, 5) means = c(3, 5, 20) # means = c(3, 5, 7) # means = c(3, 7, 12) sd = 2 sample_size = 20 # generate data df.contrast = tibble( group = rep(c(&quot;3-4&quot;, &quot;5-6&quot;, &quot;7-8&quot;), each = sample_size), performance = NA) %&gt;% mutate(performance = ifelse(group == &quot;3-4&quot;, rnorm(sample_size, mean = means[1], sd = sd), performance), performance = ifelse(group == &quot;5-6&quot;, rnorm(sample_size, mean = means[2], sd = sd), performance), performance = ifelse(group == &quot;7-8&quot;, rnorm(sample_size, mean = means[3], sd = sd), performance), group = factor(group, levels = c(&quot;3-4&quot;, &quot;5-6&quot;, &quot;7-8&quot;)), group_contrast = group %&gt;% fct_recode(`-1` = &quot;3-4&quot;, `0` = &quot;5-6&quot;, `1` = &quot;7-8&quot;) %&gt;% as.character() %&gt;% as.numeric()) Let’s define a linear contrast, and test whether it’s significant. fit = lm(formula = performance ~ group, data = df.contrast) # define the contrasts of interest contrasts = list(linear = c(-1, 0, 1)) # compute estimated marginal means leastsquare = emmeans(fit, &quot;group&quot;) # run follow-up analyses contrast(leastsquare, contrasts, adjust = &quot;bonferroni&quot;) ## contrast estimate SE df t.ratio p.value ## linear 16.9 0.548 57 30.856 &lt;.0001 13.7.2 Visualization Total variance: set.seed(1) fit_c = lm(formula = performance ~ 1, data = df.contrast) df.plot = df.contrast %&gt;% mutate(group_jitter = 1 + runif(n(), min = -0.25, max = 0.25)) df.augment = fit_c %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(performance, group, group_jitter)) ggplot(data = df.plot, mapping = aes(x = group_jitter, y = performance, fill = group)) + geom_hline(yintercept = mean(df.contrast$performance)) + geom_point(alpha = 0.5) + geom_segment(data = df.augment, aes(xend = group_jitter, yend = fitted), alpha = 0.2) + labs(y = &quot;performance&quot;) + theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.title.x = element_blank()) With contrast # make example reproducible set.seed(1) fit = lm(formula = performance ~ group_contrast, data = df.contrast) df.plot = df.contrast %&gt;% mutate(group_jitter = group %&gt;% as.numeric(), group_jitter = group_jitter + runif(n(), min = -0.4, max = 0.4)) df.tidy = fit %&gt;% tidy() %&gt;% select_if(is.numeric) %&gt;% mutate_all(funs(round, .args = list(digits = 2))) df.augment = fit %&gt;% augment() %&gt;% clean_names() %&gt;% bind_cols(df.plot %&gt;% select(group_jitter, group_contrast)) ggplot(data = df.plot, mapping = aes(x = group_jitter, y = performance, color = as.factor(group_contrast))) + geom_point(alpha = 0.8) + geom_segment(data = NULL, aes(x = 0.6, xend = 1.4, y = df.tidy$estimate[1]-df.tidy$estimate[2], yend = df.tidy$estimate[1]-df.tidy$estimate[2] ), color = &quot;red&quot;, size = 1) + geom_segment(data = NULL, aes(x = 1.6, xend = 2.4, y = df.tidy$estimate[1], yend = df.tidy$estimate[1] ), color = &quot;orange&quot;, size = 1) + geom_segment(data = NULL, aes(x = 2.6, xend = 3.4, y = df.tidy$estimate[1] + df.tidy$estimate[2], yend = df.tidy$estimate[1] + df.tidy$estimate[2] ), color = &quot;green&quot;, size = 1) + geom_segment(data = df.augment, aes(xend = group_jitter, y = performance, yend = fitted), alpha = 0.3) + labs(y = &quot;balance&quot;) + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + scale_x_continuous(breaks = 1:3, labels = levels(df.contrast$group)) + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) Results figure df.contrast %&gt;% ggplot(aes(x = group, y = performance)) + geom_point(alpha = 0.3, position = position_jitter(width = 0.1, height = 0)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, fill = &quot;white&quot;, size = 3) 13.7.3 Constrasts Estimated marginal means: df.development = df.contrast fit = lm(formula = performance ~ group, data = df.development) # check factor levels levels(df.development$group) ## [1] &quot;3-4&quot; &quot;5-6&quot; &quot;7-8&quot; # define the contrasts of interest contrasts = list(young_vs_old = c(-0.5, -0.5, 1), three_vs_five = c(-1, 1, 0)) # compute estimated marginal means leastsquare = emmeans(fit, &quot;group&quot;) # run analyses contrast(leastsquare, contrasts, adjust = &quot;bonferroni&quot;) ## contrast estimate SE df t.ratio p.value ## young_vs_old 16.09 0.474 57 33.936 &lt;.0001 ## three_vs_five 1.61 0.548 57 2.933 0.0097 ## ## P value adjustment: bonferroni method for 2 tests 13.7.4 Post-hoc tests Post-hoc tests for a single predictor: fit = lm(formula = performance ~ group, data = df.development) # post hoc tests leastsquare = emmeans(fit, &quot;group&quot;) pairs(leastsquare, adjust = &quot;bonferroni&quot;) ## contrast estimate SE df t.ratio p.value ## 3-4 - 5-6 -1.61 0.548 57 -2.933 0.0145 ## 3-4 - 7-8 -16.90 0.548 57 -30.856 &lt;.0001 ## 5-6 - 7-8 -15.29 0.548 57 -27.923 &lt;.0001 ## ## P value adjustment: bonferroni method for 3 tests Post-hoc tests for two predictors: # fit the model fit = lm(formula = balance ~ hand + skill, data = df.poker) # post hoc tests leastsquare = emmeans(fit, c(&quot;hand&quot;, &quot;skill&quot;)) pairs(leastsquare, adjust = &quot;bonferroni&quot;) ## contrast estimate SE df t.ratio p.value ## bad,average - neutral,average -4.405 0.580 296 -7.593 &lt;.0001 ## bad,average - good,average -7.085 0.580 296 -12.212 &lt;.0001 ## bad,average - bad,expert -0.724 0.474 296 -1.529 1.0000 ## bad,average - neutral,expert -5.129 0.749 296 -6.849 &lt;.0001 ## bad,average - good,expert -7.809 0.749 296 -10.427 &lt;.0001 ## neutral,average - good,average -2.680 0.580 296 -4.619 0.0001 ## neutral,average - bad,expert 3.681 0.749 296 4.914 &lt;.0001 ## neutral,average - neutral,expert -0.724 0.474 296 -1.529 1.0000 ## neutral,average - good,expert -3.404 0.749 296 -4.545 0.0001 ## good,average - bad,expert 6.361 0.749 296 8.492 &lt;.0001 ## good,average - neutral,expert 1.955 0.749 296 2.611 0.1424 ## good,average - good,expert -0.724 0.474 296 -1.529 1.0000 ## bad,expert - neutral,expert -4.405 0.580 296 -7.593 &lt;.0001 ## bad,expert - good,expert -7.085 0.580 296 -12.212 &lt;.0001 ## neutral,expert - good,expert -2.680 0.580 296 -4.619 0.0001 ## ## P value adjustment: bonferroni method for 15 tests "],
["power-analysis.html", "Chapter 14 Power analysis 14.1 Load packages and set plotting theme 14.2 Load data sets 14.3 Decision-making 14.4 Effect sizes 14.5 Determining sample size 14.6 Additional resources", " Chapter 14 Power analysis 14.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up model fits library(&quot;magrittr&quot;) # for going all in with the pipe library(&quot;lsr&quot;) # for computing effect size measures library(&quot;pwr&quot;) # for power calculations library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 14.2 Load data sets df.poker = read_csv(&quot;data/poker.csv&quot;) ## Parsed with column specification: ## cols( ## skill = col_double(), ## hand = col_double(), ## limit = col_double(), ## balance = col_double() ## ) df.credit = read_csv(&quot;data/credit.csv&quot;) %&gt;% rename(index = X1) %&gt;% clean_names() ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## Parsed with column specification: ## cols( ## X1 = col_double(), ## Income = col_double(), ## Limit = col_double(), ## Rating = col_double(), ## Cards = col_double(), ## Age = col_double(), ## Education = col_double(), ## Gender = col_character(), ## Student = col_character(), ## Married = col_character(), ## Ethnicity = col_character(), ## Balance = col_double() ## ) 14.3 Decision-making Figures to illustrate power: mu0 = 10 mu1 = 18 # mu0 = 8 # mu1 = 20 # sd0 = 3 # sd1 = 3 sd0 = 2 sd1 = 2 alpha = 0.05 # alpha = 0.01 ggplot(data = tibble(x = c(0, 30)), mapping = aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1, color = &quot;blue&quot;, args = list(mean = mu0, sd = sd0)) + stat_function(fun = &quot;dnorm&quot;, size = 1, color = &quot;red&quot;, args = list(mean = mu1, sd = sd1)) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, size = 1, fill = &quot;blue&quot;, alpha = 0.5, args = list(mean = mu0, sd = sd0), xlim = c(qnorm(1-alpha, mean = mu0, sd = sd0), 20) ) + stat_function(fun = &quot;dnorm&quot;, geom = &quot;area&quot;, size = 1, fill = &quot;red&quot;, alpha = 0.5, args = list(mean = mu1, sd = sd1), xlim = c(0, c(qnorm(1-alpha, mean = mu0, sd = sd0))) ) + geom_vline(xintercept = qnorm(1-alpha, mean = mu0, sd = sd0), size = 1) + coord_cartesian(expand = F) 14.4 Effect sizes 14.4.1 eta-squared and partial eta-squared One-way ANOVA: fit = lm(formula = balance ~ hand, data = df.poker) # use function etaSquared(fit) ## eta.sq eta.sq.part ## hand 0.3311076 0.3311076 # compute by hand fit %&gt;% anova %&gt;% tidy() %&gt;% pull(sumsq) %&gt;% divide_by(sum(.)) %&gt;% magrittr::extract(1) ## [1] 0.3311076 Two-way ANOVA: fit = lm(formula = balance ~ hand * skill, data = df.poker) # use function etaSquared(fit) ## eta.sq eta.sq.part ## hand 0.331107585 0.343119717 ## skill 0.005191225 0.008123029 ## hand:skill 0.029817351 0.044925866 # compute by hand eta_squared = fit %&gt;% anova %&gt;% tidy() %&gt;% pull(sumsq) %&gt;% divide_by(sum(.)) %&gt;% magrittr::extract(1) # compute partial eta squared by hand eta_partial_squared = fit %&gt;% anova %&gt;% tidy() %&gt;% filter(term %in% c(&quot;hand&quot;,&quot;Residuals&quot;)) %&gt;% select(term, sumsq) %&gt;% spread(term, sumsq) %&gt;% summarize(eta_partial_squared = hand / (hand + Residuals)) %&gt;% pull(eta_partial_squared) 14.4.2 Cohen’s d Cohen’s \\(d\\) is defined as: \\[ d = \\frac{\\overline y_1 - \\overline y_2}{s_p} \\] where \\[ s_p = \\sqrt\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2} \\] # use function from &quot;lsr&quot; package cohensD(x = balance ~ student, data = df.credit) ## [1] 0.8916607 # compute by hand df.cohen = df.credit %&gt;% group_by(student) %&gt;% summarize(mean = mean(balance), var = var(balance), n = n()) %&gt;% ungroup() n1 = df.cohen$n[1] n2 = df.cohen$n[2] var1 = df.cohen$var[1] var2 = df.cohen$var[2] mean1 = df.cohen$mean[1] mean2 = df.cohen$mean[2] sp = sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)) d = abs(mean1 - mean2) / sp print(d) ## [1] 0.8916607 14.5 Determining sample size 14.5.1 pwr package pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50), sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5235988 ## n = 22.55126 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50), sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) %&gt;% plot() + theme(title = element_text(size = 16)) 14.5.2 Simulation # make reproducible set.seed(1) # number of simulations n_simulations = 100 # run simulation df.power = crossing(n = seq(10, 50, 1), simulation = 1:n_simulations, p = c(0.75, 0.8, 0.85)) %&gt;% mutate(index = 1:n()) %&gt;% # add an index column group_by(index, n, simulation) %&gt;% mutate(response = rbinom(n = n(), size = n, prob = p)) %&gt;% # generate random data group_by(index, simulation, p) %&gt;% nest() %&gt;% # put data in list column mutate(fit = map(data, ~ binom.test(x = .$response, # define formula n = .$n, p = 0.5, alternative = &quot;two.sided&quot;)), p.value = map_dbl(fit, ~ .$p.value)) %&gt;% # run binomial test and extract p-value unnest(data) %&gt;% select(-fit) # data frame for plot df.plot = df.power %&gt;% group_by(n, p) %&gt;% summarize(power = sum(p.value &lt; 0.05) / n()) %&gt;% ungroup() %&gt;% mutate(p = as.factor(p)) # plot data ggplot(data = df.plot, mapping = aes(x = n, y = power, color = p, group = p)) + geom_smooth(method = &quot;loess&quot;) # based on simulations df.plot %&gt;% filter(p == 0.75, near(power, 0.8, tol = 0.02)) ## # A tibble: 0 x 3 ## # … with 3 variables: n &lt;dbl&gt;, p &lt;fct&gt;, power &lt;dbl&gt; # analytic solution pwr.p.test(h = ES.h(0.5, 0.75), power = 0.8, alternative = &quot;two.sided&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5235988 ## n = 28.62923 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided 14.6 Additional resources Getting started with pwr Visualize power Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs "],
["bootstrapping.html", "Chapter 15 Bootstrapping 15.1 Load packages and set plotting theme 15.2 What’s wrong with parametric tests? 15.3 Bootstrap resampling 15.4 Applications", " Chapter 15 Bootstrapping This chapter was written by Andrew Lampinen. 15.1 Load packages and set plotting theme library(&quot;boot&quot;) # for bootstrapping library(&quot;patchwork&quot;) # for making figure panels library(&quot;tidyverse&quot;) # for data wrangling etc. theme_set( theme_classic() #set the theme ) 15.2 What’s wrong with parametric tests? 15.2.1 T-tests on non-normal distributions Let’s see some examples! One common non-normal distribution is the log-normal distribution, i.e. a distribution that is normal after you take its logarithm. Many natural processes have distributions like this. One of particular interest to us is reaction times. num_points = 1e4 parametric_plotting_data = tibble( distribution = rep(c(&quot;Normal&quot;, &quot;Log-normal&quot;), each = num_points), value = c( rnorm(num_points, 0, 1), # normal exp(rnorm(num_points, 0, 1)) - exp(1 / 2) ) ) %&gt;% mutate(distribution = factor(distribution, levels = c(&quot;Normal&quot;, &quot;Log-normal&quot;))) Let’s see how violating the assumption of normality changes the results of t.test. We’ll compare two situations Valid: comparing two normally distributed populations with equal means but unequal variances. Invalid: comparing two log-normally distributed populations with equal means but unequal variances. ggplot(parametric_plotting_data, aes(x = value, color = distribution)) + geom_density(bw = 0.5, size = 1) + geom_vline( data = parametric_plotting_data %&gt;% group_by(distribution) %&gt;% summarize(mean_value = mean(value), sd_value = sd(value)), aes(xintercept = mean_value, color = distribution), linetype = 2, size = 1 ) + xlim(-5, 20) + facet_grid(~distribution, scales = &quot;free&quot;) + guides(color = F) + scale_color_brewer(palette = &quot;Accent&quot;) ## Warning: Removed 7 rows containing non-finite values (stat_density). ggsave(&quot;figures/log_normal_dists.png&quot;, width=5, height=3) ## Warning: Removed 7 rows containing non-finite values (stat_density). gen_data_and_test = function(num_observations_per) { x = rnorm(num_observations_per, 0, 1.1) y = rnorm(num_observations_per, 0, 1.1) pnormal = t.test(x, y, var.equal = T)$p.value # what if the data are log-normally distributed? x = exp(rnorm(num_observations_per, 0, 1.1)) y = exp(rnorm(num_observations_per, 0, 1.1)) pnotnormal = t.test(x, y, var.equal = T)$p.value return(c(pnormal, pnotnormal)) } parametric_issues_demo = function(num_tests, num_observations_per) { replicate(num_tests, gen_data_and_test(num_observations_per)) } set.seed(0) # ensures we get the same results each time we run it num_tests = 1000 # how many datasets to generate/tests to run num_observations_per = 20 # how many obsservations in each dataset parametric_issues_results = parametric_issues_demo(num_tests=num_tests, num_observations_per=num_observations_per) parametric_issues_d = data.frame(valid_tests = parametric_issues_results[1,], invalid_tests = parametric_issues_results[2,], iteration=1:num_tests) %&gt;% gather(type, p_value, contains(&quot;tests&quot;)) %&gt;% mutate(is_significant = p_value &lt; 0.05) # Number significant results with normally distributed data sum(parametric_issues_results[1,] &lt; 0.05) ## [1] 41 # number of significant results with log-normally distributed data sum(parametric_issues_results[2,] &lt; 0.05) ## [1] 25 ggplot(parametric_issues_d, aes(x=type, fill=is_significant)) + geom_bar(stat=&quot;count&quot;, color=&quot;black&quot;) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs(title=&quot;Parametric t-test&quot;) That’s a non-trivial reduction in power from a misspecified model! (~80% to ~54%). boot_mean_diff_test = function(x, y) { obs_t = t.test(x, y)$statistic boot_iterate = function(x, y, indices) { # indices is a dummy here x_samp = sample(x, length(x), replace=T) y_samp = sample(y, length(y), replace=T) mean_diff = mean(y_samp) - mean(x_samp) return(mean_diff) } boots = boot(data = c(x, y), boot_iterate, R=500) # boots = replicate(100, boot_iterate(x, y)) # quants = quantile(boots, probs=c(0.025, 0.975)) quants = boot.ci(boots)$bca[4:5] return(sign(quants[1]) == sign(quants[2])) } (Omitted because with these small sample sizes bootstrapping is problematic – permutations are better) # gen_data_and_boot_test = function(num_observations_per) { # x = rnorm(num_observations_per, 0, 1.1) # y = rnorm(num_observations_per, 0, 1.1) # # pnormal = boot_mean_diff_test(x, y) # # # what if the data are log-normally distributed? # x = exp(rnorm(num_observations_per, 0, 1.1)) # y = exp(rnorm(num_observations_per, 1, 1.1)) # # pnotnormal = boot_mean_diff_test(x, y) # return(c(pnormal, pnotnormal)) # } # # boot_results = replicate(num_tests, gen_data_and_boot_test(num_observations_per)) # sum(boot_results[1,]) # sum(boot_results[2,]) While the bootstrap actually loses power relative to a perfectly specified model, it is much more robust to changes in the assumptions of that model, and so it retains more power when assumptions are violated. perm_mean_diff_test = function(x, y) { obs_t = t.test(x, y)$statistic combined_data = c(x, y) n_combined = length(combined_data) n_x = length(x) perm_iterate = function(x, y) { perm = sample(n_combined) x_samp = combined_data[perm[1:n_x]] y_samp = combined_data[perm[-(1:n_x)]] this_t = t.test(x_samp, y_samp)$statistic return(this_t) } perms = replicate(500, perm_iterate(x, y)) quants = quantile(perms, probs=c(0.025, 0.975)) return(obs_t &lt; quants[1] | obs_t &gt; quants[2]) } # this could be much more efficient gen_data_and_norm_and_perm_test = function(num_observations_per) { d = data.frame(distribution=c(), null_true=c(), parametric=c(), permutation=c()) # normally distributed ## null x = rnorm(num_observations_per, 0, 1.1) y = rnorm(num_observations_per, 0, 1.1) sig_par = t.test(x, y)$p.value &lt; 0.05 sig_perm = perm_mean_diff_test(x, y) d = bind_rows(d, data.frame(distribution=&quot;Normal&quot;, null_true=T, parametric=sig_par, permutation=sig_perm)) ## non-null x = rnorm(num_observations_per, 0, 1.1) y = rnorm(num_observations_per, 1, 1.1) sig_par = t.test(x, y)$p.value &lt; 0.05 sig_perm = perm_mean_diff_test(x, y) d = bind_rows(d, data.frame(distribution=&quot;Normal&quot;, null_true=F, parametric=sig_par, permutation=sig_perm)) # what if the data are log-normally distributed? ## null x = exp(rnorm(num_observations_per, 0, 1.1)) y = exp(rnorm(num_observations_per, 0, 1.1)) sig_par = t.test(x, y)$p.value &lt; 0.05 sig_perm = perm_mean_diff_test(x, y) d = bind_rows(d, data.frame(distribution=&quot;Log-normal&quot;, null_true=T, parametric=sig_par, permutation=sig_perm)) ## non-null x = exp(rnorm(num_observations_per, 0, 1.1)) y = exp(rnorm(num_observations_per, 1, 1.1)) sig_par = t.test(x, y)$p.value &lt; 0.05 sig_perm = perm_mean_diff_test(x, y) d = bind_rows(d, data.frame(distribution=&quot;Log-normal&quot;, null_true=F, parametric=sig_par, permutation=sig_perm)) return(d) } num_tests = 100 perm_results = replicate(num_tests, gen_data_and_norm_and_perm_test(num_observations_per), simplify=F) %&gt;% bind_rows() perm_results = perm_results %&gt;% gather(test_type, significant, parametric, permutation) %&gt;% mutate(distribution=factor(distribution, levels=c(&quot;Normal&quot;, &quot;Log-normal&quot;)), null_true=ifelse(null_true, &quot;Null True&quot;, &quot;Alternative True&quot;)) ggplot(perm_results %&gt;% filter(null_true == &quot;Alternative True&quot;), aes(x = test_type, fill = significant)) + geom_bar(stat = &quot;count&quot;, color = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + facet_grid(null_true ~ distribution) + geom_hline( data = data.frame( null_true = &quot;Alternative True&quot;, alpha = num_tests * 0.8 ), mapping = aes(yintercept = alpha), linetype = 2, size = 1, alpha = 0.5 ) + labs(x = &quot;Test type&quot;, y = &quot;Percent&quot;) + scale_y_continuous( breaks = c(0, 0.8, 1) * num_tests, labels = paste(c(0, 80, 100), &quot;%&quot;, sep = &quot;&quot;) ) ggsave(&quot;figures/perm_test.png&quot;, width=5, height=3) perm_results %&gt;% group_by(test_type, distribution, null_true) %&gt;% summarize(pct_significant = sum(significant)/n()) ## # A tibble: 8 x 4 ## # Groups: test_type, distribution [4] ## test_type distribution null_true pct_significant ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 parametric Normal Alternative True 0.75 ## 2 parametric Normal Null True 0.07 ## 3 parametric Log-normal Alternative True 0.53 ## 4 parametric Log-normal Null True 0.02 ## 5 permutation Normal Alternative True 0.76 ## 6 permutation Normal Null True 0.07 ## 7 permutation Log-normal Alternative True 0.68 ## 8 permutation Log-normal Null True 0.03 15.2.2 Non-IID noise and linear models num_points = 500 true_intercept = 0 true_slope = 1. set.seed(0) parametric_ci_data = data.frame(IV = rep(runif(num_points, -1, 1), 2), type = rep(c(&quot;IID Error&quot;, &quot;Non-IID Error&quot;), each=num_points), error = rep(rnorm(num_points, 0, 1), 2)) %&gt;% mutate(DV = ifelse( type == &quot;IID Error&quot;, true_slope*IV + error, true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV ggplot( parametric_ci_data, aes(x = IV, y = DV, color = type) ) + geom_point(alpha = 0.5) + geom_smooth( method = &quot;lm&quot;, se = T, color = &quot;black&quot;, size = 1.5, level = 0.9999 ) + # inflating the confidence bands a bit # to show their distribution is similar scale_color_brewer(palette = &quot;Accent&quot;) + facet_wrap(~type) + guides(color = F) ggsave(&quot;figures/error_dist_non_null.png&quot;, width=5, height=3) C.f. Anscombe’s quartet, etc. summary(lm(DV ~ IV, parametric_ci_data %&gt;% filter(type==&quot;IID Error&quot;))) ## ## Call: ## lm(formula = DV ~ IV, data = parametric_ci_data %&gt;% filter(type == ## &quot;IID Error&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7575 -0.6600 -0.0231 0.6768 3.2956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03103 0.04379 -0.709 0.479 ## IV 1.11977 0.07739 14.470 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9791 on 498 degrees of freedom ## Multiple R-squared: 0.296, Adjusted R-squared: 0.2946 ## F-statistic: 209.4 on 1 and 498 DF, p-value: &lt; 2.2e-16 summary(lm(DV ~ IV, parametric_ci_data %&gt;% filter(type==&quot;Non-IID Error&quot;))) ## ## Call: ## lm(formula = DV ~ IV, data = parametric_ci_data %&gt;% filter(type == ## &quot;Non-IID Error&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0872 -0.4030 0.0441 0.5100 3.4718 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.05623 0.04928 -1.141 0.254 ## IV 1.22127 0.08708 14.024 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.102 on 498 degrees of freedom ## Multiple R-squared: 0.2831, Adjusted R-squared: 0.2817 ## F-statistic: 196.7 on 1 and 498 DF, p-value: &lt; 2.2e-16 ex2_lm_bootstrap_CIs = function(data, R = 1000) { lm_results = summary(lm(DV ~ IV, data = data))$coefficients bootstrap_coefficients = function(data, indices) { linear_model = lm(DV ~ IV, data = data[indices, ] ) # will select a bootstrap sample of the data return(linear_model$coefficients) } boot_results = boot( data = data, statistic = bootstrap_coefficients, R = R ) boot_intercept_CI = boot.ci(boot_results, index = 1, type = &quot;bca&quot;) boot_slope_CI = boot.ci(boot_results, index = 2, type = &quot;bca&quot;) return(data.frame( intercept_estimate = lm_results[1, 1], intercept_SE = lm_results[1, 2], slope_estimate = lm_results[2, 1], slope_SE = lm_results[2, 2], intercept_boot_CI_low = boot_intercept_CI$bca[4], intercept_boot_CI_hi = boot_intercept_CI$bca[5], slope_boot_CI_low = boot_slope_CI$bca[4], slope_boot_CI_hi = boot_slope_CI$bca[5] )) } set.seed(0) # for bootstraps coefficient_CI_data = parametric_ci_data %&gt;% group_by(type) %&gt;% do(ex2_lm_bootstrap_CIs(.)) %&gt;% ungroup() coefficient_CI_data = coefficient_CI_data %&gt;% gather(variable, value, -type) %&gt;% separate(variable, c(&quot;parameter&quot;, &quot;measurement&quot;), extra = &quot;merge&quot;) %&gt;% spread(measurement, value) %&gt;% mutate( parametric_CI_low = estimate - 1.96 * SE, parametric_CI_hi = estimate + 1.96 * SE ) %&gt;% gather(CI_type, value, contains(&quot;CI&quot;)) %&gt;% separate(CI_type, c(&quot;CI_type&quot;, &quot;high_or_low&quot;), extra = &quot;merge&quot;) %&gt;% spread(high_or_low, value) %&gt;% mutate(CI_type = factor(CI_type)) plot_coefficient_CI_data = function(coefficient_CI_data, errorbar_width = 0.5) { p = ggplot(data = coefficient_CI_data, aes(x = parameter, color = CI_type, y = estimate, ymin = CI_low, ymax = CI_hi)) + geom_hline( data = data.frame( parameter = c(&quot;intercept&quot;, &quot;slope&quot;), estimate = c(true_intercept, true_slope) ), mapping = aes(yintercept = estimate), linetype = 3 ) + geom_point(size = 2, position = position_dodge(width = 0.2)) + geom_errorbar(position = position_dodge(width = 0.2), width = errorbar_width) + facet_grid(~type) + scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(-0.2, 1.5)) + scale_color_brewer(palette = &quot;Dark2&quot;, drop = F) } plot_coefficient_CI_data(coefficient_CI_data) ggsave(&quot;figures/error_dist_CI_example.png&quot;, width = 5, height = 3) plot_coefficient_CI_data( coefficient_CI_data %&gt;% filter(CI_type == &quot;parametric&quot;), 0.25) ggsave(&quot;figures/error_dist_CI_example_parametric_only.png&quot;, width = 5, height = 3) Challenge Q: Why isn’t the error on the intercept changed in the scaling error case? This can result in CIs which aren’t actually at the nominal confidence level! And since CIs are equivalent to t-tests in this setting, this can also increase false positive rates. (Also equivalent to Bayesian CrIs.) num_points = 200 true_intercept = 0 true_slope = 0. set.seed(0) parametric_ci_data = data.frame(IV = rep(runif(num_points, -1, 1), 2), type = rep(c(&quot;IID Error&quot;, &quot;Non-IID Error&quot;), each=num_points), error = rep(rnorm(num_points, 0, 1), 2)) %&gt;% mutate(DV = ifelse( type == &quot;IID Error&quot;, true_slope*IV + error, true_slope*IV + 2*abs(IV)*error)) # error increases proportional to distance from 0 on the IV ggplot( parametric_ci_data, aes(x = IV, y = DV, color = type) ) + geom_point(alpha = 0.5) + geom_smooth( method = &quot;lm&quot;, se = T, color = &quot;black&quot;, size = 1.5, level = 0.9999 ) + # inflating the confidence bands a bit # to show their distribution is similar scale_color_brewer(palette = &quot;Accent&quot;) + facet_wrap(~type) + guides(color = F) ggsave(&quot;figures/error_dist_null.png&quot;, width=5, height=3) error_dist_null_sample = function(num_points) { true_intercept = 0 true_slope = 0 # We&#39;ll sample only for the scaling error case, we know IID works this_data = data.frame(IV = runif(num_points, -1, 1), error = rnorm(num_points, 0, 1)) %&gt;% mutate(DV = true_slope * IV + 2 * abs(IV) * error) # error increases proportional to distance from 0 on the IV coefficient_CI_data = ex2_lm_bootstrap_CIs(this_data, R = 200) # take fewer bootstrap samples, to speed things up coefficient_CI_data = coefficient_CI_data %&gt;% gather(variable, value) %&gt;% separate(variable, c(&quot;parameter&quot;, &quot;measurement&quot;), extra = &quot;merge&quot;) %&gt;% spread(measurement, value) %&gt;% mutate(parametric_CI_low = estimate - 1.96 * SE, parametric_CI_hi = estimate + 1.96 * SE) %&gt;% gather(CI_type, value, contains(&quot;CI&quot;)) %&gt;% separate(CI_type, c(&quot;CI_type&quot;, &quot;high_or_low&quot;), extra = &quot;merge&quot;) %&gt;% spread(high_or_low, value) %&gt;% mutate(significant = sign(CI_hi) == sign(CI_low)) %&gt;% select(parameter, CI_type, significant) return(list(coefficient_CI_data)) } num_simulations = 100 num_points = 200 set.seed(0) noise_dist_simulation_results = replicate(num_simulations, error_dist_null_sample(num_points)) %&gt;% bind_rows() ggplot(noise_dist_simulation_results, aes(x = CI_type, fill = significant)) + geom_bar(stat = &quot;count&quot;, color = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;, direction = -1) + facet_wrap(~parameter) + scale_y_continuous(breaks = c(0, 0.05 * num_simulations, num_simulations), labels = c(&quot;0%&quot;, expression(Nominal ~ alpha), &quot;100%&quot;)) + labs(x = &quot;Test type&quot;, y = &quot;Proportion significant&quot;) + geom_hline(yintercept = 0.05 * num_simulations, linetype = 2) ggsave(&quot;figures/error_dist_proportion_significant.png&quot;, width=5, height=3) noise_dist_simulation_results %&gt;% count(parameter, CI_type, significant) %&gt;% mutate(prop=n/num_simulations) ## # A tibble: 8 x 5 ## parameter CI_type significant n prop ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 intercept boot FALSE 97 0.97 ## 2 intercept boot TRUE 3 0.03 ## 3 intercept parametric FALSE 97 0.97 ## 4 intercept parametric TRUE 3 0.03 ## 5 slope boot FALSE 98 0.98 ## 6 slope boot TRUE 2 0.02 ## 7 slope parametric FALSE 88 0.88 ## 8 slope parametric TRUE 12 0.12 False positive rate nearly triples for the parametric model! 15.2.3 Density estimate conceptual plot density_similarity_conceptual_plot_data = expand.grid( x = seq(0, 4, 0.01), y = seq(0, 4, 0.01) ) %&gt;% mutate( population_1 = exp(-((x - 2)^2 + (y - 3)^2) / 8) * exp(-((x - 2 / y)^2 + (y - 1 / x)^2) / 2), # These are definitely not proper distributions population_2 = exp(-((x)^2 + (y)^2) / 8) * exp(-((x / 2)^2 + (y / 2 - 1 / x)^2) / 2) ) %&gt;% gather(population, value, contains(&quot;population&quot;)) ggplot( density_similarity_conceptual_plot_data, aes(x = x, y = y, z = value, color = population) ) + geom_contour(size = 1, bins = 8) + scale_color_brewer(palette = &quot;Dark2&quot;) + facet_wrap(~population) + labs(x = &quot;Feature 1&quot;, y = &quot;Feature 2&quot;) + guides(color = F) ggsave(&quot;figures/conceptual_density_plot.png&quot;, width=5, height=3) 15.3 Bootstrap resampling 15.3.1 Demo num_points = 100 true_intercept = 0 true_slope = 1. set.seed(2) # I p-hacked the shit out of this demo to make the ideas more clear parametric_ci_data = data.frame( IV = rep(runif(num_points, -1, 1), 2), type = rep(c(&quot;IID Error&quot;, &quot;Scaling Error&quot;), each = num_points), error = rep(rnorm(num_points, 0, 1), 2) ) %&gt;% mutate(DV = ifelse( type == &quot;IID Error&quot;, true_slope * IV + error, true_slope * IV + 2 * abs(IV) * error )) # error increases proportional to distance from 0 on the IV p = ggplot(parametric_ci_data, aes(x = IV, y = DV)) + geom_point(alpha = 0.25) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;black&quot;, size = 1.5) + facet_wrap(~type) p ggsave(&quot;figures/bootstrap_demo_0.png&quot;, width=5, height=3) set.seed(15) # See above RE: p-hacking samp_1_indices = sample(2:num_points, num_points, replace = T) samp_1 = parametric_ci_data[c(samp_1_indices, samp_1_indices + num_points), ] # take the same rows from each type of data set.seed(2) # See above RE: p-hacking samp_2_indices = sample(1:num_points, num_points, replace = T) samp_2 = parametric_ci_data[c(samp_2_indices, samp_2_indices + num_points), ] many_samples_indices = sample(1:num_points, 8 * num_points, replace = T) many_samples = bind_rows( samp_1 %&gt;% mutate(sample = 1), samp_2 %&gt;% mutate(sample = 2), parametric_ci_data[c(many_samples_indices, many_samples_indices + num_points), ] %&gt;% mutate(sample = rep(rep(3:10, each = num_points), 2)) ) p + geom_point( data = samp_1, aes(color = NA), color = &quot;red&quot;, alpha = 0.5 ) ggsave(&quot;figures/bootstrap_demo_1.png&quot;, width=5, height=3) p + geom_point( data = samp_1, aes(color = NA), color = &quot;red&quot;, alpha = 0.5 ) + geom_smooth( data = samp_1, method = &quot;lm&quot;, se = F, color = &quot;red&quot;, size = 1.5 ) ggsave(&quot;figures/bootstrap_demo_2.png&quot;, width=5, height=3) p + geom_point(data=samp_2, aes(color=NA), color=&quot;red&quot;, alpha=0.5) ggsave(&quot;figures/bootstrap_demo_3.png&quot;, width=5, height=3) p + geom_point( data = samp_2, aes(color = NA), color = &quot;red&quot;, alpha = 0.5 ) + geom_smooth( data = samp_2, method = &quot;lm&quot;, se = F, color = &quot;red&quot;, size = 1.5 ) ggsave(&quot;figures/bootstrap_demo_4.png&quot;, width=5, height=3) p + geom_smooth( data = many_samples, aes(group = sample), method = &quot;lm&quot;, se = F, size = 1.5, color = &quot;red&quot; ) ggsave(&quot;figures/bootstrap_demo_5.png&quot;, width=5, height=3) 15.4 Applications 15.4.1 Bootstrap confidence intervals num_top_points = 23 num_mid_points = 4 num_outlier_points = 2 max_score = 100 set.seed(0) test_score_data = data.frame( score = c(rbinom(num_top_points, max_score, 0.9999), rbinom(num_mid_points, max_score, 0.97), sample(0:max_score, num_outlier_points, replace = T)), type = &quot;Observed sample&quot; ) get_mean_score = function(data, indices) { return(mean(data[indices,]$score)) } bootstrap_results = boot(test_score_data, get_mean_score, R=100) bootstrap_CIs = boot.ci(bootstrap_results) ## Warning in boot.ci(bootstrap_results): bootstrap variances needed for ## studentized intervals ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as ## endpoints bootstrap_CIs ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results) ## ## Intervals : ## Level Normal Basic ## 95% ( 86.52, 102.43 ) ( 88.40, 103.45 ) ## ## Level Percentile BCa ## 95% (84.75, 99.81 ) (84.00, 99.68 ) ## Calculations and Intervals on Original Scale ## Some basic intervals may be unstable ## Some percentile intervals may be unstable ## Warning : BCa Intervals used Extreme Quantiles ## Some BCa intervals may be unstable test_summary_data = test_score_data %&gt;% summarise( mean = mean(score), se = sd(score) / sqrt(n()), parametric_CI_low = mean - 1.96 * se, parametric_CI_high = mean + 1.96 * se ) test_score_data = test_score_data %&gt;% bind_rows( data.frame(score = bootstrap_results$t, type = &quot;Boot. sampling dist.&quot;) ) %&gt;% mutate(type = factor(type, levels = c(&quot;Observed sample&quot;, &quot;Boot. sampling dist.&quot;))) ## Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector test_summary_data = test_summary_data %&gt;% mutate( type = factor(&quot;Boot. sampling dist.&quot;, levels = levels(test_score_data$type)), percentile_CI_low = bootstrap_CIs$percent[4], percentile_CI_high = bootstrap_CIs$percent[5], bca_CI_low = bootstrap_CIs$bca[4], bca_CI_high = bootstrap_CIs$bca[5] ) %&gt;% gather(CI_type, value, contains(&quot;CI&quot;)) %&gt;% separate(CI_type, c(&quot;CI_type&quot;, &quot;endpoint&quot;), extra = &quot;merge&quot;) %&gt;% spread(endpoint, value) %&gt;% mutate( y = c(170, 210, 190), CI_type = factor(CI_type, levels = c(&quot;parametric&quot;, &quot;percentile&quot;, &quot;bca&quot;), labels = c(&quot;Normal&quot;, &quot;Boot: %&quot;, &quot;Boot: BCA&quot;)) ) ggplot(test_summary_data %&gt;% filter(CI_type != &quot;Boot: BCA&quot;), aes(x = score)) + geom_histogram( data = test_score_data, binwidth = 1 ) + geom_point( mapping = aes(x = mean, y = y, color = CI_type), size = 2 ) + geom_errorbarh( mapping = aes(y = y, color = CI_type, xmin = CI_low, x = NULL, xmax = CI_high), size = 1, position = position_dodge() ) + facet_grid(type ~ ., scales = &quot;free_y&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + guides(color = guide_legend(title = &quot;CI&quot;)) ## Warning: position_dodge requires non-overlapping x intervals ggsave(&quot;figures/bootstrap_CI_0.png&quot;, width=5, height=3) ## Warning: position_dodge requires non-overlapping x intervals ggplot(test_summary_data, aes(x = score)) + geom_histogram( data = test_score_data, binwidth = 1 ) + geom_point( mapping = aes(x = mean, y = y, color = CI_type), size = 2 ) + geom_errorbarh( mapping = aes(y = y, color = CI_type, xmin = CI_low, x = NULL, xmax = CI_high), size = 1, position = position_dodge() ) + facet_grid(type ~ ., scales = &quot;free_y&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + guides(color = guide_legend(title = &quot;CI&quot;)) ## Warning: position_dodge requires non-overlapping x intervals ggsave(&quot;figures/bootstrap_CI_1.png&quot;, width = 5, height = 3) ## Warning: position_dodge requires non-overlapping x intervals 15.4.2 Bootstrap (&amp; permutation) hypothesis tests num_flips = 20 true_heads_prob = 0.9 set.seed(2) flips = rbinom(num_flips, 1, true_heads_prob) flip_data = data.frame(flip_result = factor(flips, labels = c(&quot;Tails&quot;, &quot;Heads&quot;))) flip_data_plot = ggplot(data = flip_data, aes(x = flips, fill = flips)) + geom_dotplot(binwidth = 0.03) + scale_x_continuous( breaks = c(0, 1), labels = c(&quot;tails&quot;, &quot;heads&quot;) ) + scale_y_continuous(breaks = c()) + labs(x = &quot;Flip result&quot;, y = &quot;&quot;) get_mean_heads = function(data, indices) { return(mean(data[indices, &quot;flip_result&quot;] == &quot;Heads&quot;)) } set.seed(0) bootstrap_results = boot(flip_data, get_mean_heads, R = 20000) bootstrap_CIs = boot.ci(bootstrap_results) ## Warning in boot.ci(bootstrap_results): bootstrap variances needed for ## studentized intervals bootstrap_CIs ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 20000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results) ## ## Intervals : ## Level Normal Basic ## 95% ( 0.6940, 1.0061 ) ( 0.7000, 1.0000 ) ## ## Level Percentile BCa ## 95% ( 0.70, 1.00 ) ( 0.55, 0.95 ) ## Calculations and Intervals on Original Scale flip_boot_plot = ggplot( data = data.frame(mean_flips = bootstrap_results$t), aes(x = mean_flips) ) + geom_histogram(binwidth = 0.05) + xlim(0, 1) + geom_vline( xintercept = 0.5, color = &quot;red&quot;, size = 1.1 ) + annotate(&quot;text&quot;, label = &quot;Null value&quot;, color = &quot;red&quot;, x = 0.43, y = 2500, angle = 90, size = 4 ) + annotate(&quot;text&quot;, label = &quot;95% CI&quot;, color = &quot;blue&quot;, x = 0.75, y = 5400, size = 4 ) + geom_errorbarh(aes( xmin = bootstrap_CIs$bca[4], xmax = bootstrap_CIs$bca[5], y = 5100 ), color = &quot;blue&quot;, size = 1.1, height = 200 ) + labs(x = &quot;Boot. sampling dist.&quot;) flip_boot_plot ## Warning: Removed 2 rows containing missing values (geom_bar). flip_data_plot + flip_boot_plot + plot_layout() ## Warning: Removed 2 rows containing missing values (geom_bar). ggsave(&quot;figures/bootstrap_test.png&quot;, width = 5, height = 2.5) ## Warning: Removed 2 rows containing missing values (geom_bar). bootstrap_CIs = boot.ci(bootstrap_results, conf = 0.999) ## Warning in boot.ci(bootstrap_results, conf = 0.999): bootstrap variances ## needed for studentized intervals ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as ## endpoints flip_boot_plot = ggplot( data = data.frame(mean_flips = bootstrap_results$t), aes(x = mean_flips) ) + geom_histogram(binwidth = 0.05) + xlim(0, 1) + geom_vline( xintercept = 0.5, color = &quot;red&quot;, size = 1.1 ) + annotate(&quot;text&quot;, label = &quot;Null value&quot;, color = &quot;red&quot;, x = 0.43, y = 2500, angle = 90, size = 4 ) + annotate(&quot;text&quot;, label = &quot;99.9% CI&quot;, color = &quot;blue&quot;, x = 0.75, y = 5400, size = 4 ) + geom_errorbarh(aes( xmin = bootstrap_CIs$bca[4], xmax = bootstrap_CIs$bca[5], y = 5100 ), color = &quot;blue&quot;, size = 1.1, height = 200 ) + labs(x = &quot;Boot. sampling dist.&quot;) flip_boot_plot ## Warning: Removed 2 rows containing missing values (geom_bar). flip_data_plot + flip_boot_plot + plot_layout() ## Warning: Removed 2 rows containing missing values (geom_bar). ggsave(&quot;figures/bootstrap_test_999.png&quot;, width=5, height=2.5) ## Warning: Removed 2 rows containing missing values (geom_bar). "],
["model-comparison.html", "Chapter 16 Model comparison 16.1 Load packages and set plotting theme 16.2 Determining sample size 16.3 Model comparison 16.4 Additional resources", " Chapter 16 Model comparison 16.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;pwr&quot;) # for power analysis library(&quot;cowplot&quot;) # for figure panels library(&quot;modelr&quot;) # for cross-validation library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 16.2 Determining sample size 16.2.1 pwr package Let’s figure out how many participants we would need to get a power of \\(1-\\beta = 0.8\\) for testing an alternative hypothesis \\(H_1\\) according to which a coin is biased to come up heads with \\(p = 0.75\\) against a null hypothesis \\(H_0\\) according to which the coin is far \\(p = 0.5\\). Let’s set our desired alpha level to \\(\\alpha = .05\\) and considered a one-tailed test. I’ll use the \"pwr\" library to do determine the sample size. pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50), sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5235988 ## n = 22.55126 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater I first calculated the effect size using the ES.h() function providing the two proportions as arguments. Take a look at the help file for ES.h() to figure see how it’s calculated. In short, the effect treats differences in proportions that are close to 0.5 different from ones that are close to the endpoints of the scale (i.e. 0 or 1). Intuitively, it’s more impressive to change a probability from 0.9 to 1 than it would be to change a probability from 0.5 to 0.6. The effect size captures this. We find that to reach a power of 0.8 at a \\(\\alpha = .05\\) assuming a one-tailed test, we would need to run \\(n = 23\\) participants. The \"pwr\" package also makes plots to illustrate how power changes with sample size: pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.50), sig.level = 0.05, power = 0.80, alternative = &quot;greater&quot;) %&gt;% plot() + theme(title = element_text(size = 16)) 16.2.2 map() The family of map() functions comes with the \"purrr\" package which is loaded as part of the tidyverse. It’s a powerful function that allows us to avoid writing for-loops. Using the map() function makes otherwise complex procedures much simpler, allows for code that’s easier to read, and that’s much faster to run. Her are some examples of how map() works: # using the formula notation with ~ map(.x = 1:3, .f = ~ .x^2) # the same computation using an anonymous function map(.x = 1:3, .f = function(.x) .x^2) # outputs a vector map_dbl(.x = 1:3, .f = ~ .x^2) # using a function square = function(x){x^2} map_dbl(1:3, square) # with multiple arguments map2_dbl(.x = 1:3, .y = 1:3, .f = ~ .x * .y) I encourage you to take a look at the purrr cheatsheet, as well as skimming the datacamp courses on functional programming (see Additional Resources below). Mastering map() is a key step to becoming an R power user :) 16.2.3 via simulation 16.2.3.1 simple example Let’s start with a simple example. We want to determine what power we have to correctly reject the \\(H_0\\) according to which the coin is fair, for \\(H_1: p = 0.75\\) with \\(\\alpha = 0.05\\) (one-tailed) and a sample size of \\(n = 10\\). Let’s see: # make example reproducible set.seed(1) # parameters p1 = 0.5 p2 = 0.75 alpha = 0.05 n_simulations = 100 n = 10 # set up the simulation grid df.pwr = crossing(sample_size = n, n_simulations = 1:n_simulations, p1 = p1, p2 = p2, alpha = alpha) # draw random samples from the binomial distribution df.pwr = df.pwr %&gt;% mutate(n_heads = rbinom(n = n(), size = sample_size, prob = p2)) # apply binomial test for each simulation and extract p-value df.pwr = df.pwr %&gt;% group_by(n_simulations) %&gt;% nest() %&gt;% mutate(binom_test = map(data, ~ binom.test(x = .$n_heads, n = .$sample_size, p = 0.5, alternative = &quot;greater&quot;)), p_value = map(binom_test, ~ .$p.value)) # calculate the proportion with which the H0 would be rejected (= power) df.pwr %&gt;% summarize(power = sum(p_value &lt; .05) /n()) ## # A tibble: 1 x 1 ## power ## &lt;dbl&gt; ## 1 0.18 So, the results of this example show, that with \\(n = 10\\) participants, we only have a power of .18 to reject the null hypothesis \\(H_0: p = 0.5\\) when the alternative hypothesis \\(H_1: p = 0.75\\) is true. Not an experiment we should run … 16.2.3.2 more advanced example This more advanced example, which we discussed in class, calculates power for a different sample sizes (from \\(n = 10\\) to \\(n = 50\\)), and for different alternative hypotheses \\(H_1: p = 0.75\\), \\(H_1: p = 0.8\\), and \\(H_1: p = 0.85\\). I then figure out for what n we would get a power of 0.8 assuming \\(H_1: p = 0.75\\). Otherwise, the procedure is identical to the simple example above. # make reproducible set.seed(1) # number of simulations n_simulations = 200 # run simulation df.power = crossing(n = seq(10, 50, 1), simulation = 1:n_simulations, p = c(0.75, 0.8, 0.85)) %&gt;% mutate(index = 1:n()) %&gt;% # add an index column mutate(response = rbinom(n = n(), size = n, prob = p)) %&gt;% # generate random data group_by(index, simulation, p) %&gt;% nest() %&gt;% # put data in list column mutate(fit = map(data, ~ binom.test(x = .$response, # define formula n = .$n, p = 0.5, alternative = &quot;greater&quot;)), p.value = map_dbl(fit, ~ .$p.value)) %&gt;% # run binomial test and extract p-value unnest(data) %&gt;% select(-fit) Let’s visualze the relationship between power and sample size for the three alternative hypotheses: # data frame for plot df.plot = df.power %&gt;% group_by(n, p) %&gt;% summarize(power = sum(p.value &lt; 0.05) / n()) %&gt;% ungroup() %&gt;% mutate(p = as.factor(p)) # plot data ggplot(data = df.plot, mapping = aes(x = n, y = power, color = p, group = p)) + geom_smooth(method = &quot;loess&quot;) # find optimal n based on simulations df.plot %&gt;% filter(p == 0.75, near(power, 0.8, tol = 0.01)) ## # A tibble: 1 x 3 ## n p power ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 27 0.75 0.795 Let’s compare with the solution that the pwr package gives. # analytic solution pwr.p.test(h = ES.h(p1 = 0.75, p2 = 0.5), power = 0.8, sig.level = 0.05, alternative = &quot;greater&quot;) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.5235988 ## n = 22.55126 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater Pretty close! To get more accuracy in our simulation, we would simply need to increase the number of simulated statistical tests we perform to calculate power. 16.3 Model comparison In general, we want our models to explain the data we observed, and correctly predict future data. Often, there is a trade-off between how well the model fits the data we have (e.g. how much of the variance it explains), and how well the model will predict future data. If our model is too complex, then it will not only capture the systematicity in the data but also fit to the noise in the data. If our mdoel is too simple, however, it will not capture some of the systematicity that’s actually present in the data. The goal, as always in statistical modeling, is to find a model that finds the sweet spot between simplicity and complexity. 16.3.1 Fitting vs. predicting Let’s illustrate the trad-off between complexity and simplicty for fitting vs. prediction. We generate data from a model of the following form: \\[ Y_i = \\beta_0 + \\beta_1 \\cdot X_i + \\beta_2 + X_i^2 + \\epsilon_i \\] where \\[ \\epsilon_i \\sim \\mathcal{N}(\\text{mean} = 0, ~\\text{sd} = 20) \\] Here, I’ll use the following parameters: \\(\\beta_0 = 10\\), \\(\\beta_1 = 3\\), and \\(\\beta_2 = 2\\) to generate the data: set.seed(1) n_plots = 3 n_samples = 20 # sample size n_parameters = c(1:4, seq(7, 19, length.out = 5)) # number of parameters in the polynomial regression # generate data df.data = tibble( x = runif(n_samples, min = 0, max = 10), y = 10 + 3 * x + 3 * x^2 + rnorm(n_samples, sd = 20) ) # plotting function plot_fit = function(i){ # calculate RMSE rmse = lm(formula = y ~ poly(x, degree = i, raw = TRUE), data = df.data) %&gt;% augment() %&gt;% summarize(rmse = .resid^2 %&gt;% mean() %&gt;% sqrt() %&gt;% round(2)) # make a plot ggplot(data = df.data, mapping = aes(x = x, y = y)) + geom_point(size = 2) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, degree = i, raw = TRUE)) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = str_c(&quot;RMSE = &quot;, rmse), hjust = 1.1, vjust = -0.3) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) } # save plots in a list l.p = map(n_parameters, plot_fit) # make figure panel plot_grid(plotlist = l.p, ncol = 3) ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading As we can see, RMSE becomes smaller and smaller the more parameters the model has to fit the data. But how does the RMSE look like for new data that is generated from the same underlying ground truth? set.seed(1) n_plots = 3 n_samples = 20 # sample size n_parameters = c(1:4, seq(7, 19, length.out = 5)) # number of parameters in the polynomial regression # generate data df.data = tibble( x = runif(n_samples, min = 0, max = 10), y = 10 + 3 * x + 3 * x^2 + rnorm(n_samples, sd = 20) ) # generate some more data df.more_data = tibble( x = runif(50, min = 0, max = 10), y = 10 + 3 * x + 3 * x^2 + rnorm(50, sd = 20) ) # list for plots l.p = list() # plotting function plot_fit = function(i){ # calculate RMSE for fitted data fit = lm(formula = y ~ poly(x, degree = i, raw = TRUE), data = df.data) rmse = fit %&gt;% augment() %&gt;% summarize(rmse = .resid^2 %&gt;% mean() %&gt;% sqrt() %&gt;% round(2)) # calculate RMSE for new data rmse_new = fit %&gt;% augment(newdata = df.more_data) %&gt;% summarize(rmse = (y - .fitted)^2 %&gt;% mean() %&gt;% sqrt() %&gt;% round(2)) # make a plot ggplot(data = df.data, mapping = aes(x = x, y = y)) + geom_point(size = 2) + geom_point(data = df.more_data, size = 2, color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, formula = y ~ poly(x, degree = i, raw = TRUE)) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = str_c(&quot;RMSE = &quot;, rmse), hjust = 1.1, vjust = -0.3) + annotate(geom = &quot;text&quot;, x = Inf, y = -Inf, label = str_c(&quot;RMSE = &quot;, rmse_new), hjust = 1.1, vjust = -2, color = &quot;red&quot;) + theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank()) } # map over the parameters l.p = map(n_parameters, plot_fit) # make figure panel plot_grid(plotlist = l.p, ncol = 3) ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading The RMSE in black shows the root mean squared error for the data that the model was fit on. The RMSE in red shows the RMSE on the new data. As you can see, the complex models do really poorly. They overfit the noise in the original data which leads to make poor predictions for new data. The simplest model (with two parameters) doesn’t do particularly well either since it misses out on the quadratic trend in the data. Both the model with the quadratic term (top middle) and a model that includes a cubic term (top right) provide a good balance – their RMSE on the new data is lowest. Let’s generate another data set: # make example reproducible set.seed(1) # parameters sample_size = 100 b0 = 1 b1 = 2 b2 = 3 sd = 0.5 # sample df.data = tibble( participant = 1:sample_size, x = runif(sample_size, min = 0, max = 1), y = b0 + b1*x + b2*x^2 + rnorm(sample_size, sd = sd) ) And plot it: ggplot(data = df.data, mapping = aes(x = x, y = y)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)) + geom_point() 16.3.2 F-test Let’s fit three models of increasing complexity to the data. The model which fits the way in which the data were generated has the following form: \\[ \\widehat Y_i = b_0 + b_1 \\cdot X_i + b_2 \\cdot X_i^2 \\] # fit models to the data fit_simple = lm(y ~ 1 + x, data = df.data) fit_correct = lm(y ~ 1 + x + I(x^2), data = df.data) fit_complex = lm(y ~ 1 + x + I(x^2) + I(x^3), data = df.data) # compare the models using an F-test anova(fit_simple, fit_correct) ## Analysis of Variance Table ## ## Model 1: y ~ 1 + x ## Model 2: y ~ 1 + x + I(x^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 25.297 ## 2 97 21.693 1 3.6039 16.115 0.0001175 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit_correct, fit_complex) ## Analysis of Variance Table ## ## Model 1: y ~ 1 + x + I(x^2) ## Model 2: y ~ 1 + x + I(x^2) + I(x^3) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 21.693 ## 2 96 21.643 1 0.050399 0.2236 0.6374 The F-test tells us that fit_correct explains significantly more variance than fit_simple, whereas fit_complex doesn’t explain significantly more variance than fit_correct. But, as discussed in class, there are many situations in which we cannot use the F-test to compare models. Namely, whenever we want to compare unnested models where one models does not include all the predictors of the other model. But, we can still use cross-validation in this case. Let’s take a look. 16.3.3 Cross-validation Cross-validation is a powerful technique for finding the sweet spot between simplicity and complexity. Moreover, we can use cross-validation to compare models that we cannot compare using the F-test approach that we’ve been using up until now. There are many different kinds of cross-validation. All have the same idea in common though: we first fit the model to a subset of the data, often called training data and then check how well the model captures the held-out data, often called test data Different versions of cross-validation differ in how the training and test data sets are defined. We’ll look at three different cross-validation techniques: Leave-on-out cross-validation k-fold cross-validation Monte Carlo cross-validation 16.3.3.1 Leave-one-out cross-validation I’ve used code similar to this one to illustrate how LOO works in class. Here is a simple data set with 9 data points. We fit 9 models, where for each model, the training set includes one of the data points, and then we look at how well the model captures the held-out data point. We can then characterize the model’s performance by calculating the mean squared error across the 9 runs. # make example reproducible set.seed(1) # sample df.loo = tibble( x = 1:9, y = c(5, 2, 4, 10, 3, 4, 10, 2, 8) ) df.loo_cross = df.loo %&gt;% crossv_loo() %&gt;% mutate(fit = map(train, ~ lm(y ~ x, data = .)), tidy = map(fit, tidy)) %&gt;% unnest(tidy) # original plot df.plot = df.loo %&gt;% mutate(color = 1) # fit to all data except one fun.cv_plot = function(data_point){ # determine which point to leave out df.plot$color[data_point] = 2 # fit df.fit = df.plot %&gt;% filter(color != 2) %&gt;% lm(y ~ x, data = .) %&gt;% augment(newdata = df.plot[df.plot$color == 2,]) %&gt;% clean_names() p = ggplot(df.plot, aes(x, y, color = as.factor(color))) + geom_segment(aes(xend = x, yend = fitted), data = df.fit, color = &quot;red&quot;, size = 1) + geom_point(size = 2) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;black&quot;, fullrange = T, data = df.plot %&gt;% filter(color != 2)) + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + theme(legend.position = &quot;none&quot;, axis.title = element_blank(), axis.ticks = element_blank(), axis.text = element_blank()) return(p) } # save plots in list l.plots = map(1:9, fun.cv_plot) # make figure panel plot_grid(plotlist = l.plots, ncol = 3) As you can see, the regression line changes quite a bit depending on which data point is in the test set. Now, let’s use LOO to evaluate the models on the data set I’ve created above: # fit the models and calculate the RMSE for each model on the test set df.cross = df.data %&gt;% crossv_loo() %&gt;% # function which generates training and test data sets mutate(model_simple = map(train, ~ lm(y ~ 1 + x, data = .)), model_correct = map(train, ~ lm(y ~ 1 + x + I(x^2), data = .)), model_complex = map(train, ~ lm(y ~ 1 + x + I(x^2) + I(x^3), data = .))) %&gt;% gather(&quot;model&quot;, &quot;fit&quot;, contains(&quot;model&quot;)) %&gt;% mutate(rmse = map2_dbl(fit, test, rmse)) # show the average RMSE for each model df.cross %&gt;% group_by(model) %&gt;% summarize(mean_rmse = mean(rmse) %&gt;% round(3)) ## # A tibble: 3 x 2 ## model mean_rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 model_complex 0.382 ## 2 model_correct 0.378 ## 3 model_simple 0.401 As we can see, the model_correct has the lowest average RMSE on the test data. One downside with LOO is that it becomes unfeasible when the number of data points is very large, as the number of cross validation runs equals the number of data points. The next cross-validation procedures help in this case. 16.3.3.2 k-fold cross-validation For k-fold cross-validation, we split the data set in k folds, and then use k-1 folds as the training set, and the remaining fold as the test set. The code is almost identical as before. Instead of crossv_loo(), we use the crossv_kfold() function instead and say how many times we want to “fold” the data. # crossvalidation scheme df.cross = df.data %&gt;% crossv_kfold(k = 10) %&gt;% mutate(model_simple = map(train, ~ lm(y ~ 1 + x, data = .)), model_correct = map(train, ~ lm(y ~ 1 + x + I(x^2), data = .)), model_complex = map(train, ~ lm(y ~ 1 + x + I(x^2) + I(x^3), data = .))) %&gt;% gather(&quot;model&quot;, &quot;fit&quot;, contains(&quot;model&quot;)) %&gt;% mutate(rsquare = map2_dbl(fit, test, rsquare)) df.cross %&gt;% group_by(model) %&gt;% summarize(median_rsquare = median(rsquare)) ## # A tibble: 3 x 2 ## model median_rsquare ## &lt;chr&gt; &lt;dbl&gt; ## 1 model_complex 0.907 ## 2 model_correct 0.906 ## 3 model_simple 0.884 Note, for this example, I’ve calculated \\(R^2\\) (the variance explained by each model) instead of RMSE – just to show you that you can do this, too. Often it’s useful to do both: show how well the model correlates, but also show the error. 16.3.3.3 Monte Carlo cross-validation Finally, let’s consider another very flexible version of cross-validation. For this version of cross-validation, we determine how many random splits into training set and test set we would like to do, and what proportion of the data should be in the test set. # crossvalidation scheme df.cross = df.data %&gt;% crossv_mc(n = 50, test = 0.5) %&gt;% # number of samples, and percentage of test mutate(model_simple = map(train, ~ lm(y ~ 1 + x, data = .x)), model_correct = map(train, ~ lm(y ~ 1 + x + I(x^2), data = .x)), model_complex = map(train, ~ lm(y ~ 1 + x + I(x^2) + I(x^3), data = .))) %&gt;% gather(&quot;model&quot;, &quot;fit&quot;, contains(&quot;model&quot;)) %&gt;% mutate(rmse = map2_dbl(fit, test, rmse)) df.cross %&gt;% group_by(model) %&gt;% summarize(mean_rmse = mean(rmse)) ## # A tibble: 3 x 2 ## model mean_rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 model_complex 0.493 ## 2 model_correct 0.485 ## 3 model_simple 0.513 In this example, I’ve asked for \\(n = 50\\) splits and for each split, half of the data was in the training set, and half of the data in the test set. 16.3.4 Bootstrap We can also use the modelr package for bootstrapping. The idea is the same as when we did cross-validation. We create a number of data sets from our original data set. Instead of splitting the data set in a training and test data set, for bootstrapping, we sample values from the original data set with replacement. Doing so, we can, for example, calculate the confidence interval of different statistics of interest. Here is an example for how to boostrap confidence intervals for a mean. # make example reproducible set.seed(1) sample_size = 10 # sample df.data = tibble( participant = 1:sample_size, x = runif(sample_size, min = 0, max = 1) ) # mean of the actual sample mean(df.data$x) ## [1] 0.5515139 # bootstrap to get confidence intervals around the mean df.data %&gt;% bootstrap(n = 1000) %&gt;% # create 1000 boostrapped samples mutate(estimate = map_dbl(strap, ~ mean(.$data$x[.$idx]))) %&gt;% # get the sample mean summarize(mean = mean(estimate), low = quantile(estimate, 0.025), # calculate the 2.5 / 97.5 percentiles high = quantile(estimate, 0.975)) ## # A tibble: 1 x 3 ## mean low high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.545 0.367 0.725 Note the somewhat weird construction ~ mean(.$data$x[.$idx])). This is just because the bootstrap function stores the information about each boostrapped data set in that way. Each boostrapped sample simply points to the original data set, and then uses a different set of indices idx to indicate which values from the original data set it sampled (with replacement). 16.3.5 AIC and BIC The Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) are defined as follows: \\[ \\text{AIC} = 2k-2\\ln(\\hat L) \\] \\[ \\text{BIC} = \\ln(n)k-2\\ln(\\hat L) \\] where \\(k\\) is the number of parameters in the model, \\(n\\) is the number of observations, and \\(\\hat L\\) is the maximized value of the likelihood function of the model. Both AIC and BIC trade off model fit (as measured by the maximum likelihood of the data \\(\\hat L\\)) and the number of parameters in the model. Calculating AIC and BIC in R is straightforward. We simply need to fit a linear model, and then call the AIC() or BIC() functions on the fitted model like so: set.seed(0) # let&#39;s generate some data df.example = tibble( x = runif(20, min = 0, max = 1), y = 1 + 3 * x + rnorm(20, sd = 2) ) # fit a linear model fit = lm(formula = y ~ 1 + x, data = df.example) # get AIC AIC(fit) ## [1] 75.47296 # get BIC BIC(fit) ## [1] 78.46016 We can also just use the broom package to get that information: fit %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.255 0.214 1.45 6.16 0.0232 2 -34.7 75.5 78.5 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Both AIC and BIC take the number of parameters and the model’s likelihood into account. BIC additionally considers the number of observations. But how is the likelihood of a linear model determined? Let’s visualize the data first: # plot the data with a linear model fit ggplot(df.example, aes(x, y)) + geom_point(size = 2) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) Now, let’s take a look at the residuals by plotting the fitted values on the x axis, and the residuals on the y axis. # residual plot df.plot = df.example %&gt;% lm(y ~ x, data = .) %&gt;% augment() %&gt;% clean_names() ggplot(df.plot, aes(fitted, resid)) + geom_point(size = 2) Remember that the linear model makes the assumption that the residuals are normally distributed with mean 0 (which is always the case if we fit a linear model) and some fitted standard deviation. In fact, the standard deviation of the normal distribution is fitted such that the overall likelihood of the data is maximized. Let’s make a plot that shows a normal distribution alongside the residuals: # define a normal distribution df.normal = tibble( y = seq(-5, 5, 0.1), x = dnorm(y, sd = 2) + 3.9 ) # show the residual plot together with the normal distribution df.plot %&gt;% ggplot(aes(x = fitted, y = resid)) + geom_point() + geom_path(data = df.normal, aes(x = x, y = y), size = 2) To determine the likelihood of the data given the model \\(\\hat L\\), we now calculate the likelihood of each point (with the dnorm() function), and then multiply the likelihood of each data point to get the overall likelihood. We can simply multiply the data points since we also assume that the data points are independent. Instead of multiplying likelihoods, we often sum the log likelihoods instead. This is because if we multiply many small values, the overall value gets to close to 0 so that computers get confused. By taking logs instead, we avoid these nasty precision errors. To better understand AIC and BIC, let’s calculate them by hand: # we first get the estimate of the standard deviation of the residuals sigma = fit %&gt;% glance() %&gt;% pull(sigma) # then we calculate the log likelihood of the model log_likelihood = fit %&gt;% augment() %&gt;% mutate(likelihood = dnorm(.resid, sd = sigma)) %&gt;% summarize(logLik = sum(log(likelihood))) %&gt;% as.numeric() # then we calculate AIC and BIC using the formulas introduced above aic = 2*3 - 2 * log_likelihood bic = log(nrow(df.example)) * 3 - 2 * log_likelihood print(aic) ## [1] 75.58017 print(bic) ## [1] 78.56737 Cool! The values are the same as when we use the glance() function like so (except for a small difference due to rounding): fit %&gt;% glance() %&gt;% select(AIC, BIC) ## # A tibble: 1 x 2 ## AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; ## 1 75.5 78.5 16.3.5.1 log() is your friend ggplot(data = tibble(x = c(0, 1)), mapping = aes(x = x)) + stat_function(fun = &quot;log&quot;, size = 1) + labs(x = &quot;probability&quot;, y = &quot;log(probability)&quot;) + theme(axis.text = element_text(size = 24), axis.title = element_text(size = 26)) 16.4 Additional resources 16.4.1 Cheatsheet purrr 16.4.2 Datacamp course Foundations of Functional Programming with purrr Intermediate functional programming with purrr 16.4.3 Reading R for Data Science: Chapter 25 16.4.4 Misc G*Power 3.1: Software for power calculations "],
["linear-mixed-effects-models-1.html", "Chapter 17 Linear mixed effects models 1 17.1 Load packages and set plotting theme 17.2 Things that came up in class 17.3 Dependence 17.4 Additional resources", " Chapter 17 Linear mixed effects models 1 17.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;patchwork&quot;) # for making figure panels library(&quot;lme4&quot;) # for linear mixed effects models library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 17.2 Things that came up in class 17.2.1 Comparing t-test with F-test in lm() What’s the difference between the t-test on individual predictors in the model and the F-test comparing two models (one with, and one without the predictor)? Let’s generate some data first: # make example reproducible set.seed(1) # parameters sample_size = 100 b0 = 1 b1 = 0.5 b2 = 0.5 sd = 0.5 # sample df.data = tibble( participant = 1:sample_size, x1 = runif(sample_size, min = 0, max = 1), x2 = runif(sample_size, min = 0, max = 1), # simple additive model y = b0 + b1 * x1 + b2 * x2 + rnorm(sample_size, sd = sd) ) # fit linear model fit = lm(formula = y ~ 1 + x1 + x2, data = df.data) # print model summary fit %&gt;% summary() ## ## Call: ## lm(formula = y ~ 1 + x1 + x2, data = df.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9290 -0.3084 -0.0716 0.2676 1.1659 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.9953 0.1395 7.133 1.77e-10 *** ## x1 0.4654 0.1817 2.561 0.01198 * ## x2 0.5072 0.1789 2.835 0.00558 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4838 on 97 degrees of freedom ## Multiple R-squared: 0.1327, Adjusted R-squared: 0.1149 ## F-statistic: 7.424 on 2 and 97 DF, p-value: 0.001 Let’s visualize the data: df.data %&gt;% ggplot(data = ., mapping = aes(x = x1, y = y, color = x2)) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + geom_point() 17.2.1.1 Global F-test The global F-test which is shown by the F-statistic at the bottom of the summary() output compares the full model with a model that only has an intercept. So, to use our model comparison approach, we would compare the following two models: # fit models model_compact = lm(formula = y ~ 1, data = df.data) model_augmented = lm(formula = y ~ 1 + x1 + x2, data = df.data) # compare models using the F-test anova(model_compact, model_augmented) ## Analysis of Variance Table ## ## Model 1: y ~ 1 ## Model 2: y ~ 1 + x1 + x2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 26.175 ## 2 97 22.700 2 3.4746 7.4236 0.001 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note how the result of the F-test using the anova() function which compares the two models is identical to the F-statistic reported at the end of the summary function. 17.2.1.2 Test for individual predictors To test for individual predictors in the model, we compare two models, a compact model without that predictor, and an augmented model with that predictor. Let’s test the significance of x1. # fit models model_compact = lm(formula = y ~ 1 + x2, data = df.data) model_augmented = lm(formula = y ~ 1 + x1 + x2, data = df.data) # compare models using the F-test anova(model_compact, model_augmented) ## Analysis of Variance Table ## ## Model 1: y ~ 1 + x2 ## Model 2: y ~ 1 + x1 + x2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 24.235 ## 2 97 22.700 1 1.5347 6.558 0.01198 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note how the p-value that we get from the F-test is equivalent to the one that we get from the t-test reported in the summary() function. The F-test statistic (in the anova() result) and the t-value (in the summary() of the linear model) are deterministically related. In fact, the relationship is just: \\[ t = \\sqrt{F} \\] Let’s check that that’s correct: # get the t-value from the fitted lm t_value = fit %&gt;% tidy() %&gt;% filter(term == &quot;x1&quot;) %&gt;% pull(statistic) # get the F-value from comparing the compact model (without x1) with the # augmented model (with x1) f_value = anova(model_compact, model_augmented) %&gt;% tidy() %&gt;% pull(statistic) %&gt;% .[2] # t-value print(str_c(&quot;t_value: &quot;, t_value)) ## [1] &quot;t_value: 2.56085255904998&quot; # square root of f_value print(str_c(&quot;sqrt of f_value: &quot;, sqrt(f_value))) ## [1] &quot;sqrt of f_value: 2.56085255904998&quot; Yip, they are the same. 17.3 Dependence Let’s generate a data set in which two observations from the same participants are dependent, and then let’s also shuffle this data set to see whether taking into account the dependence in the data matters. # make example reproducible set.seed(1) df.dependence = data_frame( participant = 1:20, condition1 = rnorm(20), condition2 = condition1 + rnorm(20, mean = 0.2, sd = 0.1) ) %&gt;% mutate(condition2shuffled = sample(condition2)) # shuffles the condition label ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. Let’s visualize the original and shuffled data set: df.plot = df.dependence %&gt;% gather(&quot;condition&quot;, &quot;value&quot;, -participant) %&gt;% mutate(condition = str_replace(condition, &quot;condition&quot;, &quot;&quot;)) p1 = ggplot(data = df.plot %&gt;% filter(condition != &quot;2shuffled&quot;), mapping = aes(x = condition, y = value)) + geom_line(aes(group = participant), alpha = 0.3) + geom_point() + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, fill = &quot;red&quot;, size = 4) + labs(title = &quot;original&quot;, tag = &quot;a)&quot;) p2 = ggplot(data = df.plot %&gt;% filter(condition != &quot;2&quot;), mapping = aes(x = condition, y = value)) + geom_line(aes(group = participant), alpha = 0.3) + geom_point() + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, fill = &quot;red&quot;, size = 4) + labs(title = &quot;shuffled&quot;, tag = &quot;b)&quot;) p1 + p2 Let’s save the two original and shuffled data set as two separate data sets. # separate the data sets df.original = df.dependence %&gt;% gather(&quot;condition&quot;, &quot;value&quot;, -participant) %&gt;% mutate(condition = str_replace(condition, &quot;condition&quot;, &quot;&quot;)) %&gt;% filter(condition != &quot;2shuffled&quot;) df.shuffled = df.dependence %&gt;% gather(&quot;condition&quot;, &quot;value&quot;, -participant) %&gt;% mutate(condition = str_replace(condition, &quot;condition&quot;, &quot;&quot;)) %&gt;% filter(condition != &quot;2&quot;) Let’s run a linear model, and independent samples t-test on the original data set. # linear model (assuming independent samples) lm(formula = value ~ condition, data = df.original) %&gt;% summary() ## ## Call: ## lm(formula = value ~ condition, data = df.original) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4100 -0.5530 0.1945 0.5685 1.4578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1905 0.2025 0.941 0.353 ## condition2 0.1994 0.2864 0.696 0.491 ## ## Residual standard error: 0.9058 on 38 degrees of freedom ## Multiple R-squared: 0.01259, Adjusted R-squared: -0.0134 ## F-statistic: 0.4843 on 1 and 38 DF, p-value: 0.4907 t.test(df.original$value[df.original$condition == &quot;1&quot;], df.original$value[df.original$condition == &quot;2&quot;], alternative = &quot;two.sided&quot;, paired = F ) ## ## Welch Two Sample t-test ## ## data: df.original$value[df.original$condition == &quot;1&quot;] and df.original$value[df.original$condition == &quot;2&quot;] ## t = -0.69595, df = 37.99, p-value = 0.4907 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.7792396 0.3805339 ## sample estimates: ## mean of x mean of y ## 0.1905239 0.3898767 The mean difference between the conditions is extremely small, and non-significant (if we ignore the dependence in the data). Let’s fit a linear mixed effects model with a random intercept for each participant: # fit a linear mixed effects model lmer(formula = value ~ condition + (1 | participant), data = df.original) %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ condition + (1 | participant) ## Data: df.original ## ## REML criterion at convergence: 17.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.55996 -0.36399 -0.03341 0.34400 1.65823 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 0.816722 0.90373 ## Residual 0.003796 0.06161 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.19052 0.20255 0.941 ## condition2 0.19935 0.01948 10.231 ## ## Correlation of Fixed Effects: ## (Intr) ## condition2 -0.048 To test for whether condition is a significant predictor, we need to use our model comparison approach: # fit models fit.compact = lmer(formula = value ~ 1 + (1 | participant), data = df.original) fit.augmented = lmer(formula = value ~ condition + (1 | participant), data = df.original) # compare via Chisq-test anova(fit.compact, fit.augmented) ## refitting model(s) with ML (instead of REML) ## Data: df.original ## Models: ## fit.compact: value ~ 1 + (1 | participant) ## fit.augmented: value ~ condition + (1 | participant) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## fit.compact 3 53.315 58.382 -23.6575 47.315 ## fit.augmented 4 17.849 24.605 -4.9247 9.849 37.466 1 9.304e-10 ## ## fit.compact ## fit.augmented *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This result is identical to running a paired samples t-test: t.test(df.original$value[df.original$condition == &quot;1&quot;], df.original$value[df.original$condition == &quot;2&quot;], alternative = &quot;two.sided&quot;, paired = T) ## ## Paired t-test ## ## data: df.original$value[df.original$condition == &quot;1&quot;] and df.original$value[df.original$condition == &quot;2&quot;] ## t = -10.231, df = 19, p-value = 3.636e-09 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2401340 -0.1585717 ## sample estimates: ## mean of the differences ## -0.1993528 But, unlike in the paired samples t-test, the linear mixed effects model explicitly models the variation between participants, and it’s a much more flexible approach for modeling dependence in data. Let’s fit a linear model and a linear mixed effects model to the original (non-shuffled) data. # model assuming independence fit.independent = lm(formula = value ~ 1 + condition, data = df.original) # model assuming dependence fit.dependent = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.original) Let’s visualize the linear model’s predictions: # plot with predictions by fit.independent fit.independent %&gt;% augment() %&gt;% bind_cols(df.original %&gt;% select(participant)) %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) And this is what the residuals look like: # make example reproducible set.seed(1) fit.independent %&gt;% augment() %&gt;% bind_cols(df.original %&gt;% select(participant)) %&gt;% clean_names() %&gt;% mutate(index = as.numeric(condition), index = index + runif(n(), min = -0.3, max = 0.3)) %&gt;% ggplot(data = ., mapping = aes(x = index, y = value, group = participant, color = condition)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, formula = &quot;y ~ 1&quot;, aes(group = condition)) + geom_segment(aes(xend = index, yend = fitted), alpha = 0.5) + scale_color_brewer(palette = &quot;Set1&quot;) + scale_x_continuous(breaks = 1:2, labels = 1:2) + labs(x = &quot;condition&quot;) + theme(legend.position = &quot;none&quot;) It’s clear from this residual plot, that fitting two separate lines (or points) is not much better than just fitting one line (or point). Let’s visualize the predictions of the linear mixed effects model: # plot with predictions by fit.independent fit.dependent %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) Let’s compare the residuals of the linear model with that of the linear mixed effects model: # linear model p1 = fit.independent %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = fitted, y = resid)) + geom_point() + coord_cartesian(ylim = c(-2.5, 2.5)) # linear mixed effects model p2 = fit.dependent %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = fitted, y = resid)) + geom_point() + coord_cartesian(ylim = c(-2.5, 2.5)) p1 + p2 The residuals of the linear mixed effects model are much smaller. Let’s test whether taking the individual variation into account is worth it (statistically speaking). # fit models (without and with dependence) fit.compact = lm(formula = value ~ 1 + condition, data = df.original) fit.augmented = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.original) # compare models # note: the lmer model has to be supplied first anova(fit.augmented, fit.compact) ## refitting model(s) with ML (instead of REML) ## Data: df.original ## Models: ## fit.compact: value ~ 1 + condition ## fit.augmented: value ~ 1 + condition + (1 | participant) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## fit.compact 3 109.551 114.617 -51.775 103.551 ## fit.augmented 4 17.849 24.605 -4.925 9.849 93.701 1 &lt; 2.2e-16 ## ## fit.compact ## fit.augmented *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes, the likelihood of the data given the linear mixed effects model is significantly higher compared to its likelihood given the linear model. 17.4 Additional resources 17.4.1 Readings Linear mixed effects models tutorial by Bodo Winter "],
["linear-mixed-effects-models-2.html", "Chapter 18 Linear mixed effects models 2 18.1 Load packages and set plotting theme 18.2 Things that came up in class 18.3 Simulating a linear mixed effects model 18.4 The effect of outliers 18.5 Different slopes 18.6 Simpson’s paradox 18.7 Additional resources", " Chapter 18 Linear mixed effects models 2 18.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;patchwork&quot;) # for making figure panels library(&quot;lme4&quot;) # for linear mixed effects models library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 18.2 Things that came up in class 18.2.1 Difference between replicate() and map() replicate() comes with base R whereas map() is part of the tidyverse. map() can do everything that replicate() can do and more. However, if you just want to run the same function (without changing the parameters) multiple times, you might as well use replicate(). Here are some examples for what you can do with replicate() and map(). # draw from a normal distribution and take mean fun.normal_means = function(n, mean, sd){ mean(rnorm(n = n, mean = mean, sd = sd)) } # execute the function 4 times replicate(n = 4, fun.normal_means(n = 20, mean = 1, sd = 0.5)) ## [1] 0.8218493 0.9400450 1.0408978 1.1576367 # same same but different map_dbl(.x = c(20, 20, 20, 20), ~ fun.normal_means(n = .x, mean = 1, sd = 0.5)) ## [1] 1.0171130 1.1284499 0.9981453 1.1829231 # and more flexible map_dbl(.x = c(1, 1, 10, 10), ~ fun.normal_means(n = 20, mean = .x, sd = 0.5)) ## [1] 1.126746 1.055712 9.907735 9.909135 18.3 Simulating a linear mixed effects model To generate some data for a linear mixed effects model with random intercepts, we do pretty much what we are used to doing when we generated data for a linear model. However, this time, we have an additional parameter that captures the variance in the intercepts between participants. So, we draw a separate (offset from the global) intercept for each participant from this distribution. # make example reproducible set.seed(1) # parameters sample_size = 100 b0 = 1 b1 = 2 sd_residual = 1 sd_participant = 0.5 # randomly draw intercepts for each participant intercepts = rnorm(sample_size, sd = sd_participant) # generate the data df.mixed = tibble( condition = rep(0:1, each = sample_size), participant = rep(1:sample_size, 2)) %&gt;% group_by(condition) %&gt;% mutate(value = b0 + b1 * condition + intercepts + rnorm(n(), sd = sd_residual)) %&gt;% ungroup %&gt;% mutate(condition = as.factor(condition), participant = as.factor(participant)) Let’s fit a model to this data now and take a look at the summary output: # fit model fit.mixed = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.mixed) fit.mixed %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + condition + (1 | participant) ## Data: df.mixed ## ## REML criterion at convergence: 606 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.53710 -0.62295 -0.04364 0.67035 2.19899 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 0.1607 0.4009 ## Residual 1.0427 1.0211 ## Number of obs: 200, groups: participant, 100 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.0166 0.1097 9.267 ## condition1 2.0675 0.1444 14.317 ## ## Correlation of Fixed Effects: ## (Intr) ## condition1 -0.658 Let’s visualize the model’s predictions: fit.mixed %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) Let’s simulate some data from this fitted model: # simulated data fit.mixed %&gt;% simulate() %&gt;% bind_cols(df.mixed) %&gt;% ggplot(data = ., mapping = aes(x = condition, y = sim_1, group = participant)) + geom_line(alpha = 0.5) + geom_point(alpha = 0.5) Even though we only fitted random intercepts in this model, when we simulate from the model, we get different slopes since, when simulating new data, the model takes our uncertainty in the residuals into account as well. Let’s see whether fitting random intercepts was worth it in this case: # using chisq test fit.compact = lm(formula = value ~ 1 + condition, data = df.mixed) fit.augmented = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.mixed) anova(fit.augmented, fit.compact) ## refitting model(s) with ML (instead of REML) ## Data: df.mixed ## Models: ## fit.compact: value ~ 1 + condition ## fit.augmented: value ~ 1 + condition + (1 | participant) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## fit.compact 3 608.6 618.49 -301.3 602.6 ## fit.augmented 4 608.8 621.99 -300.4 600.8 1.7999 1 0.1797 Nope, it’s not worth it in this case. That said, even though having random intercepts does not increase the likelihood of the data given the model significantly, we should still include random intercepts to capture the dependence in the data. 18.4 The effect of outliers Let’s take 20 participants from our df.mixed data set, and make one of the participants be an outlier: # let&#39;s make one outlier df.outlier = df.mixed %&gt;% mutate(participant = participant %&gt;% as.character() %&gt;% as.numeric()) %&gt;% filter(participant &lt;= 20) %&gt;% mutate(value = ifelse(participant == 20, value + 30, value), participant = as.factor(participant)) Let’s fit the model and look at the summary: # fit model fit.outlier = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.outlier) fit.outlier %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + condition + (1 | participant) ## Data: df.outlier ## ## REML criterion at convergence: 192 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.44598 -0.48367 0.03043 0.44689 1.41232 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 45.1359 6.7183 ## Residual 0.6738 0.8209 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 2.7091 1.5134 1.790 ## condition1 2.1512 0.2596 8.287 ## ## Correlation of Fixed Effects: ## (Intr) ## condition1 -0.086 The variance for the participants’ intercepts has increased dramatically! Let’s visualize the data together with the model’s predictions: fit.outlier %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) The model is still able to capture the participants quite well. But note what its simulated data looks like now: # simulated data from lmer with outlier fit.outlier %&gt;% simulate() %&gt;% bind_cols(df.outlier) %&gt;% ggplot(data = ., mapping = aes(x = condition, y = sim_1, group = participant)) + geom_line(alpha = 0.5) + geom_point(alpha = 0.5) The simulated data doesn’t look like our original data. This is because one normal distribution is used to model the variance in the intercepts between participants. 18.5 Different slopes Let’s generate data where the effect of condition is different for participants: # make example reproducible set.seed(1) tmp = rnorm(n = 20) df.slopes = tibble( condition = rep(1:2, each = 20), participant = rep(1:20, 2), value = ifelse(condition == 1, tmp, mean(tmp) + rnorm(n = 20, sd = 0.3)) # regression to the mean ) %&gt;% mutate(condition = as.factor(condition), participant = as.factor(participant)) Let’s fit a model with random intercepts. fit.slopes = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.slopes) ## boundary (singular) fit: see ?isSingular fit.slopes %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + condition + (1 | participant) ## Data: df.slopes ## ## REML criterion at convergence: 83.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.5808 -0.3184 0.0130 0.4551 2.0913 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 0.0000 0.0000 ## Residual 0.4512 0.6717 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.190524 0.150197 1.268 ## condition2 -0.001941 0.212411 -0.009 ## ## Correlation of Fixed Effects: ## (Intr) ## condition2 -0.707 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Note how the summary says “singular fit”, and how the variance for random intercepts is 0. Here, fitting random intercepts did not help the model fit at all, so the lmer gave up … How about fitting random slopes? # fit model lmer(formula = value ~ 1 + condition + (1 + condition | participant), data = df.slopes) This won’t work because the model has more parameters than there are data points. To fit random slopes, we need more than 2 observations per participants. 18.6 Simpson’s paradox Taking dependence in the data into account is extremely important. The Simpson’s paradox is an instructive example for what can go wrong when we ignore the dependence in the data. Let’s start by simulating some data to demonstrate the paradox. # make example reproducible set.seed(2) n_participants = 20 n_observations = 10 slope = -10 sd_error = 0.4 sd_participant = 5 intercept = rnorm(n_participants, sd = sd_participant) %&gt;% sort() df.simpson = tibble(x = runif(n_participants * n_observations, min = 0, max = 1)) %&gt;% arrange(x) %&gt;% mutate(intercept = rep(intercept, each = n_observations), y = intercept + x * slope + rnorm(n(), sd = sd_error), participant = factor(intercept, labels = 1:n_participants)) Let’s visualize the overall relationship between x and y with a simple linear model. # overall effect ggplot(data = df.simpson, mapping = aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) As we see, overall, there is a positive relationship between x and y. lm(formula = y ~ x, data = df.simpson) %&gt;% summary() ## ## Call: ## lm(formula = y ~ x, data = df.simpson) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8731 -0.6362 0.2272 1.0051 2.6410 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.1151 0.2107 -33.76 &lt;2e-16 *** ## x 6.3671 0.3631 17.54 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.55 on 198 degrees of freedom ## Multiple R-squared: 0.6083, Adjusted R-squared: 0.6064 ## F-statistic: 307.5 on 1 and 198 DF, p-value: &lt; 2.2e-16 And this relationship is significant. Let’s take another look at the data use different colors for the different participants. # effect by participant ggplot(data = df.simpson, mapping = aes(x = x, y = y, color = participant)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + theme(legend.position = &quot;none&quot;) And let’s fit a different regression for each participant: # effect by participant ggplot(data = df.simpson, mapping = aes(x = x, y = y, color = participant, group = participant)) + geom_point() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + theme(legend.position = &quot;none&quot;) What this plot shows, is that for almost all individual participants, the relationship between x and y is negative. The different participants where along the x spectrum they are. Let’s fit a linear mixed effects model with random intercepts: fit.lmer = lmer(formula = y ~ 1 + x + (1 | participant), data = df.simpson) fit.lmer %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: y ~ 1 + x + (1 | participant) ## Data: df.simpson ## ## REML criterion at convergence: 345.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.43394 -0.59687 0.04493 0.62694 2.68828 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 21.4898 4.6357 ## Residual 0.1661 0.4075 ## Number of obs: 200, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -0.1577 1.3230 -0.119 ## x -7.6678 1.6572 -4.627 ## ## Correlation of Fixed Effects: ## (Intr) ## x -0.621 As we can see, the fixed effect for x is now negative! fit.lmer %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., aes(x = x, y = y, group = participant, color = participant)) + geom_point() + geom_line(aes(y = fitted), size = 1, color = &quot;black&quot;) + theme(legend.position = &quot;none&quot;) Lesson learned: taking dependence into account is critical for drawing correct inferences! 18.7 Additional resources 18.7.1 Readings Linear mixed effects models tutorial by Bodo Winter Simpson’s paradox Tutorial on pooling "],
["linear-mixed-effects-models-3.html", "Chapter 19 Linear mixed effects models 3 19.1 Load packages and set plotting theme 19.2 Load data set 19.3 Things that came up in class 19.4 Pooling and shrinkage 19.5 Bootstrapping 19.6 Getting p-values 19.7 Understanding the lmer() syntax", " Chapter 19 Linear mixed effects models 3 19.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;patchwork&quot;) # for making figure panels library(&quot;lme4&quot;) # for linear mixed effects models library(&quot;modelr&quot;) # for bootstrapping library(&quot;boot&quot;) # also for bootstrapping library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 19.2 Load data set # load sleepstudy data set df.sleep = sleepstudy %&gt;% as_tibble() %&gt;% clean_names() %&gt;% mutate(subject = as.character(subject)) %&gt;% select(subject, days, reaction) # add two fake participants (with missing data) df.sleep = df.sleep %&gt;% bind_rows( tibble(subject = &quot;374&quot;, days = 0:1, reaction = c(286, 288)), tibble(subject = &quot;373&quot;, days = 0, reaction = 245) ) 19.3 Things that came up in class 19.3.1 One-tailed vs. two-tailed tests 19.3.1.1 t distribution Some code to draw a t-distribution: tibble(x = c(-4, 4)) %&gt;% ggplot(data = ., mapping = aes(x = x)) + stat_function(fun = &quot;dt&quot;, args = list(df = 20), size = 1, geom = &quot;area&quot;, fill = &quot;red&quot;, # xlim = c(qt(0.95, df = 20), qt(0.999, df = 20))) + # xlim = c(qt(0.001, df = 20), qt(0.05, df = 20))) + xlim = c(qt(0.001, df = 20), qt(0.025, df = 20))) + stat_function(fun = &quot;dt&quot;, args = list(df = 20), size = 1, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(qt(0.975, df = 20), qt(0.999, df = 20))) + stat_function(fun = &quot;dt&quot;, args = list(df = 20), size = 1) + coord_cartesian(expand = F) 19.3.1.2 F distribution Some code to draw an F-distribution tibble(x = c(0, 5)) %&gt;% ggplot(data = ., mapping = aes(x = x)) + stat_function(fun = &quot;df&quot;, args = list(df1 = 100, df2 = 10), size = 1, geom = &quot;area&quot;, fill = &quot;red&quot;, xlim = c(qf(0.95, df1 = 100, df2 = 10), qf(0.999, df1 = 100, df2 = 10))) + stat_function(fun = &quot;df&quot;, args = list(df1 = 100, df2 = 10), size = 1) + coord_cartesian(expand = F) 19.3.2 Mixtures of participants What if we have groups of participants who differ from each other? Let’s generate data for which this is the case. # make example reproducible set.seed(1) sample_size = 20 b0 = 1 b1 = 2 sd_residual = 0.5 sd_participant = 0.5 mean_group1 = 1 mean_group2 = 10 df.mixed = tibble( condition = rep(0:1, each = sample_size), participant = rep(1:sample_size, 2)) %&gt;% group_by(participant) %&gt;% mutate(group = sample(1:2, size = 1), intercept = ifelse(group == 1, rnorm(n(), mean = mean_group1, sd = sd_participant), rnorm(n(), mean = mean_group2, sd = sd_participant))) %&gt;% group_by(condition) %&gt;% mutate(value = b0 + b1 * condition + intercept + rnorm(n(), sd = sd_residual)) %&gt;% ungroup %&gt;% mutate(condition = as.factor(condition), participant = as.factor(participant)) 19.3.2.1 Ignoring mixture Let’ first fit a model that ignores the fact that there are two different groups of participatns. # fit model fit.mixed = lmer(formula = value ~ 1 + condition + (1 | participant), data = df.mixed) fit.mixed %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + condition + (1 | participant) ## Data: df.mixed ## ## REML criterion at convergence: 165.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6437 -0.4510 -0.0246 0.4987 1.5265 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 21.5142 4.6383 ## Residual 0.3521 0.5934 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 7.2229 1.0456 6.908 ## condition1 1.6652 0.1876 8.875 ## ## Correlation of Fixed Effects: ## (Intr) ## condition1 -0.090 Let’s look at the model’s predictions: fit.mixed %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) And let’s simulate some data from the fitted model: # simulated data fit.mixed %&gt;% simulate() %&gt;% bind_cols(df.mixed) %&gt;% ggplot(data = ., mapping = aes(x = condition, y = sim_1, group = participant)) + geom_line(alpha = 0.5) + geom_point(alpha = 0.5) As we can see, the simulated data doesn’t look like the data that was used to fit the model. 19.3.2.2 Modeling mixture Now, let’s fit a model that takes the differences between groups into account by adding a fixed effect for group. # fit model fit.grouped = lmer(formula = value ~ 1 + group + condition + (1 | participant), data = df.mixed) fit.grouped %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + group + condition + (1 | participant) ## Data: df.mixed ## ## REML criterion at convergence: 83.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.56168 -0.69876 0.05887 0.50419 2.30259 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 0.1147 0.3387 ## Residual 0.3521 0.5934 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -6.8299 0.4055 -16.842 ## group 9.0663 0.2424 37.409 ## condition1 1.6652 0.1876 8.875 ## ## Correlation of Fixed Effects: ## (Intr) group ## group -0.926 ## condition1 -0.231 0.000 Note how the variance of the random intercepts is much smaller now that we’ve taken the group structure in the data into account. Let’s visualize the model’s predictions: fit.grouped %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) And simulate some data from the model: # simulated data fit.grouped %&gt;% simulate() %&gt;% bind_cols(df.mixed) %&gt;% ggplot(data = ., mapping = aes(x = condition, y = sim_1, group = participant)) + geom_line(alpha = 0.5) + geom_point(alpha = 0.5) This time, the simulated data looks much more like the data that was used to fit the model. Yay! 19.3.2.3 Heterogeneity in variance The example above has shown that we can take overall differences between groups into account by adding a fixed effect. Can we also deal with heterogeneity in variance between groups? For example, what if the responses of one group exhibit much more variance than the responses of another group? Let’s first generate some data with heterogeneous variance: # make example reproducible set.seed(1) sample_size = 20 b0 = 1 b1 = 2 sd_residual = 0.5 mean_group1 = 1 sd_group1 = 1 mean_group2 = 30 sd_group2 = 10 df.variance = tibble( condition = rep(0:1, each = sample_size), participant = rep(1:sample_size, 2)) %&gt;% group_by(participant) %&gt;% mutate(group = sample(1:2, size = 1), intercept = ifelse(group == 1, rnorm(n(), mean = mean_group1, sd = sd_group1), rnorm(n(), mean = mean_group2, sd = sd_group2))) %&gt;% group_by(condition) %&gt;% mutate(value = b0 + b1 * condition + intercept + rnorm(n(), sd = sd_residual)) %&gt;% ungroup %&gt;% mutate(condition = as.factor(condition), participant = as.factor(participant)) Let’s fit the model: # fit model fit.variance = lmer(formula = value ~ 1 + group + condition + (1 | participant), data = df.variance) fit.variance %&gt;% summary() ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: value ~ 1 + group + condition + (1 | participant) ## Data: df.variance ## ## REML criterion at convergence: 250 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.70344 -0.21278 0.07355 0.43873 1.39493 ## ## Random effects: ## Groups Name Variance Std.Dev. ## participant (Intercept) 17.60 4.196 ## Residual 26.72 5.169 ## Number of obs: 40, groups: participant, 20 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) -26.5805 4.1525 -6.401 ## group 29.6200 2.5010 11.843 ## condition1 0.1853 1.6346 0.113 ## ## Correlation of Fixed Effects: ## (Intr) group ## group -0.934 ## condition1 -0.197 0.000 Look at the data and model predictions: fit.variance %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = condition, y = value, group = participant)) + geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + geom_point(aes(y = fitted), color = &quot;red&quot;) + geom_line(aes(y = fitted), color = &quot;red&quot;) And the simulated data: # simulated data fit.mixed %&gt;% simulate() %&gt;% bind_cols(df.mixed) %&gt;% ggplot(data = ., mapping = aes(x = condition, y = sim_1, group = participant)) + geom_line(alpha = 0.5) + geom_point(alpha = 0.5) The lmer() fails here. It uses one normal distribution to model the variance between participants. It cannot account for the fact that the answers of one groups of participants vary more than the answers from another groups of participants. Again, the simulated data doesn’t look the original data, even though we did take the grouping into account. 19.4 Pooling and shrinkage Let’s illustrate the concept of pooling and shrinkage via the sleep data set that comes with the lmer package. We’ve already loaded the data set into our environment as df.sleep. Let’s start by visualizing the data # visualize the data ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) The plot shows the effect of the number of days of sleep deprivation on the average reaction time (presumably in an experiment). Note that for participant 373 and 374 we only have one and two data points respectively. 19.4.1 Complete pooling Let’s first fit a model the simply combines all the data points. This model ignores the dependence structure in the data (i.e. the fact that we have repeated observations from the same participants). fit.complete = lm(formula = reaction ~ days, data = df.sleep) fit.params = tidy(fit.complete) fit.complete %&gt;% summary() ## ## Call: ## lm(formula = reaction ~ days, data = df.sleep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -110.646 -27.951 1.829 26.388 139.875 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 252.321 6.406 39.389 &lt; 2e-16 *** ## days 10.328 1.210 8.537 5.48e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47.43 on 181 degrees of freedom ## Multiple R-squared: 0.2871, Adjusted R-squared: 0.2831 ## F-statistic: 72.88 on 1 and 181 DF, p-value: 5.484e-15 And let’s visualize the predictions of this model. # visualization (aggregate) ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_abline(intercept = fit.params$estimate[1], slope = fit.params$estimate[2], color = &quot;blue&quot;) + geom_point() + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) And here is what the model’s predictions look like separated by participant. # visualization (separate participants) ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_abline(intercept = fit.params$estimate[1], slope = fit.params$estimate[2], color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) The model predicts the same relationship between sleep deprivation and reaction time for each participant (not surprising since we didn’t even tell the model that this data is based on different participants). 19.4.2 No pooling We could also fit separate regressions for each participant. Let’s do that. # fit regressions and extract parameter estimates df.no_pooling = df.sleep %&gt;% group_by(subject) %&gt;% nest(days, reaction) %&gt;% mutate(fit = map(data, ~ lm(reaction ~ days, data = .)), params = map(fit, tidy)) %&gt;% unnest(params) %&gt;% select(subject, term, estimate) %&gt;% complete(subject, term, fill = list(estimate = 0)) %&gt;% spread(term, estimate) %&gt;% clean_names() And let’s visualize what the predictions of these separate regressions would look like: ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_abline(data = df.no_pooling %&gt;% filter(subject != 373), aes(intercept = intercept, slope = days), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) When we fit separate regression, no information is shared between participants. 19.4.3 Partial pooling By usign linear mixed effects models, we are partially pooling information. That is, the estimates for one participant are influenced by the rest of the participants. We’ll fit a number of mixed effects models that differ in their random effects structure. 19.4.3.1 Random intercept and random slope This model allows for random differences in the intercepts and slopes between subjects (and also models the correlation between intercepts and slopes). Let’s fit the model fit.random_intercept_slope = lmer(formula = reaction ~ 1 + days + (1 + days | subject), data = df.sleep) and take a look at the model’s predictions: fit.random_intercept_slope %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = days, y = reaction)) + geom_line(aes(y = fitted), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? As we can see, the lines for each participant are different. We’ve allowed for the intercept as well as the relationship between sleep deprivation and reaction time to be different between participants. 19.4.3.2 Only random intercepts Let’s fit a model that only allows for the intercepts to vary between participants. fit.random_intercept = lmer(formula = reaction ~ 1 + days + (1 | subject), data = df.sleep) And let’s visualize what these predictions look like: fit.random_intercept %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = days, y = reaction)) + geom_line(aes(y = fitted), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? Now, all the lines are parallel but the intercept differs between participants. 19.4.3.3 Only random slopes Finally, let’s compare a model that only allows for the slopes to differ but not the intercepts. fit.random_slope = lmer(formula = reaction ~ 1 + days + (0 + days | subject), data = df.sleep) And let’s visualize the model fit: fit.random_slope %&gt;% augment() %&gt;% clean_names() %&gt;% ggplot(data = ., mapping = aes(x = days, y = reaction)) + geom_line(aes(y = fitted), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) ## geom_path: Each group consists of only one observation. Do you need to ## adjust the group aesthetic? Here, all the lines have the same starting point (i.e. the same intercept) but the slopes are different. 19.4.4 Compare results Let’s compare the results of the different methods – complete pooling, no pooling, and partial pooling (with random intercepts and slopes). # complete pooling fit.complete_pooling = lm(formula = reaction ~ days, data = df.sleep) df.complete_pooling = fit.complete_pooling %&gt;% augment() %&gt;% bind_rows( fit.complete_pooling %&gt;% augment(newdata = tibble(subject = c(&quot;373&quot;, &quot;374&quot;), days = rep(10, 2))) ) %&gt;% clean_names() %&gt;% select(reaction, days, complete_pooling = fitted) # no pooling df.no_pooling = df.sleep %&gt;% group_by(subject) %&gt;% nest(days, reaction) %&gt;% mutate(fit = map(data, ~ lm(reaction ~ days, data = .)), augment = map(fit, augment)) %&gt;% unnest(augment) %&gt;% clean_names() %&gt;% select(subject, reaction, days, no_pooling = fitted) # partial pooling fit.lmer = lmer(formula = reaction ~ 1 + days + (1 + days | subject), data = df.sleep) df.partial_pooling = fit.lmer %&gt;% augment() %&gt;% bind_rows( fit.lmer %&gt;% augment(newdata = tibble(subject = c(&quot;373&quot;, &quot;374&quot;), days = rep(10, 2))) ) %&gt;% clean_names() %&gt;% select(subject, reaction, days, partial_pooling = fitted) # combine results df.pooling = df.partial_pooling %&gt;% left_join(df.complete_pooling) %&gt;% left_join(df.no_pooling) Let’s compare the predictions of the different models visually: ggplot(data = df.pooling, mapping = aes(x = days, y = reaction)) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;orange&quot;, fullrange = T) + geom_line(aes(y = complete_pooling), color = &quot;green&quot;) + geom_line(aes(y = partial_pooling), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject, ncol = 5) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) ## Warning: Removed 4 rows containing non-finite values (stat_smooth). ## Warning: Removed 4 rows containing missing values (geom_point). To better see the differences between the approaches, let’s focus on the predictions for the participants with incomplete data: # subselection ggplot(data = df.pooling %&gt;% filter(subject %in% c(&quot;373&quot;, &quot;374&quot;)), mapping = aes(x = days, y = reaction)) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;orange&quot;, fullrange = T) + geom_line(aes(y = complete_pooling), color = &quot;green&quot;) + geom_line(aes(y = partial_pooling), color = &quot;blue&quot;) + geom_point() + facet_wrap(~subject) + labs(x = &quot;Days of sleep deprivation&quot;, y = &quot;Average reaction time (ms)&quot;) + scale_x_continuous(breaks = 0:4 * 2) + theme(strip.text = element_text(size = 12), axis.text.y = element_text(size = 12)) ## Warning: Removed 4 rows containing non-finite values (stat_smooth). ## Warning: Removed 4 rows containing missing values (geom_point). 19.4.5 Coefficients One good way to get a sense for what the different models are doing is by taking a look at the coefficients: fit.complete_pooling %&gt;% coef() ## (Intercept) days ## 252.32070 10.32766 fit.random_intercept %&gt;% coef() ## $subject ## (Intercept) days ## 308 292.2749 10.43191 ## 309 174.0559 10.43191 ## 310 188.7454 10.43191 ## 330 256.0247 10.43191 ## 331 261.8141 10.43191 ## 332 259.8262 10.43191 ## 333 268.0765 10.43191 ## 334 248.6471 10.43191 ## 335 206.5096 10.43191 ## 337 323.5643 10.43191 ## 349 230.5114 10.43191 ## 350 265.6957 10.43191 ## 351 243.7988 10.43191 ## 352 287.8850 10.43191 ## 369 258.6454 10.43191 ## 370 245.2931 10.43191 ## 371 248.3508 10.43191 ## 372 269.6861 10.43191 ## 373 248.2086 10.43191 ## 374 273.9400 10.43191 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; fit.random_slope %&gt;% coef() ## $subject ## (Intercept) days ## 308 252.2965 19.9526801 ## 309 252.2965 -4.3719650 ## 310 252.2965 -0.9574726 ## 330 252.2965 8.9909957 ## 331 252.2965 10.5394285 ## 332 252.2965 11.3994289 ## 333 252.2965 12.6074020 ## 334 252.2965 10.3413879 ## 335 252.2965 -0.5722073 ## 337 252.2965 24.2246485 ## 349 252.2965 7.7702676 ## 350 252.2965 15.0661415 ## 351 252.2965 7.9675415 ## 352 252.2965 17.0002999 ## 369 252.2965 11.6982767 ## 370 252.2965 11.3939807 ## 371 252.2965 9.4535879 ## 372 252.2965 13.4569059 ## 373 252.2965 10.4142695 ## 374 252.2965 11.9097917 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; fit.random_intercept_slope %&gt;% coef() ## $subject ## (Intercept) days ## 308 253.9479 19.6264139 ## 309 211.7328 1.7319567 ## 310 213.1579 4.9061843 ## 330 275.1425 5.6435987 ## 331 273.7286 7.3862680 ## 332 260.6504 10.1632535 ## 333 268.3684 10.2245979 ## 334 244.5523 11.4837825 ## 335 251.3700 -0.3355554 ## 337 286.2321 19.1090061 ## 349 226.7662 11.5531963 ## 350 238.7807 17.0156766 ## 351 256.2344 7.4119501 ## 352 272.3512 13.9920698 ## 369 254.9484 11.2985741 ## 370 226.3701 15.2027922 ## 371 252.5051 9.4335432 ## 372 263.8916 11.7253342 ## 373 248.9752 10.3915245 ## 374 271.1451 11.0782697 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; 19.4.6 Shrinkage In mixed effects models, the variance of parameter estimates across participants shrinks compared to a no pooling model (where we fit a different regression to each participant). Expressed differently, individual parameter estimates are borrowing strength from the overall data set in mixed effects models. # get estimates from partial pooling model df.partial_pooling = fit.random_intercept_slope %&gt;% coef() %&gt;% .[[1]] %&gt;% rownames_to_column(&quot;subject&quot;) %&gt;% clean_names() # combine estimates from no pooling with partial pooling model df.plot = df.sleep %&gt;% group_by(subject) %&gt;% nest(days, reaction) %&gt;% mutate(fit = map(data, ~ lm(reaction ~ days, data = .)), tidy = map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% select(subject, term, estimate) %&gt;% spread(term, estimate) %&gt;% clean_names() %&gt;% mutate(method = &quot;no pooling&quot;) %&gt;% bind_rows(df.partial_pooling %&gt;% mutate(method = &quot;partial pooling&quot;)) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(subject, method)) %&gt;% mutate(index = factor(index, levels = c(&quot;intercept&quot;, &quot;days&quot;))) # visualize the results ggplot(data = df.plot, mapping = aes(x = value, group = method, fill = method)) + stat_density(position = &quot;identity&quot;, geom = &quot;area&quot;, color = &quot;black&quot;, alpha = 0.3) + facet_grid(cols = vars(index), scales = &quot;free&quot;) ## Warning: Removed 1 rows containing non-finite values (stat_density). 19.5 Bootstrapping Bootstrapping is a good way to estimate our uncertainty on the parameter estimates in the model. 19.5.1 Linear model Let’s briefly review how to do bootstrapping in a simple linear model. # fit model fit.lm = lm(formula = reaction ~ 1 + days, data = df.sleep) # coefficients fit.lm %&gt;% coef() ## (Intercept) days ## 252.32070 10.32766 # bootstrapping df.boot = df.sleep %&gt;% bootstrap(n = 100, id = &quot;id&quot;) %&gt;% mutate(fit = map(strap, ~ lm(formula = reaction ~ 1 + days, data = .)), tidy = map(fit, tidy)) %&gt;% unnest(tidy) %&gt;% select(id, term, estimate) %&gt;% spread(term, estimate) %&gt;% clean_names() Let’s illustrate the linear model with a confidence interval (making parametric assumptions using the t-distribution). ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_smooth(method = &quot;lm&quot;) + geom_point(alpha = 0.3) And let’s compare this with the different regression lines that we get out of our bootstrapped samples: ggplot(data = df.sleep, mapping = aes(x = days, y = reaction)) + geom_abline(data = df.boot, aes(intercept = intercept, slope = days, group = id), alpha = 0.1) + geom_point(alpha = 0.3) 19.5.1.1 bootmer() function For the linear mixed effects model, we can use the bootmer() function to do bootstrapping. # fit the model fit.lmer = lmer(formula = reaction ~ 1 + days + (1 + days | subject), data = df.sleep) # bootstrap parameter estimates boot.lmer = bootMer(fit.lmer, FUN = fixef, nsim = 100) # compute confidence interval boot.ci(boot.lmer, index = 2, type = &quot;perc&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot.lmer, type = &quot;perc&quot;, index = 2) ## ## Intervals : ## Level Percentile ## 95% ( 7.36, 13.52 ) ## Calculations and Intervals on Original Scale ## Some percentile intervals may be unstable # plot estimates boot.lmer$t %&gt;% as_tibble() %&gt;% clean_names() %&gt;% mutate(id = 1:n()) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, - id) %&gt;% ggplot(data = ., mapping = aes(x = value)) + geom_density() + facet_grid(cols = vars(index), scales = &quot;free&quot;) + coord_cartesian(expand = F) 19.6 Getting p-values We can use the “lmerTest” package to get p-values for the different fixed effects. lmerTest::lmer(formula = reaction ~ 1 + days + (1 + days | subject), data = df.sleep) %&gt;% summary() ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: reaction ~ 1 + days + (1 + days | subject) ## Data: df.sleep ## ## REML criterion at convergence: 1771.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9707 -0.4703 0.0276 0.4594 5.2009 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 582.73 24.140 ## days 35.03 5.919 0.07 ## Residual 649.36 25.483 ## Number of obs: 183, groups: subject, 20 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 252.543 6.433 19.294 39.256 &lt; 2e-16 *** ## days 10.452 1.542 17.163 6.778 3.06e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## days -0.137 19.7 Understanding the lmer() syntax Here is an overview of how to specify different kinds of linear mixed effects models. formula description dv ~ x1 + (1 | g) Random intercept for each level of g dv ~ x1 + (0 + x1 | g) Random slope for each level of g dv ~ x1 + (x1 | g) Correlated random slope and intercept for each level of g dv ~ x1 + (x1 || g) Uncorrelated random slope and intercept for each level of g dv ~ x1 + (1 | school) + (1 | teacher) Random intercept for each level of school and for each level of teacher (crossed) dv ~ x1 + (1 | school/teacher) Random intercept for each level of school and for each level of teacher in school (nested) "],
["generalized-linear-model.html", "Chapter 20 Generalized linear model 20.1 Load packages and set plotting theme 20.2 Load data set 20.3 Logistic regression 20.4 Simulate a logistic regression 20.5 Testing hypotheses 20.6 Logistic mixed effects model 20.7 Additional information", " Chapter 20 Generalized linear model 20.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;titanic&quot;) # titanic dataset library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;broom&quot;) # for tidying up linear models library(&quot;lme4&quot;) # for linear mixed effects models library(&quot;boot&quot;) # for bootstrapping (also has an inverse logit function) library(&quot;effects&quot;) # for showing effects in linear, generalized linear, and other models library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 20.2 Load data set df.titanic = titanic_train %&gt;% clean_names() %&gt;% mutate(sex = as.factor(sex)) Let’s take a quick look at the data: df.titanic %&gt;% glimpse() ## Observations: 891 ## Variables: 12 ## $ passenger_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, … ## $ pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, … ## $ name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Brad… ## $ sex &lt;fct&gt; male, female, female, female, male, male, male, mal… ## $ age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 3… ## $ sib_sp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, … ## $ parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, … ## $ ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;11380… ## $ fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 5… ## $ cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;… ## $ embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;… # Table of the first 10 entries df.titanic %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) passenger_id survived pclass name sex age sib_sp parch ticket fare cabin embarked 1 0 3 Braund, Mr. Owen Harris male 22 1 0 A/5 21171 7.25 S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 PC 17599 71.28 C85 C 3 1 3 Heikkinen, Miss. Laina female 26 0 0 STON/O2. 3101282 7.92 S 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 113803 53.10 C123 S 5 0 3 Allen, Mr. William Henry male 35 0 0 373450 8.05 S 6 0 3 Moran, Mr. James male NA 0 0 330877 8.46 Q 7 0 1 McCarthy, Mr. Timothy J male 54 0 0 17463 51.86 E46 S 8 0 3 Palsson, Master. Gosta Leonard male 2 3 1 349909 21.07 S 9 1 3 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) female 27 0 2 347742 11.13 S 10 1 2 Nasser, Mrs. Nicholas (Adele Achem) female 14 1 0 237736 30.07 C 20.3 Logistic regression Let’s see if we can predict whether or not a passenger survived based on the price of their ticket. Let’s run a simple regression first: # fit a linear model fit.lm = lm(formula = survived ~ 1 + fare, data = df.titanic) # summarize the results fit.lm %&gt;% summary() ## ## Call: ## lm(formula = survived ~ 1 + fare, data = df.titanic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9653 -0.3391 -0.3222 0.6044 0.6973 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.3026994 0.0187849 16.114 &lt; 2e-16 *** ## fare 0.0025195 0.0003174 7.939 6.12e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4705 on 889 degrees of freedom ## Multiple R-squared: 0.06621, Adjusted R-squared: 0.06516 ## F-statistic: 63.03 on 1 and 889 DF, p-value: 6.12e-15 Look’s like fare is a significant predictor of whether or not a person survived. Let’s visualize the model’s predictions: ggplot(data = df.titanic, mapping = aes(x = fare, y = survived)) + geom_smooth(method = &quot;lm&quot;) + geom_point() + labs(y = &quot;survived&quot;) This doesn’t look good! The model predicts intermediate values of survived (which doesn’t make sense given that a person either survived or didn’t survive). Furthermore, the model predicts values greater than 1 for fares greather than ~ 300. Let’s run a logistic regression instead. # fit a logistic regression fit.glm = glm(formula = survived ~ 1 + fare, family = &quot;binomial&quot;, data = df.titanic) fit.glm %&gt;% summary() ## ## Call: ## glm(formula = survived ~ 1 + fare, family = &quot;binomial&quot;, data = df.titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4906 -0.8878 -0.8531 1.3429 1.5942 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.941330 0.095129 -9.895 &lt; 2e-16 *** ## fare 0.015197 0.002232 6.810 9.79e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.7 on 890 degrees of freedom ## Residual deviance: 1117.6 on 889 degrees of freedom ## AIC: 1121.6 ## ## Number of Fisher Scoring iterations: 4 And let’s visualize the predictions of the logistic regression: ggplot(data = df.titanic, mapping = aes(x = fare, y = survived)) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) + geom_point() + labs(y = &quot;p(survived)&quot;) Much better! Note that we’ve changed the interpretation of our dependent variable. We are now predicting the probability that a person survived based on their fare. The model now only predicts values between 0 and 1. To achieve this, we apply a logit transform to the outcome variable like so: \\[ \\ln(\\frac{\\pi_i}{1-\\pi_i}) = b_0 + b_1 \\cdot X_i + e_i \\] where \\(\\pi_i\\) is the probability of passenger \\(i\\) having survived. Importantly, this affects our interpretation of the model parameters. They are now defined in log-odds, and can apply an inverse logit transformation to turn this back into a probability: With \\[ \\pi = P(Y = 1) \\] and the logit transformation \\[ \\ln(\\frac{\\pi}{1-\\pi}) = V, \\] where \\(V\\) is just a placeholder for our linear model formula, we can go back to \\(\\pi\\) through the inverse logit transformation like so: \\[ \\pi = \\frac{e^V}{1 + e^V} \\] In R, we can use log(x) to calculate the natural logarithm \\(\\ln(x)\\), and exp(x) to calculate e^x. 20.3.1 Interpreting the parameters fit.glm %&gt;% summary() ## ## Call: ## glm(formula = survived ~ 1 + fare, family = &quot;binomial&quot;, data = df.titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4906 -0.8878 -0.8531 1.3429 1.5942 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.941330 0.095129 -9.895 &lt; 2e-16 *** ## fare 0.015197 0.002232 6.810 9.79e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.7 on 890 degrees of freedom ## Residual deviance: 1117.6 on 889 degrees of freedom ## AIC: 1121.6 ## ## Number of Fisher Scoring iterations: 4 The estimate for the intercept and fare are in log-odds. We apply the inverse logit transformation to turn these into probabilities: fit.glm$coefficients[1] %&gt;% inv.logit() ## (Intercept) ## 0.2806318 Here, we see that the intercept is \\(p = 0.28\\). That is, the predicted chance of survival for someone who didn’t pay any fare at all is 28% according to the model. Interpreting the slope is a little more tricky. Let’s look at a situation first where we have a binary predictor. 20.3.1.1 Binary predictor Let’s see whether the probability of survival differed between male and female passengers. fit.glm2 = glm(formula = survived ~ sex, family = &quot;binomial&quot;, data = df.titanic) fit.glm2 %&gt;% summary() ## ## Call: ## glm(formula = survived ~ sex, family = &quot;binomial&quot;, data = df.titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6462 -0.6471 -0.6471 0.7725 1.8256 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.0566 0.1290 8.191 2.58e-16 *** ## sexmale -2.5137 0.1672 -15.036 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.7 on 890 degrees of freedom ## Residual deviance: 917.8 on 889 degrees of freedom ## AIC: 921.8 ## ## Number of Fisher Scoring iterations: 4 It looks like it did! Let’s visualize: df.titanic %&gt;% mutate(survived = factor(survived, labels = c(&quot;died&quot;, &quot;survived&quot;))) %&gt;% ggplot(data = ., mapping = aes(x = sex, fill = survived)) + geom_bar(position = &quot;fill&quot;, color = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + labs(x = &quot;&quot;, fill = &quot;&quot;, y = &quot;probability&quot;) And let’s interpret the parameters by applying the inverse logit transform. To get the prediction for female passengers we get \\[ \\widehat{\\ln(\\frac{\\pi_i}{1-\\pi_i})} = b_0 + b_1 \\cdot \\text{sex}_i = b_0 + b_1 \\cdot 0 = b_0 \\] since we dummy coded the predictor and female is our reference category. To get the predicted probability of surival for women we do the logit transform: \\[ \\pi = \\frac{e^{b_0}}{1 + e^{b_0}} \\] The predicted probability is: fit.glm2$coefficients[1] %&gt;% inv.logit() ## (Intercept) ## 0.7420382 To get the prediction for male passengers we have: \\[ \\widehat{\\ln(\\frac{\\pi_i}{1-\\pi_i})} = b_0 + b_1 \\cdot \\text{sex}_i = b_0 + b_1 \\cdot 1 = b_0 + b_1 \\] Applying the logit transform like so \\[ \\pi = \\frac{e^{b_0 + b_1}}{1 + e^{b_0 + b_1}} \\] The predicted probability of male passengers surviving is: sum(fit.glm2$coefficients) %&gt;% inv.logit() ## [1] 0.1889081 Here is the same information in a table: df.titanic %&gt;% count(sex, survived) %&gt;% mutate(p = n/sum(n)) %&gt;% group_by(sex) %&gt;% mutate(`p(survived|sex)` = p/sum(p)) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) sex survived n p p(survived|sex) female 0 81 0.09 0.26 female 1 233 0.26 0.74 male 0 468 0.53 0.81 male 1 109 0.12 0.19 20.3.1.2 Continuous predictor To interpret the predictions when a continuous predictor is invovled, it’s easiest to consider a few concrete cases. Here, I use the augment() function from the “broom” package to get the model’s predictions for some values of interest: fit.glm %&gt;% augment(newdata = tibble(fare = c(0, 10, 50, 100, 500))) %&gt;% clean_names() %&gt;% select(fare, prediction = fitted) %&gt;% mutate(`p(survival)` = prediction %&gt;% inv.logit()) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) fare prediction p(survival) 0 -0.94 0.28 10 -0.79 0.31 50 -0.18 0.45 100 0.58 0.64 500 6.66 1.00 20.3.1.3 Several predictors Let’s fit a logistic regression that predicts the probability of survival based both on the passenger’s sex and what fare they paid (allowing for an interaction of the two predictors): fit.glm = glm(formula = survived ~ 1 + sex * fare, family = &quot;binomial&quot;, data = df.titanic) fit.glm %&gt;% summary() ## ## Call: ## glm(formula = survived ~ 1 + sex * fare, family = &quot;binomial&quot;, ## data = df.titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6280 -0.6279 -0.5991 0.8172 1.9288 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.408428 0.189999 2.150 0.031584 * ## sexmale -2.099345 0.230291 -9.116 &lt; 2e-16 *** ## fare 0.019878 0.005372 3.701 0.000215 *** ## sexmale:fare -0.011617 0.005934 -1.958 0.050252 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.66 on 890 degrees of freedom ## Residual deviance: 879.85 on 887 degrees of freedom ## AIC: 887.85 ## ## Number of Fisher Scoring iterations: 5 And let’s visualize the result: df.titanic %&gt;% mutate(sex = as.factor(sex)) %&gt;% ggplot(data = ., mapping = aes(x = fare, y = survived, color = sex, group = sex)) + geom_point(alpha = 0.1, size = 2) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), alpha = 0.2, aes(fill = sex)) + scale_color_brewer(palette = &quot;Set1&quot;) We notice that there is one outlier who was male and paid a $500 fare (or maybe this is a mistake in the data entry?!). Let’s remove this outlier and see what happens: df.titanic %&gt;% filter(fare &lt; 500) %&gt;% mutate(sex = as.factor(sex)) %&gt;% ggplot(data = ., mapping = aes(x = fare, y = survived, color = sex, group = sex)) + geom_point(alpha = 0.1, size = 2) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), alpha = 0.2, aes(fill = sex)) + scale_color_brewer(palette = &quot;Set1&quot;) There is still a clear difference between female and male passengers, but the prediction for male passengers has changed a bit. Let’s look at a concrete example: # with the outlier: # predicted probability of survival for a male passenger who paid $200 for their fare inv.logit(fit.glm$coefficients[1] + fit.glm$coefficients[2] + fit.glm$coefficients[3] * 200 + fit.glm$coefficients[4] * 200) ## (Intercept) ## 0.4903402 # without the outlier: # predicted probability of survival for a male passenger who paid $200 for their fare fit.glm_no_outlier = glm(formula = survived ~ 1 + sex * fare, family = &quot;binomial&quot;, data = df.titanic %&gt;% filter(fare &lt; 500)) inv.logit(fit.glm_no_outlier$coefficients[1] + fit.glm_no_outlier$coefficients[2] + fit.glm_no_outlier$coefficients[3] * 200 + fit.glm_no_outlier$coefficients[4] * 200) ## (Intercept) ## 0.4658284 With the oulier removed, the predicted probability of survival for a male passenger who paid $200 decreases from 49% to 47%. 20.3.1.4 Using the “effects” package The “effects” package helps with the interpretation of the results. It applies the inverse logit transform for us, and shows the predictions for a range of cases. # show effects allEffects(mod = fit.glm, xlevels = list(fare = c(0, 100, 200, 300, 400, 500))) ## model: survived ~ 1 + sex * fare ## ## sex*fare effect ## fare ## sex 0 100 200 300 400 500 ## female 0.6007108 0.9165428 0.9876799 0.9982941 0.9997660 0.9999679 ## male 0.1556552 0.2963415 0.4903402 0.6872927 0.8339147 0.9198098 I’ve used the xlevels argument to specify for what values of the predictor fare, I’d like get the predicted values. 20.4 Simulate a logistic regression As always, to better understand a statistical modeling procedure, it’s helpful to simulate data from the assumed data-generating process, fit the model, and see whether we can reconstruct the parameters. # make example reproducible set.seed(1) # set parameters sample_size = 1000 b0 = 0 b1 = 1 # b1 = 8 # generate data df.data = tibble( x = rnorm(n = sample_size), y = b0 + b1 * x, p = inv.logit(y)) %&gt;% mutate(response = rbinom(n(), size = 1, p = p)) # fit model fit = glm(formula = response ~ 1 + x, family = &quot;binomial&quot;, data = df.data) # model summary fit %&gt;% summary() ## ## Call: ## glm(formula = response ~ 1 + x, family = &quot;binomial&quot;, data = df.data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1137 -1.0118 -0.4591 1.0287 2.2591 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.06214 0.06918 -0.898 0.369 ## x 0.92905 0.07937 11.705 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1385.4 on 999 degrees of freedom ## Residual deviance: 1209.6 on 998 degrees of freedom ## AIC: 1213.6 ## ## Number of Fisher Scoring iterations: 3 Nice! The inferred estimates are very close to the parameter values we used to simulate the data. Let’s visualize the result: ggplot(data = df.data, mapping = aes(x = x, y = response)) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) + geom_point(alpha = 0.1) + labs(y = &quot;p(response)&quot;) 20.4.0.1 Calculate the model’s likelihood To calculate the likelihood of the data for a given logistic model, we look at the actual response, and the probability of the predicted response, and then determine the likelihood of the observation assuming a bernoulli process. To get the overall likelihood of the data, we then multiply the likelihood of each data point (or take the logs first and then the sum to get the log-likelihood). This table illustrate the steps involved: fit %&gt;% augment() %&gt;% clean_names() %&gt;% mutate(p = inv.logit(fitted)) %&gt;% select(response, p) %&gt;% mutate(p_response = ifelse(response == 1, p, 1-p), log_p = log(p_response)) %&gt;% rename(`p(Y = 1)` = p, `p(Y = response)` = p_response, `log(p(Y = response))` = log_p) %&gt;% head(10) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) response p(Y = 1) p(Y = response) log(p(Y = response)) 1 0.34 0.34 -1.07 0 0.53 0.47 -0.75 1 0.30 0.30 -1.20 1 0.81 0.81 -0.22 1 0.56 0.56 -0.58 0 0.30 0.70 -0.36 1 0.60 0.60 -0.52 1 0.65 0.65 -0.43 1 0.62 0.62 -0.48 0 0.41 0.59 -0.54 Let’s calculate the log-likelihood by hand: fit %&gt;% augment() %&gt;% clean_names() %&gt;% mutate(p = inv.logit(fitted), log_likelihood = response * log(p) + (1 - response) * log(1 - p)) %&gt;% summarize(log_likelihood = sum(log_likelihood)) ## # A tibble: 1 x 1 ## log_likelihood ## &lt;dbl&gt; ## 1 -605. And compare it with the model summary fit %&gt;% glance() %&gt;% select(logLik, AIC, BIC) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) logLik AIC BIC -604.82 1213.64 1223.45 We’re getting the same result – neat! Now we know how the likelihood of the data is calculated for a logistic regression model. 20.5 Testing hypotheses To test hypotheses, we use our gold old model comparison approach: # fit compact model fit.compact = glm(formula = survived ~ 1 + fare, family = &quot;binomial&quot;, data = df.titanic) # fit augmented model fit.augmented = glm(formula = survived ~ 1 + sex + fare, family = &quot;binomial&quot;, data = df.titanic) # likelihood ratio test anova(fit.compact, fit.augmented, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: survived ~ 1 + fare ## Model 2: survived ~ 1 + sex + fare ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 889 1117.57 ## 2 888 884.31 1 233.26 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that in order to get a p-value out of this, we need to specify what statistical test we’d like to run. In this case, we use the likelihood ratio test (“LRT”). 20.6 Logistic mixed effects model Just like we can build linear mixed effects models using lmer() instead of lm(), we can also build a logistic mixed effects regression using glmer() instead of glm(). Let’s read in some data: # load bdf data set from nlme package data(bdf, package = &quot;nlme&quot;) df.language = bdf %&gt;% clean_names() %&gt;% filter(repeatgr != 2) %&gt;% mutate(repeatgr = repeatgr %&gt;% as.character() %&gt;% as.numeric()) rm(bdf) Fit the model, and print out the results: fit = glmer(repeatgr ~ 1 + ses * minority + (1 | school_nr), data = df.language, family = &quot;binomial&quot;) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.00172186 ## (tol = 0.001, component 1) fit %&gt;% summary() ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: repeatgr ~ 1 + ses * minority + (1 | school_nr) ## Data: df.language ## ## AIC BIC logLik deviance df.resid ## 1660.9 1689.6 -825.5 1650.9 2278 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.8943 -0.4062 -0.3151 -0.2233 5.9156 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school_nr (Intercept) 0.2464 0.4964 ## Number of obs: 2283, groups: school_nr, 131 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.478689 0.206057 -2.323 0.0202 * ## ses -0.061214 0.007915 -7.733 1.05e-14 *** ## minorityY 0.482829 0.472314 1.022 0.3067 ## ses:minorityY 0.010820 0.022867 0.473 0.6361 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ses mnrtyY ## ses -0.907 ## minorityY -0.400 0.368 ## ses:minrtyY 0.298 -0.318 -0.865 ## convergence code: 0 ## Model failed to converge with max|grad| = 0.00172186 (tol = 0.001, component 1) 20.7 Additional information 20.7.1 Datacamp Multiple and logistic regression Generalized linear models in R Categorical data in the tidyverse "],
["bayesian-data-analysis-1.html", "Chapter 21 Bayesian data analysis 1 21.1 Load packages and set plotting theme 21.2 Things that came up", " Chapter 21 Bayesian data analysis 1 In this lecture, we did not perform any Bayesian data analysis. I discussed the costs and benefits of Bayesian analysis and introduced the Bayesian model of multi-modal integration based on the Plinko task. 21.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;modelr&quot;) # for permutation test library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 21.2 Things that came up 21.2.1 Bias in Cosyne 2019 conference admission? Code up the data: # data frame df.conference = tibble(sex = rep(c(&quot;female&quot;, &quot;male&quot;), c(264, 677)), accepted = rep(c(&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;), c(83, 264 - 83, 255, 677 - 255))) %&gt;% mutate(accepted = factor(accepted, levels = c(&quot;no&quot;, &quot;yes&quot;), labels = 0:1), sex = as.factor(sex)) Visualize the results: df.conference %&gt;% ggplot(data = ., mapping = aes(x = sex, fill = accepted)) + geom_bar(color = &quot;black&quot;) + scale_fill_brewer(palette = &quot;Set1&quot;) + coord_flip() + theme(legend.direction = &quot;horizontal&quot;, legend.position = &quot;top&quot;) + guides(fill = guide_legend(reverse = T)) Run a logistic regression with one binary predictor (Binomial test): # logistic regression fit.glm = glm(formula = accepted ~ 1 + sex, family = &quot;binomial&quot;, data = df.conference) # model summary fit.glm %&gt;% summary() ## ## Call: ## glm(formula = accepted ~ 1 + sex, family = &quot;binomial&quot;, data = df.conference) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.9723 -0.9723 -0.8689 1.3974 1.5213 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7797 0.1326 -5.881 4.07e-09 *** ## sexmale 0.2759 0.1545 1.786 0.0741 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1228.9 on 940 degrees of freedom ## Residual deviance: 1225.6 on 939 degrees of freedom ## AIC: 1229.6 ## ## Number of Fisher Scoring iterations: 4 The results of the logistic regression are not quite significant (at least when considering a two-tailed test) with \\(p = .0741\\). Let’s run a permutation test (as suggested by the tweet I showed in class): # make example reproducible set.seed(1) # difference in proportion fun.difference = function(df){ df %&gt;% as_tibble() %&gt;% count(sex, accepted) %&gt;% group_by(sex) %&gt;% mutate(proportion = n / sum(n)) %&gt;% filter(accepted == 1) %&gt;% select(sex, proportion) %&gt;% spread(sex, proportion) %&gt;% mutate(difference = male - female) %&gt;% pull(difference) } # actual difference difference = df.conference %&gt;% fun.difference() # permutation test df.permutation = df.conference %&gt;% permute(n = 1000, sex) %&gt;% mutate(difference = map_dbl(perm, ~ fun.difference(.))) Let’s calculate the p-value based on the permutation test: sum(df.permutation$difference &gt; difference) / nrow(df.permutation) ## [1] 0.026 And let’s visualize the result (showing our observed value and comparing it to the sampling distribution under the null hypothesis): df.permutation %&gt;% ggplot(data = ., mapping = aes(x = difference)) + stat_density(geom = &quot;line&quot;) + geom_vline(xintercept = difference, color = &quot;red&quot;, size = 1) "],
["bayesian-data-analysis-2.html", "Chapter 22 Bayesian data analysis 2 22.1 Load packages and set plotting theme 22.2 Doing Bayesian inference “by hand” 22.3 Distributions 22.4 Inference via sampling 22.5 Greta", " Chapter 22 Bayesian data analysis 2 22.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;greta&quot;) # for writing Bayesian models library(&quot;tidybayes&quot;) # tidying up results from Bayesian models library(&quot;cowplot&quot;) # for making figure panels library(&quot;ggrepel&quot;) # for labels in ggplots library(&quot;gganimate&quot;) # for animations library(&quot;extraDistr&quot;) # additional probability distributions library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 22.2 Doing Bayesian inference “by hand” 22.2.1 Sequential updating based on the Beta distribution # data data = c(0, 1, 1, 0, 1, 1, 1, 1) # whether observation is a success or failure success = c(0, cumsum(data)) failure = c(0, cumsum(1 - data)) # I&#39;ve added 0 at the beginning to show the prior # plotting function fun.plot_beta = function(success, failure){ ggplot(data = tibble(x = c(0, 1)), mapping = aes(x = x)) + stat_function(fun = &quot;dbeta&quot;, args = list(shape1 = success + 1, shape2 = failure + 1), geom = &quot;area&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;) + coord_cartesian(expand = F) + scale_x_continuous(breaks = seq(0.25, 0.75, 0.25)) + theme(axis.title = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), plot.margin = margin(r = 1, t = 0.5, unit = &quot;cm&quot;)) } # generate the plots plots = map2(success, failure, ~ fun.plot_beta(.x, .y)) # make a grid of plots plot_grid(plotlist = plots) 22.2.2 Coin flip example Is the coin biased? # data data = rep(0:1, c(8, 2)) # parameters theta = c(0.1, 0.5, 0.9) # prior prior = c(0.25, 0.5, 0.25) # prior = c(0.1, 0.1, 0.8) # alternative setting of the prior # prior = c(0.000001, 0.000001, 0.999998) # another prior setting # likelihood likelihood = dbinom(sum(data == 1), size = length(data), prob = theta) # posterior posterior = likelihood * prior / sum(likelihood * prior) # store in data frame df.coins = tibble( theta = theta, prior = prior, likelihood = likelihood, posterior = posterior ) Visualize the results: df.coins %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -theta) %&gt;% mutate(index = factor(index, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), theta = factor(theta, labels = c(&quot;p = 0.1&quot;, &quot;p = 0.5&quot;, &quot;p = 0.9&quot;))) %&gt;% ggplot(data = ., mapping = aes(x = theta, y = value, fill = index)) + geom_bar(stat = &quot;identity&quot;, color = &quot;black&quot;) + facet_grid(rows = vars(index), switch = &quot;y&quot;, scales = &quot;free&quot;) + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + annotate(&quot;segment&quot;, x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + theme(legend.position = &quot;none&quot;, strip.background = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.x = element_blank(), axis.line = element_blank()) 22.2.3 Bayesian inference by discretization 22.2.4 Effect of the prior # grid theta = seq(0, 1, 0.01) # data data = rep(0:1, c(8, 2)) # calculate posterior df.prior_effect = tibble(theta = theta, prior_uniform = dbeta(theta, shape1 = 1, shape2 = 1), prior_normal = dbeta(theta, shape1 = 5, shape2 = 5), prior_biased = dbeta(theta, shape1 = 8, shape2 = 2)) %&gt;% gather(&quot;prior_index&quot;, &quot;prior&quot;, -theta) %&gt;% mutate(likelihood = dbinom(sum(data == 1), size = length(data), prob = theta)) %&gt;% group_by(prior_index) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) %&gt;% ungroup() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(theta, prior_index)) # make the plot df.prior_effect %&gt;% mutate(index = factor(index, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), prior_index = factor(prior_index, levels = c(&quot;prior_uniform&quot;, &quot;prior_normal&quot;, &quot;prior_biased&quot;), labels = c(&quot;uniform&quot;, &quot;symmetric&quot;, &quot;asymmetric&quot;))) %&gt;% ggplot(data = ., mapping = aes(x = theta, y = value, color = index)) + geom_line(size = 1) + facet_grid(cols = vars(prior_index), rows = vars(index), scales = &quot;free&quot;, switch = &quot;y&quot;) + scale_x_continuous(breaks = seq(0, 1, 0.2)) + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + annotate(&quot;segment&quot;, x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + theme(legend.position = &quot;none&quot;, strip.background = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.text.x = element_text(size = 10), axis.line = element_blank()) Figure 22.1: Illustration of how the prior affects the posterior. 22.2.5 Effect of the likelihood # grid theta = seq(0, 1, 0.01) df.likelihood_effect = tibble(theta = theta, prior = dbeta(theta, shape1 = 2, shape2 = 8), likelihood_left = dbeta(theta, shape1 = 1, shape2 = 9), likelihood_center = dbeta(theta, shape1 = 5, shape2 = 5), likelihood_right = dbeta(theta, shape1 = 9, shape2 = 1)) %&gt;% gather(&quot;likelihood_index&quot;, &quot;likelihood&quot;, -c(&quot;theta&quot;, &quot;prior&quot;)) %&gt;% group_by(likelihood_index) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) %&gt;% ungroup() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(theta, likelihood_index)) df.likelihood_effect %&gt;% mutate(index = factor(index, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), likelihood_index = factor(likelihood_index, levels = c(&quot;likelihood_left&quot;, &quot;likelihood_center&quot;, &quot;likelihood_right&quot;), labels = c(&quot;left&quot;, &quot;center&quot;, &quot;right&quot;))) %&gt;% ggplot(data = ., mapping = aes(x = theta, y = value, color = index)) + geom_line(size = 1) + facet_grid(cols = vars(likelihood_index), rows = vars(index), scales = &quot;free&quot;, switch = &quot;y&quot;) + scale_x_continuous(breaks = seq(0, 1, 0.2)) + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + annotate(&quot;segment&quot;, x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + theme(legend.position = &quot;none&quot;, strip.background = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.text.x = element_text(size = 10), axis.line = element_blank(), strip.text.x = element_blank()) Figure 22.2: Illustration of how the likelihood of the data affects the posterior. 22.2.6 Effect of the sample size # grid theta = seq(0, 1, 0.01) df.sample_size_effect = tibble(theta = theta, prior = dbeta(theta, shape1 = 5, shape2 = 5), likelihood_low = dbeta(theta, shape1 = 2, shape2 = 8), likelihood_medium = dbeta(theta, shape1 = 10, shape2 = 40), likelihood_high = dbeta(theta, shape1 = 20, shape2 = 80)) %&gt;% gather(&quot;likelihood_index&quot;, &quot;likelihood&quot;, -c(&quot;theta&quot;, &quot;prior&quot;)) %&gt;% group_by(likelihood_index) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) %&gt;% ungroup() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(theta, likelihood_index)) df.sample_size_effect %&gt;% mutate(index = factor(index, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), likelihood_index = factor(likelihood_index, levels = c(&quot;likelihood_low&quot;, &quot;likelihood_medium&quot;, &quot;likelihood_high&quot;), labels = c(&quot;n = low&quot;, &quot;n = medium&quot;, &quot;n = high&quot;))) %&gt;% ggplot(data = ., mapping = aes(x = theta, y = value, color = index)) + geom_line(size = 1) + facet_grid(cols = vars(likelihood_index), rows = vars(index), scales = &quot;free&quot;, switch = &quot;y&quot;) + scale_x_continuous(breaks = seq(0, 1, 0.2)) + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + annotate(&quot;segment&quot;, x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + theme(legend.position = &quot;none&quot;, strip.background = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.text.x = element_text(size = 10), axis.line = element_blank()) 22.3 Distributions 22.3.1 Normal vs Student-t distribution tibble(x = c(-5, 5)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1, color = &quot;blue&quot;) + stat_function(fun = &quot;dt&quot;, size = 1, color = &quot;red&quot;, args = list(df = 1)) Figure 22.3: Comparison between the normal distribution and the student-t distribution. 22.3.2 Beta distributions fun.draw_beta = function(shape1, shape2){ ggplot(data = tibble(x = c(0, 1)), aes(x = x)) + stat_function(fun = &quot;dbeta&quot;, size = 1, color = &quot;black&quot;, args = list(shape1 = shape1, shape2 = shape2)) + annotate(geom = &quot;text&quot;, label = str_c(&quot;Beta(&quot;, shape1,&quot;,&quot;,shape2,&quot;)&quot;), x = 0.5, y = Inf, hjust = 0.5, vjust = 1.1, size = 4) + scale_x_continuous(breaks = seq(0, 1, 0.2)) + theme(axis.title.x = element_blank()) } shape1 = c(1, 0.5, 5, 1, 8, 20) shape2 = c(1, 0.5, 5, 9, 2, 20) p.list = map2(.x = shape1, .y = shape2, ~ fun.draw_beta(.x, .y)) plot_grid(plotlist = p.list) Figure 22.4: Beta distributions with different parameter settings. 22.3.3 Normal distributions tibble(x = c(-10, 10)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 1, color = &quot;blue&quot;, args = list(sd = 1)) + stat_function(fun = &quot;dnorm&quot;, size = 1, color = &quot;red&quot;, args = list(sd = 5)) Figure 22.5: Normal distributions with different standard deviation. 22.3.4 Distributions for non-negative parameters tibble(x = c(0, 10)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = &quot;dcauchy&quot;, size = 1, color = &quot;blue&quot;, args = list(location = 0, scale = 1), xlim = c(0, 10)) + stat_function(fun = &quot;dgamma&quot;, size = 1, color = &quot;red&quot;, args = list(shape = 4, rate = 2)) Figure 22.6: Cauchy and Gamma distribution. 22.4 Inference via sampling Example for how we can compute probabilities based on random samples generated from a distribution. # generate samples df.samples = tibble(x = rnorm(n = 10000, mean = 1, sd = 2)) # visualize distribution ggplot(data = df.samples, mapping = aes(x = x)) + stat_density(geom = &quot;line&quot;, color = &quot;red&quot;, size = 2) + stat_function(fun = &quot;dnorm&quot;, args = list(mean = 1, sd = 2), color = &quot;black&quot;, linetype = 2) # calculate probability based on samples df.samples %&gt;% summarize(prob = sum(x &gt;= 0 &amp; x &lt; 4)/n()) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.622 # calculate probability based on theoretical distribution pnorm(4, mean = 1, sd = 2) - pnorm(0, mean = 1, sd = 2) ## [1] 0.6246553 22.5 Greta You can find out more about how get started with “greta” here: https://greta-stats.org/articles/get_started.html. Make sure to install the development version of “greta” (as shown in the “install-packages” code chunk above: devtools::install_github(\"greta-dev/greta\")). 22.5.1 Attitude data set # load the attitude data set df.attitude = attitude Visualize relationship between how well complaints are handled and the overall rating of an employee ggplot(data = df.attitude, mapping = aes(x = complaints, y = rating)) + geom_point() 22.5.2 Frequentist analysis # fit model fit = lm(formula = rating ~ 1 + complaints, data = df.attitude) # print summary fit %&gt;% summary() ## ## Call: ## lm(formula = rating ~ 1 + complaints, data = df.attitude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.8799 -5.9905 0.1783 6.2978 9.6294 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.37632 6.61999 2.172 0.0385 * ## complaints 0.75461 0.09753 7.737 1.99e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.993 on 28 degrees of freedom ## Multiple R-squared: 0.6813, Adjusted R-squared: 0.6699 ## F-statistic: 59.86 on 1 and 28 DF, p-value: 1.988e-08 Visualize the model’s predictions ggplot(data = df.attitude, mapping = aes(x = complaints, y = rating)) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + geom_point() 22.5.3 Bayesian regression 22.5.3.1 Fit the model # variables &amp; priors b0 = normal(0, 10) b1 = normal(0, 10) sd = cauchy(0, 3, truncation = c(0, Inf)) # linear predictor mu = b0 + b1 * df.attitude$complaints # observation model (likelihood) distribution(df.attitude$rating) = normal(mu, sd) # define the model m = model(b0, b1, sd) Visualize the model as graph: # plotting plot(m) Draw samples from the posterior distribution: # sampling draws = mcmc(m, n_samples = 1000) # tidy up the draws df.draws = tidy_draws(draws) %&gt;% clean_names() 22.5.3.2 Visualize the priors These are the priors I used for the intercept, regression weights, and the standard deviation of the Gaussian likelihood function: # Gaussian ggplot(tibble(x = c(-30, 30)), aes(x = x)) + stat_function(fun = &quot;dnorm&quot;, size = 2, args = list(sd = 10)) # Cauchy ggplot(tibble(x = c(0, 30)), aes(x = x)) + stat_function(fun = &quot;dcauchy&quot;, size = 2, args = list(location = 0, scale = 3)) 22.5.3.3 Visualize the posteriors This is what the posterior looks like for the three parameters in the model: df.draws %&gt;% select(draw:sd) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -draw) %&gt;% ggplot(data = ., mapping = aes(x = value)) + stat_density(geom = &quot;line&quot;) + facet_grid(rows = vars(index), scales = &quot;free_y&quot;, switch = &quot;y&quot;) + annotate(&quot;segment&quot;, x = -Inf, xend = Inf, y = -Inf, yend = -Inf) + annotate(&quot;segment&quot;, x = -Inf, xend = -Inf, y = -Inf, yend = Inf) + theme(legend.position = &quot;none&quot;, strip.background = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.text.x = element_text(size = 10), axis.line = element_blank(), strip.text.x = element_blank()) 22.5.3.4 Visualize model predictions Let’s take some samples from the posterior to visualize the model predictions: ggplot(data = df.attitude, mapping = aes(x = complaints, y = rating)) + geom_abline(data = df.draws %&gt;% sample_n(size = 50), aes(intercept = b0, slope = b1), alpha = 0.3, color = &quot;lightblue&quot;) + geom_point() 22.5.3.5 Posterior predictive check Let’s make an animation that illustrates what predicted data sets (based on samples from the posterior) would look like: df.draws %&gt;% sample_n(size = 10) %&gt;% mutate(complaints = list(seq(min(df.attitude$complaints), max(df.attitude$complaints), length.out = nrow(df.attitude)))) %&gt;% unnest(complaints) %&gt;% mutate(prediction = b0 + b1 * complaints + rnorm(n(), sd = sd)) %&gt;% ggplot(aes(x = complaints, y = prediction)) + geom_point(alpha = 0.8, color = &quot;lightblue&quot;) + geom_point(data = df.attitude, aes(y = rating, x = complaints)) + coord_cartesian(xlim = c(20, 100), ylim = c(20, 100)) + transition_manual(draw) # animate(p, nframes = 60, width = 800, height = 600, res = 96, type = &quot;cairo&quot;) # anim_save(&quot;posterior_predictive.gif&quot;) 22.5.3.6 Prior predictive check And let’s illustrate what data we would have expected to see just based on the information that we encoded in our priors. sample_size = 10 tibble( b0 = rnorm(sample_size, mean = 0, sd = 10), b1 = rnorm(sample_size, mean = 0, sd = 10), sd = rhcauchy(sample_size, sigma = 3), draw = 1:sample_size ) %&gt;% mutate(complaints = list(runif(nrow(df.attitude), min = min(df.attitude$complaints), max = max(df.attitude$complaints)))) %&gt;% unnest(complaints) %&gt;% mutate(prediction = b0 + b1 * complaints + rnorm(n(), sd = sd)) %&gt;% ggplot(aes(x = complaints, y = prediction)) + geom_point(alpha = 0.8, color = &quot;lightblue&quot;) + geom_point(data = df.attitude, aes(y = rating, x = complaints)) + transition_manual(draw) # animate(p, nframes = 60, width = 800, height = 600, res = 96, type = &quot;cairo&quot;) # anim_save(&quot;prior_predictive.gif&quot;) "],
["bayesian-data-analysis-3.html", "Chapter 23 Bayesian data analysis 3 23.1 Load packages and set plotting theme 23.2 Load data set 23.3 Poker 23.4 Dealing with heteroscedasticity 23.5 Ordinal regression 23.6 Additional resources", " Chapter 23 Bayesian data analysis 3 23.1 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;modelr&quot;) # for doing modeling stuff library(&quot;tidybayes&quot;) # tidying up results from Bayesian models library(&quot;brms&quot;) # Bayesian regression models with Stan library(&quot;rstanarm&quot;) # for Bayesian models library(&quot;cowplot&quot;) # for making figure panels library(&quot;ggrepel&quot;) # for labels in ggplots library(&quot;gganimate&quot;) # for animations library(&quot;GGally&quot;) # for pairs plot library(&quot;bayesplot&quot;) # for visualization of Bayesian model fits library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 23.2 Load data set Load the poker data set. df.poker = read_csv(&quot;data/poker.csv&quot;) %&gt;% mutate(skill = factor(skill, levels = 1:2, labels = c(&quot;expert&quot;, &quot;average&quot;)), skill = fct_relevel(skill, &quot;average&quot;, &quot;expert&quot;), hand = factor(hand, levels = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)), limit = factor(limit, levels = 1:2, labels = c(&quot;fixed&quot;, &quot;none&quot;)), participant = 1:n()) %&gt;% select(participant, everything()) ## Parsed with column specification: ## cols( ## skill = col_double(), ## hand = col_double(), ## limit = col_double(), ## balance = col_double() ## ) 23.3 Poker 23.3.1 Visualization Let’s visualize the data first: df.poker %&gt;% ggplot(mapping = aes(x = hand, y = balance, fill = hand)) + geom_point(alpha = 0.2, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, size = 4) + labs(y = &quot;final balance (in Euros)&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + theme(legend.position = &quot;none&quot;) 23.3.2 Linear model And let’s now fit a simple (frequentist) regression model: fit.lm = lm(formula = balance ~ 1 + hand, data = df.poker) fit.lm %&gt;% summary() ## ## Call: ## lm(formula = balance ~ 1 + hand, data = df.poker) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.9264 -2.5902 -0.0115 2.6573 15.2834 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.9415 0.4111 14.451 &lt; 2e-16 *** ## handneutral 4.4051 0.5815 7.576 4.55e-13 *** ## handgood 7.0849 0.5815 12.185 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.111 on 297 degrees of freedom ## Multiple R-squared: 0.3377, Adjusted R-squared: 0.3332 ## F-statistic: 75.7 on 2 and 297 DF, p-value: &lt; 2.2e-16 23.3.3 Bayesian model Now, let’s fit a Bayesian regression model using the brm() function: fit.brm1 = brm(formula = balance ~ 1 + hand, data = df.poker, file = &quot;cache/brm1&quot;) fit.brm1 %&gt;% summary() ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: balance ~ 1 + hand ## Data: df.poker (Number of observations: 300) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.95 0.42 5.12 6.79 3609 1.00 ## handneutral 4.38 0.59 3.24 5.56 3489 1.00 ## handgood 7.07 0.59 5.94 8.22 3553 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 4.13 0.17 3.81 4.47 3578 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I use the file = argument to save the model’s results so that when I run this code chunk again, the model doesn’t need to be fit again (fitting Bayesian models takes a while …). 23.3.3.1 Visualize the posteriors Let’s visualize what the posterior for the different parameters looks like. We use the geom_halfeyeh() function from the “tidybayes” package to do so: fit.brm1 %&gt;% posterior_samples() %&gt;% select(-lp__) %&gt;% gather(&quot;variable&quot;, &quot;value&quot;) %&gt;% ggplot(data = ., mapping = aes(y = variable, x = value)) + geom_halfeyeh() And let’s look at how the samples from the posterior are correlated with each other: fit.brm1 %&gt;% posterior_samples() %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs(lower = list(continuous = wrap(&quot;points&quot;, alpha = 0.03)), upper = list(continuous = wrap(&quot;cor&quot;, size = 6))) + theme(panel.grid.major = element_blank(), text = element_text(size = 12)) 23.3.3.2 Compute highest density intervals To compute the MAP (maximum a posteriori probability) estimate and highest density interval, we use the mode_hdi() function that comes with the “tidybayes” package. fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(starts_with(&quot;b_&quot;), sigma) %&gt;% mode_hdi() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(.width:.interval)) %&gt;% select(index, value) %&gt;% mutate(index = ifelse(str_detect(index, fixed(&quot;.&quot;)), index, str_c(index, &quot;.mode&quot;))) %&gt;% separate(index, into = c(&quot;parameter&quot;, &quot;type&quot;), sep = &quot;\\\\.&quot;) %&gt;% spread(type, value) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) parameter lower mode upper b_handgood 5.93 7.10 8.20 b_handneutral 3.19 4.42 5.47 b_intercept 5.11 6.02 6.78 sigma 3.81 4.10 4.46 23.3.3.3 Posterior predictive check To check whether the model did a good job capturing the data, we can simulate what future data the Baysian model predicts, now that it has learned from the data we feed into it. pp_check(fit.brm1, nsamples = 100) This looks good! The predicted shaped of the data based on samples from the posterior distribution looks very similar to the shape of the actual data. Let’s make a hypothetical outcome plot that shows what concrete data sets the model would predict: # generate predictive samples df.predictive_samples = fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(contains(&quot;b_&quot;), sigma) %&gt;% sample_n(size = 20) %&gt;% mutate(sample = 1:n()) %&gt;% group_by(sample) %&gt;% nest() %&gt;% mutate(bad = map(data, ~ .$b_intercept + rnorm(100, sd = .$sigma)), neutral = map(data, ~ .$b_intercept + .$b_handneutral + rnorm(100, sd = .$sigma)), good = map(data, ~ .$b_intercept + .$b_handgood + rnorm(100, sd = .$sigma))) %&gt;% unnest(bad, neutral, good) # plot the results as an animation df.predictive_samples %&gt;% gather(&quot;hand&quot;, &quot;balance&quot;, -sample) %&gt;% mutate(hand = factor(hand, levels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;))) %&gt;% ggplot(mapping = aes(x = hand, y = balance, fill = hand)) + geom_point(alpha = 0.2, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, size = 4) + labs(y = &quot;final balance (in Euros)&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + theme(legend.position = &quot;none&quot;) + transition_manual(sample) # animate(p, nframes = 120, width = 800, height = 600, res = 96, type = &quot;cairo&quot;) # anim_save(&quot;poker_posterior_predictive.gif&quot;) 23.3.3.4 Test hypothesis One key advantage of Bayesian over frequentist analysis is that we can test hypothesis in a very flexible manner by directly probing our posterior samples in different ways. We may ask, for example, what the probability is that the parameter for the difference between a bad hand and a neutral hand (b_handneutral) is greater than 0. Let’s plot the posterior distribution together with the criterion: fit.brm1 %&gt;% posterior_samples() %&gt;% select(b_handneutral) %&gt;% gather(&quot;variable&quot;, &quot;value&quot;) %&gt;% ggplot(data = ., mapping = aes(y = variable, x = value)) + geom_halfeyeh() + geom_vline(xintercept = 0, color = &quot;red&quot;) We see that the posterior is definitely greater than 0. We can ask many different kinds of questions about the data by doing basic arithmetic on our posterior samples. The hypothesis() function makes this even easier. Here are some examples: # the probability that the posterior for handneutral is less than 0 hypothesis(fit.brm1, hypothesis = &quot;handneutral &lt; 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (handneutral) &lt; 0 4.38 0.59 -Inf 5.36 0 ## Post.Prob Star ## 1 0 ## --- ## &#39;*&#39;: The expected value under the hypothesis lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that the posterior for handneutral is greater than 4 hypothesis(fit.brm1, hypothesis = &quot;handneutral &gt; 4&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (handneutral)-(4) &gt; 0 0.38 0.59 -0.56 Inf 2.89 ## Post.Prob Star ## 1 0.74 ## --- ## &#39;*&#39;: The expected value under the hypothesis lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that good hands make twice as much as bad hands hypothesis(fit.brm1, hypothesis = &quot;Intercept + handgood &gt; 2 * Intercept&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (Intercept+handgo... &gt; 0 1.12 0.94 -0.42 Inf 7.6 ## Post.Prob Star ## 1 0.88 ## --- ## &#39;*&#39;: The expected value under the hypothesis lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that neutral hands make less than the average of bad and good hands hypothesis(fit.brm1, hypothesis = &quot;Intercept + handneutral &lt; (Intercept + Intercept + handgood) / 2&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (Intercept+handne... &lt; 0 0.85 0.49 -Inf 1.66 0.04 ## Post.Prob Star ## 1 0.04 ## --- ## &#39;*&#39;: The expected value under the hypothesis lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. Let’s double check one example, and calculate the result directly based on the posterior samples: df.hypothesis = fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(neutral = b_intercept + b_handneutral, bad_good_average = (b_intercept + b_intercept + b_handgood)/2, hypothesis = neutral &lt; bad_good_average) df.hypothesis %&gt;% summarize(p = sum(hypothesis)/n()) ## p ## 1 0.04175 23.3.3.5 Bayes factor Another way of testing hypothesis is via the Bayes factor. Let’s fit the two models we are interested in comparing with each other: fit.brm2 = brm(formula = balance ~ 1 + hand, data = df.poker, save_all_pars = T, file = &quot;cache/brm2&quot;) fit.brm3 = brm(formula = balance ~ 1 + hand + skill, data = df.poker, save_all_pars = T, file = &quot;cache/brm3&quot;) And then compare the models useing the bayes_factor() function: bayes_factor(fit.brm3, fit.brm2) ## Iteration: 1 ## Iteration: 2 ## Iteration: 3 ## Iteration: 4 ## Iteration: 5 ## Iteration: 1 ## Iteration: 2 ## Iteration: 3 ## Iteration: 4 ## Iteration: 5 ## Estimated Bayes factor in favor of bridge1 over bridge2: 3.83873 23.3.3.6 Full specification So far, we have used the defaults that brm() comes with and not bothered about specifiying the priors, etc. 23.3.3.6.1 Getting the priors Notice that we didn’t specify any priors in the model. By default, “brms” assigns weakly informative priors to the parameters in the model. We can see what these are by running the following command: fit.brm1 %&gt;% prior_summary() ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b handgood ## 3 b handneutral ## 4 student_t(3, 10, 10) Intercept ## 5 student_t(3, 0, 10) sigma We can also get information about which priors need to be specified before fitting a model: get_prior(formula = balance ~ 1 + hand, family = &quot;gaussian&quot;, data = df.poker) ## prior class coef group resp dpar nlpar bound ## 1 b ## 2 b handgood ## 3 b handneutral ## 4 student_t(3, 10, 10) Intercept ## 5 student_t(3, 0, 10) sigma Here is an example for what a more complete model specification could look like: fit.brm4 = brm( formula = balance ~ 1 + hand, family = &quot;gaussian&quot;, data = df.poker, prior = c( prior(normal(0, 10), class = &quot;b&quot;, coef = &quot;handgood&quot;), prior(normal(0, 10), class = &quot;b&quot;, coef = &quot;handneutral&quot;), prior(student_t(3, 3, 10), class = &quot;Intercept&quot;), prior(student_t(3, 0, 10), class = &quot;sigma&quot;) ), inits = list( list(Intercept = 0, sigma = 1, handgood = 5, handneutral = 5), list(Intercept = -5, sigma = 3, handgood = 2, handneutral = 2), list(Intercept = 2, sigma = 1, handgood = -1, handneutral = 1), list(Intercept = 1, sigma = 2, handgood = 2, handneutral = -2) ), iter = 4000, warmup = 1000, chains = 4, file = &quot;cache/brm4&quot;, seed = 1 ) fit.brm4 %&gt;% summary() ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: balance ~ 1 + hand ## Data: df.poker (Number of observations: 300) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.96 0.41 5.15 6.76 9191 1.00 ## handneutral 4.37 0.58 3.23 5.53 9629 1.00 ## handgood 7.05 0.58 5.93 8.19 9778 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 4.13 0.17 3.81 4.49 12855 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can also take a look at the Stan code that the brm() function creates: fit.brm4 %&gt;% stancode() ## // generated with brms 2.7.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K; // number of population-level effects ## matrix[N, K] X; // population-level design matrix ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## int Kc = K - 1; ## matrix[N, K - 1] Xc; // centered version of X ## vector[K - 1] means_X; // column means of X before centering ## for (i in 2:K) { ## means_X[i - 1] = mean(X[, i]); ## Xc[, i - 1] = X[, i] - means_X[i - 1]; ## } ## } ## parameters { ## vector[Kc] b; // population-level effects ## real temp_Intercept; // temporary intercept ## real&lt;lower=0&gt; sigma; // residual SD ## } ## transformed parameters { ## } ## model { ## vector[N] mu = temp_Intercept + Xc * b; ## // priors including all constants ## target += normal_lpdf(b[1] | 0, 10); ## target += normal_lpdf(b[2] | 0, 10); ## target += student_t_lpdf(temp_Intercept | 3, 3, 10); ## target += student_t_lpdf(sigma | 3, 0, 10) ## - 1 * student_t_lccdf(0 | 3, 0, 10); ## // likelihood including all constants ## if (!prior_only) { ## target += normal_lpdf(Y | mu, sigma); ## } ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = temp_Intercept - dot_product(means_X, b); ## } One thing worth noticing: by default, “brms” centers the predictors which makes it easier to assign a default prior over the intercept. 23.3.3.7 Inference diagnostics So far, we’ve assumed that the inference has worked out. We can check this by running plot() on our brm object: plot(fit.brm1) Let’s make our own version of a trace plot for one parameter in the model: fit.brm1 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() We can also take a look at the auto-correlation plot. Ideally, we want to generate independent samples from the posterior. So we don’t want subsequent samples to be strongly correlated with each other. Let’s take a look: variables = fit.brm1 %&gt;% get_variables() %&gt;% .[1:4] fit.brm1 %&gt;% posterior_samples() %&gt;% mcmc_acf(pars = variables, lags = 4) Looking good! The autocorrelation should become very small as the lag increases (indicating that we are getting independent samples from the posterior). 23.3.3.7.1 When things go wrong Let’s try to fit a model to very little data (just two observations) with extremely uninformative priors: df.data = tibble(y = c(-1, 1)) fit.brm5 = brm( data = df.data, family = gaussian, formula = y ~ 1, prior = c( prior(uniform(-1e10, 1e10), class = Intercept), prior(uniform(0, 1e10), class = sigma) ), inits = list( list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1) ), iter = 4000, warmup = 1000, chains = 2, file = &quot;cache/brm5&quot; ) Let’s take a look at the posterior distributions of the model parameters: summary(fit.brm5) ## Warning: There were 225 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: df.data (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample ## Intercept -51502175.28 484211604.68 -1258318917.93 524975203.26 103 ## Rhat ## Intercept 1.04 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 398611802.92 1217973385.49 852.93 4284866883.47 13 1.07 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Not looking good – The estimates and credible intervals are off the charts. And the effective samples sizes in the chains are very small. Let’s visualize the trace plots: plot(fit.brm5) fit.brm5 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() Given that we have so little data in this case, we need to help the model a little bit by providing some slighlty more specific priors. fit.brm6 = brm( data = df.data, family = gaussian, formula = y ~ 1, prior = c( prior(normal(0, 10), class = Intercept), # more reasonable priors prior(cauchy(0, 1), class = sigma) ), iter = 4000, warmup = 1000, chains = 2, seed = 1, file = &quot;cache/brm6&quot; ) Let’s take a look at the posterior distributions of the model parameters: summary(fit.brm6) ## Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: df.data (Number of observations: 2) ## Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.13 1.69 -4.10 3.06 881 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 2.04 1.88 0.61 6.95 1152 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This looks much better. There is still quite a bit of uncertainty in our paremeter estimates, but it has reduced dramatically. Let’s visualize the trace plots: plot(fit.brm6) fit.brm6 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() Looking mostly good – except for one hiccup on sigma … 23.4 Dealing with heteroscedasticity Let’s generate some fake developmental data where the variance in the data is greatest for young children, smaller for older children, and even smaller for adults: # make example reproducible set.seed(0) df.variance = tibble( group = rep(c(&quot;3yo&quot;, &quot;5yo&quot;, &quot;adults&quot;), each = 20), response = rnorm(60, mean = rep(c(0, 5, 8), each = 20), sd = rep(c(3, 1.5, 0.3), each = 20)) ) df.variance %&gt;% ggplot(aes(x = group, y = response)) + geom_jitter(height = 0, width = 0.1, alpha = 0.7) While frequentist models (such as a linear regression) assume equality of variance, Baysian models afford us with the flexibility of inferring both the parameter estimates of the groups (i.e. the means and differences between the means), as well as the variances. We simply define a multivariate model which tries to fit both the response as well as the variance sigma: fit.brm7 = brm( formula = bf(response ~ group, sigma ~ group), data = df.variance, file = &quot;cache/brm7&quot; ) Let’s take a look at the model output: summary(fit.brm7) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: response ~ group ## sigma ~ group ## Data: df.variance (Number of observations: 60) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.00 0.72 -1.38 1.45 1273 1.00 ## sigma_Intercept 1.15 0.17 0.85 1.51 2033 1.00 ## group5yo 5.16 0.77 3.60 6.62 1424 1.00 ## groupadults 7.96 0.72 6.49 9.37 1276 1.00 ## sigma_group5yo -1.05 0.23 -1.51 -0.59 2355 1.00 ## sigma_groupadults -2.19 0.23 -2.65 -1.74 2231 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). And let’s visualize the results: df.variance %&gt;% expand(group) %&gt;% add_fitted_draws(fit.brm7, dpar = TRUE) %&gt;% select(group, .row, .draw, posterior = .value, mu, sigma) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, c(mu, sigma)) %&gt;% ggplot(aes(x = value, y = group)) + geom_halfeyeh() + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + facet_grid(cols = vars(index)) This plot shows what the posterior looks like for both mu (the inferred means), and for sigma (the inferred variances) for the different groups. 23.5 Ordinal regression For more information, see this tutorial. While running an ordinal regression is far from trivial in frequentist world, it’s easy to do using “brms”. Let’s load the cars data and turn the number of cylinders into an ordered factor: df.cars = mtcars %&gt;% mutate(cyl = ordered(cyl)) # creates an ordered factor Let’s check that the cylinders are indeed ordered now: df.cars %&gt;% str() ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : Ord.factor w/ 3 levels &quot;4&quot;&lt;&quot;6&quot;&lt;&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... fit.brm8 = brm(formula = cyl ~ mpg, data = df.cars, family = &quot;cumulative&quot;, file = &quot;cache/brm8&quot;, seed = 1) Visualize the results: data_plot = df.cars %&gt;% ggplot(aes(x = mpg, y = cyl, color = cyl)) + geom_point() + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;cyl&quot;) fit_plot = df.cars %&gt;% data_grid(mpg = seq_range(mpg, n = 101)) %&gt;% add_fitted_draws(fit.brm8, value = &quot;P(cyl | mpg)&quot;, category = &quot;cyl&quot;) %&gt;% ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) + stat_lineribbon(aes(fill = cyl), alpha = 1/5, .width = c(0.95)) + scale_color_brewer(palette = &quot;Dark2&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;) plot_grid(ncol = 1, align = &quot;v&quot;, data_plot, fit_plot ) Posterior predictive check: df.cars %&gt;% select(mpg) %&gt;% add_predicted_draws(fit.brm8, prediction = &quot;cyl&quot;, seed = 1234) %&gt;% ggplot(aes(x = mpg, y = cyl)) + geom_count(color = &quot;gray75&quot;) + geom_point(aes(fill = cyl), data = df.cars, shape = 21, size = 2) + scale_fill_brewer(palette = &quot;Dark2&quot;) + geom_label_repel( data = . %&gt;% ungroup() %&gt;% filter(cyl == &quot;8&quot;) %&gt;% filter(mpg == max(mpg)) %&gt;% dplyr::slice(1), label = &quot;posterior predictions&quot;, xlim = c(26, NA), ylim = c(NA, 2.8), point.padding = 0.3, label.size = NA, color = &quot;gray50&quot;, segment.color = &quot;gray75&quot;) + geom_label_repel( data = df.cars %&gt;% filter(cyl == &quot;6&quot;) %&gt;% filter(mpg == max(mpg)) %&gt;% dplyr::slice(1), label = &quot;observed data&quot;, xlim = c(26, NA), ylim = c(2.2, NA), point.padding = 0.2, label.size = NA, segment.color = &quot;gray35&quot;) 23.6 Additional resources Tutorial on visualizing brms posteriors with tidybayes Hypothetical outcome plots Visual MCMC diagnostics How to model slider data the Baysian way "],
["mediation-moderation.html", "Chapter 24 Mediation &amp; Moderation 24.1 Recommended reading 24.2 Load packages and set plotting theme 24.3 Mediation 24.4 Moderation 24.5 Additional resources", " Chapter 24 Mediation &amp; Moderation These notes are adapted from this tutorial: Mediation and moderation Mediation analysis tests a hypothetical causal chain where one variable X affects a second variable M and, in turn, that variable affects a third variable Y. Mediators describe the how or why of a (typically well-established) relationship between two other variables and are sometimes called intermediary variables since they often describe the process through which an effect occurs. This is also sometimes called an indirect effect. For instance, people with higher incomes tend to live longer but this effect is explained by the mediating influence of having access to better health care. 24.1 Recommended reading Fiedler, Schott, and Meiser (2011) MacKinnon, Fairchild, and Fritz (2007) 24.2 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;mediation&quot;) # for mediation and moderation analysis library(&quot;multilevel&quot;) # Sobel test library(&quot;broom&quot;) # tidying up regression results library(&quot;brms&quot;) # Bayesian regression models library(&quot;tidybayes&quot;) # visualize the posterior library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 24.3 Mediation Figure 24.1: Basic mediation model. c = the total effect of X on Y; c = c’ + ab; c’ = the direct effect of X on Y after controlling for M; c’ = c - ab; ab = indirect effect of X on Y. Mediation tests whether the effects of X (the independent variable) on Y (the dependent variable) operate through a third variable, M (the mediator). In this way, mediators explain the causal relationship between two variables or “how” the relationship works, making it a very popular method in psychological research. Figure 24.1 shows the standard mediation model. Perfect mediation occurs when the effect of X on Y decreases to 0 with M in the model. Partial mediation occurs when the effect of X on Y decreases by a nontrivial amount (the actual amount is up for debate) with M in the model. Important: Both mediation and moderation assume that the DV did not CAUSE the mediator/moderator. 24.3.1 Generate data # make example reproducible set.seed(123) # number of participants n = 100 # generate data df.mediation = tibble( x = rnorm(n, 75, 7), # grades m = 0.7 * x + rnorm(n, 0, 5), # self-esteem y = 0.4 * m + rnorm(n, 0, 5) # happiness ) 24.3.2 Method 1: Baron &amp; Kenny’s (1986) indirect effect method The Baron and Kenny (1986) method is among the original methods for testing for mediation but tends to have low statistical power. It is covered in this chapter because it provides a very clear approach to establishing relationships between variables and is still occassionally requested by reviewers. The three steps: Estimate the relationship between \\(X\\) and \\(Y\\) (hours since dawn on degree of wakefulness). Path “c” must be significantly different from 0; must have a total effect between the IV &amp; DV. Estimate the relationship between \\(X\\) and \\(M\\) (hours since dawn on coffee consumption). Path “a” must be significantly different from 0; IV and mediator must be related. Estimate the relationship between \\(M\\) and \\(Y\\) controlling for \\(X\\) (coffee consumption on wakefulness, controlling for hours since dawn). Path “b” must be significantly different from 0; mediator and DV must be related. The effect of \\(X\\) on \\(Y\\) decreases with the inclusion of \\(M\\) in the model. 24.3.2.1 Total effect Total effect of X on Y (not controlling for M). # fit the model fit.y_x = lm(formula = y ~ 1 + x, data = df.mediation) # summarize the results fit.y_x %&gt;% summary() ## ## Call: ## lm(formula = y ~ 1 + x, data = df.mediation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.917 -3.738 -0.259 2.910 12.540 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.78300 6.16002 1.426 0.1571 ## x 0.16899 0.08116 2.082 0.0399 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.16 on 98 degrees of freedom ## Multiple R-squared: 0.04237, Adjusted R-squared: 0.0326 ## F-statistic: 4.336 on 1 and 98 DF, p-value: 0.03993 24.3.2.2 Path a fit.m_x = lm(formula = m ~ 1 + x, data = df.mediation) fit.m_x %&gt;% summary() ## ## Call: ## lm(formula = m ~ 1 + x, data = df.mediation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5367 -3.4175 -0.4375 2.9032 16.4520 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.29696 5.79432 0.396 0.693 ## x 0.66252 0.07634 8.678 8.87e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.854 on 98 degrees of freedom ## Multiple R-squared: 0.4346, Adjusted R-squared: 0.4288 ## F-statistic: 75.31 on 1 and 98 DF, p-value: 8.872e-14 24.3.2.3 Path b and c’ Effect of M on Y controlling for X. fit.y_mx = lm(formula = y ~ 1 + m + x, data = df.mediation) fit.y_mx %&gt;% summary() ## ## Call: ## lm(formula = y ~ 1 + m + x, data = df.mediation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3651 -3.3037 -0.6222 3.1068 10.3991 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.80952 5.68297 1.374 0.173 ## m 0.42381 0.09899 4.281 4.37e-05 *** ## x -0.11179 0.09949 -1.124 0.264 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.756 on 97 degrees of freedom ## Multiple R-squared: 0.1946, Adjusted R-squared: 0.1779 ## F-statistic: 11.72 on 2 and 97 DF, p-value: 2.771e-05 24.3.2.4 Interpretation fit.y_x %&gt;% tidy() %&gt;% mutate(path = &quot;c&quot;) %&gt;% bind_rows( fit.m_x %&gt;% tidy() %&gt;% mutate(path = &quot;a&quot;), fit.y_mx %&gt;% tidy() %&gt;% mutate(path = c(&quot;(Intercept)&quot;, &quot;b&quot;, &quot;c&#39;&quot;)) ) %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% mutate(significance = p.value &lt; .05, dv = ifelse(path %in% c(&quot;c&#39;&quot;, &quot;b&quot;), &quot;y&quot;, &quot;m&quot;)) %&gt;% select(path, iv = term, dv, estimate, p.value, significance) ## # A tibble: 4 x 6 ## path iv dv estimate p.value significance ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 c x m 0.169 3.99e- 2 TRUE ## 2 a x m 0.663 8.87e-14 TRUE ## 3 b m y 0.424 4.37e- 5 TRUE ## 4 c&#39; x y -0.112 2.64e- 1 FALSE Here we find that our total effect model shows a significant positive relationship between hours since dawn (X) and wakefulness (Y). Our Path A model shows that hours since down (X) is also positively related to coffee consumption (M). Our Path B model then shows that coffee consumption (M) positively predicts wakefulness (Y) when controlling for hours since dawn (X). Since the relationship between hours since dawn and wakefulness is no longer significant when controlling for coffee consumption, this suggests that coffee consumption does in fact mediate this relationship. However, this method alone does not allow for a formal test of the indirect effect so we don’t know if the change in this relationship is truly meaningful. 24.3.3 Method 2: Sobel Test The Sobel Test tests whether the indirect effect from X via M to Y is significant. library(&quot;multilevel&quot;) # run the sobel test fit.sobel = sobel(pred = df.mediation$x, med = df.mediation$m, out = df.mediation$y) # calculate the p-value (1 - pnorm(fit.sobel$z.value))*2 ## [1] 0.0001233403 The relationship between “hours since dawn” and “wakefulness” is significantly mediated by “coffee consumption”. The Sobel Test is largely considered an outdated method since it assumes that the indirect effect (ab) is normally distributed and tends to only have adequate power with large sample sizes. Thus, again, it is highly recommended to use the mediation bootstrapping method instead. 24.3.4 Method 3: Bootstrapping The “mediation” packages uses the more recent bootstrapping method of Preacher and Hayes (2004) to address the power limitations of the Sobel Test. This method does not require that the data are normally distributed, and is particularly suitable for small sample sizes. library(&quot;mediation&quot;) # bootstrapped mediation fit.mediation = mediate(model.m = fit.m_x, model.y = fit.y_mx, treat = &quot;x&quot;, mediator = &quot;m&quot;, boot = T) # summarize results fit.mediation %&gt;% summary() ## ## Causal Mediation Analysis ## ## Nonparametric Bootstrap Confidence Intervals with the Percentile Method ## ## Estimate 95% CI Lower 95% CI Upper p-value ## ACME 0.28078 0.14059 0.42 &lt;2e-16 *** ## ADE -0.11179 -0.29276 0.10 0.272 ## Total Effect 0.16899 -0.00415 0.34 0.064 . ## Prop. Mediated 1.66151 -3.22476 11.46 0.064 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Sample Size Used: 100 ## ## ## Simulations: 1000 ACME = Average causal mediation effect ADE = Average direct effect Total effect = ACME + ADE Plot the results: fit.mediation %&gt;% plot() 24.3.4.1 Interpretation The mediate() function gives us our Average Causal Mediation Effects (ACME), our Average Direct Effects (ADE), our combined indirect and direct effects (Total Effect), and the ratio of these estimates (Prop. Mediated). The ACME here is the indirect effect of M (total effect - direct effect) and thus this value tells us if our mediation effect is significant. 24.3.5 Method 4: Bayesian approach # model specification y_mx = bf(y ~ 1 + m + x) m_x = bf(m ~ 1 + x) # fit the model fit.brm_mediation = brm( formula = y_mx + m_x + set_rescor(FALSE), data = df.mediation, file = &quot;cache/brm_mediation&quot;, seed = 1 ) # summarize the result fit.brm_mediation %&gt;% summary() ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: y ~ 1 + m + x ## m ~ 1 + x ## Data: df.mediation (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## y_Intercept 7.80 5.67 -3.21 18.67 5173 1.00 ## m_Intercept 2.33 5.72 -9.30 13.69 5851 1.00 ## y_m 0.43 0.10 0.23 0.62 4469 1.00 ## y_x -0.11 0.10 -0.31 0.09 4324 1.00 ## m_x 0.66 0.07 0.51 0.81 5922 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma_y 4.80 0.35 4.18 5.58 5747 1.00 ## sigma_m 4.91 0.36 4.27 5.71 5195 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). set_rescor(FALSE) makes it such that the residual correlation between the response variables is not modeled. # check inference fit.brm_mediation %&gt;% plot() Looks pretty solid! Let’s get the posterior samples df.samples = fit.brm_mediation %&gt;% posterior_samples() %&gt;% mutate(ab = b_m_x * b_y_m, total = ab + b_y_x) And visualize the posterior: df.samples %&gt;% select(ab, total) %&gt;% gather(&quot;effect&quot;, &quot;value&quot;) %&gt;% ggplot(aes(y = effect, x = value)) + geom_halfeyeh() + coord_cartesian(ylim = c(1.5, 2.3)) Let’s also get some summaries of the posterior (MAP with highest density intervals). df.samples %&gt;% select(ab, total) %&gt;% gather(&quot;effect&quot;, &quot;value&quot;) %&gt;% group_by(effect) %&gt;% mode_hdi(value) %&gt;% clean_names() ## # A tibble: 2 x 7 ## effect value lower upper width point interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ab 0.287 0.139 0.426 0.95 mode hdi ## 2 total 0.178 0.0131 0.333 0.95 mode hdi 24.4 Moderation Figure 24.2: Basic moderation model. Moderation can be tested by looking for significant interactions between the moderating variable (Z) and the IV (X). Notably, it is important to mean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. 24.4.1 Generate data # make example reproducible set.seed(123) # number of participants n = 100 df.moderation = tibble( x = abs(rnorm(n, 6, 4)), # hours of sleep x1 = abs(rnorm(n, 60, 30)), # adding some systematic variance to our DV z = rnorm(n, 30, 8), # ounces of coffee consumed y = abs((-0.8 * x) * (0.2 * z) - 0.5 * x - 0.4 * x1 + 10 + rnorm(n, 0, 3)) # attention Paid ) 24.4.2 Moderation analysis # scale the predictors df.moderation = df.moderation %&gt;% mutate_at(vars(x, z), ~ scale(.)[,]) # run regression model with interaction fit.moderation = lm(formula = y ~ 1 + x * z, data = df.moderation) # summarize result fit.moderation %&gt;% summary() ## ## Call: ## lm(formula = y ~ 1 + x * z, data = df.moderation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.466 -8.972 -0.233 6.180 38.051 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.544 1.173 41.390 &lt; 2e-16 *** ## x 17.863 1.196 14.936 &lt; 2e-16 *** ## z 8.393 1.181 7.108 2.08e-10 *** ## x:z 6.094 1.077 5.656 1.59e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.65 on 96 degrees of freedom ## Multiple R-squared: 0.7661, Adjusted R-squared: 0.7587 ## F-statistic: 104.8 on 3 and 96 DF, p-value: &lt; 2.2e-16 24.4.2.1 Visualize result # generate data grid with three levels of the moderator df.newdata = df.moderation %&gt;% expand(x = c(min(x), max(x)), z = c(mean(z) - sd(z), mean(z), mean(z) + sd(z))) %&gt;% mutate(moderator = rep(c(&quot;low&quot;, &quot;average&quot;, &quot;high&quot;), nrow(.)/3)) # predictions for the three levels of the moderator df.prediction = fit.moderation %&gt;% augment(newdata = df.newdata) %&gt;% mutate(moderator = factor(moderator, levels = c(&quot;high&quot;, &quot;average&quot;, &quot;low&quot;))) # visualize the result df.moderation %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(y = .fitted, group = moderator, color = moderator), data = df.prediction, size = 1) + labs(x = &quot;hours of sleep (z-scored)&quot;, y = &quot;attention paid&quot;, color = &quot;coffee consumed&quot;) + scale_color_brewer(palette = &quot;Set1&quot;) df.prediction %&gt;% head(9) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) x z moderator .fitted .se.fit -1.83 -1 low 18.58 3.75 -1.83 0 average 15.80 2.51 -1.83 1 high 13.02 2.99 2.41 -1 low 68.52 4.32 2.41 0 average 91.60 3.09 2.41 1 high 114.68 4.12 24.5 Additional resources 24.5.1 Books Introduction to Mediation, Moderation, and Conditional Process Analysis (Second Edition): A Regression-Based Approach Recoded with BRMS and Tidyverse 24.5.2 Tutorials R tutorial on mediation and moderation R tutorial on moderated mediation Path analysis with brms References "],
["cheatsheets-6.html", "Chapter 25 Cheatsheets 25.1 Statistics 25.2 R", " Chapter 25 Cheatsheets This chapter contains a selection of useful cheatsheets. For updates check here: https://www.rstudio.com/resources/cheatsheets/ Most of the cheatsheets have more than one page. To see the full cheatsheet, rightclick on it and select Open image in New Window 25.1 Statistics Figure 25.1: Stats cheatsheet 25.2 R Figure 25.2: Data wrangling in the tidyverse Figure 25.3: advancedr Figure 25.4: base-r Figure 25.5: data-import Figure 25.6: data-transformation Figure 25.7: data-visualization Figure 25.8: how-big-is-your-graph Figure 25.9: latexsheet Figure 25.10: leaflet Figure 25.11: lubridate Figure 25.12: mosaic Figure 25.13: purrr Figure 25.14: regexcheatsheet Figure 25.15: rmarkdown-reference Figure 25.16: rmarkdown Figure 25.17: rstudio-ide Figure 25.18: shiny Figure 25.19: strings Figure 25.20: syntax Figure 25.21: tidyeval Figure 25.22: visualization principles "],
["references.html", "References", " References "]
]
