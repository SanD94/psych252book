[
["bayesian-data-analysis-3.html", "Chapter 23 Bayesian data analysis 3 23.1 Learning goals 23.2 Load packages and set plotting theme 23.3 Load data set 23.4 Poker 23.5 Dealing with heteroscedasticity 23.6 Ordinal regression 23.7 Additional resources 23.8 Session info", " Chapter 23 Bayesian data analysis 3 23.1 Learning goals Building Bayesian models with brms. Model evaluation: Visualizing and interpreting results. Testing hypotheses. Inference evaluation: Did things work out? Some cool examples: Evidence for null results. Dealing with unequal variance. Zero-one inflated beta binomial model. Ordinal logistic regression. Regression with strictly positive weights. 23.2 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;modelr&quot;) # for doing modeling stuff library(&quot;tidybayes&quot;) # tidying up results from Bayesian models library(&quot;brms&quot;) # Bayesian regression models with Stan library(&quot;rstanarm&quot;) # for Bayesian models library(&quot;cowplot&quot;) # for making figure panels library(&quot;ggrepel&quot;) # for labels in ggplots library(&quot;gganimate&quot;) # for animations library(&quot;GGally&quot;) # for pairs plot library(&quot;bayesplot&quot;) # for visualization of Bayesian model fits library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. opts_chunk$set( comment = &quot;&quot;, results = &quot;hold&quot;, fig.show = &quot;hold&quot; ) theme_set( theme_classic() + #set the theme theme(text = element_text(size = 20)) #set the default text size ) 23.3 Load data set Load the poker data set. df.poker = read_csv(&quot;data/poker.csv&quot;) %&gt;% mutate(skill = factor(skill, levels = 1:2, labels = c(&quot;expert&quot;, &quot;average&quot;)), skill = fct_relevel(skill, &quot;average&quot;, &quot;expert&quot;), hand = factor(hand, levels = 1:3, labels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;)), limit = factor(limit, levels = 1:2, labels = c(&quot;fixed&quot;, &quot;none&quot;)), participant = 1:n()) %&gt;% select(participant, everything()) Parsed with column specification: cols( skill = col_double(), hand = col_double(), limit = col_double(), balance = col_double() ) 23.4 Poker 23.4.1 Visualization Let’s visualize the data first: df.poker %&gt;% ggplot(mapping = aes(x = hand, y = balance, fill = hand)) + geom_point(alpha = 0.2, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, size = 4) + labs(y = &quot;final balance (in Euros)&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + theme(legend.position = &quot;none&quot;) 23.4.2 Linear model And let’s now fit a simple (frequentist) regression model: fit.lm = lm(formula = balance ~ 1 + hand, data = df.poker) fit.lm %&gt;% summary() Call: lm(formula = balance ~ 1 + hand, data = df.poker) Residuals: Min 1Q Median 3Q Max -12.9264 -2.5902 -0.0115 2.6573 15.2834 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.9415 0.4111 14.451 &lt; 2e-16 *** handneutral 4.4051 0.5815 7.576 4.55e-13 *** handgood 7.0849 0.5815 12.185 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.111 on 297 degrees of freedom Multiple R-squared: 0.3377, Adjusted R-squared: 0.3332 F-statistic: 75.7 on 2 and 297 DF, p-value: &lt; 2.2e-16 23.4.3 Bayesian model Now, let’s fit a Bayesian regression model using the brm() function: fit.brm1 = brm(formula = balance ~ 1 + hand, data = df.poker, file = &quot;cache/brm1&quot;) fit.brm1 %&gt;% summary() Family: gaussian Links: mu = identity; sigma = identity Formula: balance ~ 1 + hand Data: df.poker (Number of observations: 300) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 5.95 0.42 5.12 6.79 1.00 3689 3116 handneutral 4.38 0.59 3.24 5.56 1.00 3493 3258 handgood 7.07 0.59 5.94 8.22 1.00 3572 2857 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 4.13 0.17 3.81 4.47 1.00 3586 2876 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). I use the file = argument to save the model’s results so that when I run this code chunk again, the model doesn’t need to be fit again (fitting Bayesian models takes a while …). 23.4.3.1 Visualize the posteriors Let’s visualize what the posterior for the different parameters looks like. We use the geom_halfeyeh() function from the “tidybayes” package to do so: fit.brm1 %&gt;% posterior_samples() %&gt;% select(-lp__) %&gt;% gather(&quot;variable&quot;, &quot;value&quot;) %&gt;% ggplot(data = ., mapping = aes(y = variable, x = value)) + geom_halfeyeh() And let’s look at how the samples from the posterior are correlated with each other: fit.brm1 %&gt;% posterior_samples() %&gt;% select(b_Intercept:sigma) %&gt;% ggpairs(lower = list(continuous = wrap(&quot;points&quot;, alpha = 0.03)), upper = list(continuous = wrap(&quot;cor&quot;, size = 6))) + theme(panel.grid.major = element_blank(), text = element_text(size = 12)) 23.4.3.2 Compute highest density intervals To compute the MAP (maximum a posteriori probability) estimate and highest density interval, we use the mode_hdi() function that comes with the “tidybayes” package. fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(starts_with(&quot;b_&quot;), sigma) %&gt;% mode_hdi() %&gt;% gather(&quot;index&quot;, &quot;value&quot;, -c(.width:.interval)) %&gt;% select(index, value) %&gt;% mutate(index = ifelse(str_detect(index, fixed(&quot;.&quot;)), index, str_c(index, &quot;.mode&quot;))) %&gt;% separate(index, into = c(&quot;parameter&quot;, &quot;type&quot;), sep = &quot;\\\\.&quot;) %&gt;% spread(type, value) %&gt;% kable(digits = 2) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) parameter lower mode upper b_handgood 5.93 7.10 8.20 b_handneutral 3.19 4.42 5.47 b_intercept 5.11 6.02 6.78 sigma 3.81 4.10 4.46 23.4.3.3 Posterior predictive check To check whether the model did a good job capturing the data, we can simulate what future data the Baysian model predicts, now that it has learned from the data we feed into it. pp_check(fit.brm1, nsamples = 100) This looks good! The predicted shaped of the data based on samples from the posterior distribution looks very similar to the shape of the actual data. Let’s make a hypothetical outcome plot that shows what concrete data sets the model would predict: # generate predictive samples df.predictive_samples = fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(contains(&quot;b_&quot;), sigma) %&gt;% sample_n(size = 20) %&gt;% mutate(sample = 1:n()) %&gt;% group_by(sample) %&gt;% nest() %&gt;% mutate(bad = map(data, ~ .$b_intercept + rnorm(100, sd = .$sigma)), neutral = map(data, ~ .$b_intercept + .$b_handneutral + rnorm(100, sd = .$sigma)), good = map(data, ~ .$b_intercept + .$b_handgood + rnorm(100, sd = .$sigma))) %&gt;% unnest(c(bad, neutral, good)) # plot the results as an animation df.predictive_samples %&gt;% select(-data) %&gt;% gather(&quot;hand&quot;, &quot;balance&quot;, -sample) %&gt;% mutate(hand = factor(hand, levels = c(&quot;bad&quot;, &quot;neutral&quot;, &quot;good&quot;))) %&gt;% ggplot(mapping = aes(x = hand, y = balance, fill = hand)) + geom_point(alpha = 0.2, position = position_jitter(height = 0, width = 0.1)) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, geom = &quot;linerange&quot;, size = 1) + stat_summary(fun.y = &quot;mean&quot;, geom = &quot;point&quot;, shape = 21, size = 4) + labs(y = &quot;final balance (in Euros)&quot;) + scale_fill_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;green&quot;)) + theme(legend.position = &quot;none&quot;) + transition_manual(sample) # animate(p, nframes = 120, width = 800, height = 600, res = 96, type = &quot;cairo&quot;) # anim_save(&quot;poker_posterior_predictive.gif&quot;) 23.4.3.4 Test hypothesis One key advantage of Bayesian over frequentist analysis is that we can test hypothesis in a very flexible manner by directly probing our posterior samples in different ways. We may ask, for example, what the probability is that the parameter for the difference between a bad hand and a neutral hand (b_handneutral) is greater than 0. Let’s plot the posterior distribution together with the criterion: fit.brm1 %&gt;% posterior_samples() %&gt;% select(b_handneutral) %&gt;% gather(&quot;variable&quot;, &quot;value&quot;) %&gt;% ggplot(data = ., mapping = aes(y = variable, x = value)) + geom_halfeyeh() + geom_vline(xintercept = 0, color = &quot;red&quot;) We see that the posterior is definitely greater than 0. We can ask many different kinds of questions about the data by doing basic arithmetic on our posterior samples. The hypothesis() function makes this even easier. Here are some examples: # the probability that the posterior for handneutral is less than 0 hypothesis(fit.brm1, hypothesis = &quot;handneutral &lt; 0&quot;) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (handneutral) &lt; 0 4.38 0.59 3.44 5.36 0 Post.Prob Star 1 0 --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that the posterior for handneutral is greater than 4 hypothesis(fit.brm1, hypothesis = &quot;handneutral &gt; 4&quot;) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (handneutral)-(4) &gt; 0 0.38 0.59 -0.56 1.36 2.89 Post.Prob Star 1 0.74 --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that good hands make twice as much as bad hands hypothesis(fit.brm1, hypothesis = &quot;Intercept + handgood &gt; 2 * Intercept&quot;) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (Intercept+handgo... &gt; 0 1.12 0.94 -0.42 2.66 7.6 Post.Prob Star 1 0.88 --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. # the probability that neutral hands make less than the average of bad and good hands hypothesis(fit.brm1, hypothesis = &quot;Intercept + handneutral &lt; (Intercept + Intercept + handgood) / 2&quot;) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (Intercept+handne... &lt; 0 0.85 0.49 0.05 1.66 0.04 Post.Prob Star 1 0.04 --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. Let’s double check one example, and calculate the result directly based on the posterior samples: df.hypothesis = fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(neutral = b_intercept + b_handneutral, bad_good_average = (b_intercept + b_intercept + b_handgood)/2, hypothesis = neutral &lt; bad_good_average) df.hypothesis %&gt;% summarize(p = sum(hypothesis)/n()) p 1 0.04175 23.4.3.4.1 Multiple comparisons We can also use the hypothesis() function to test multiple hypotheses at the same time. h = c(&quot;Intercept + handneutral &lt; (Intercept + Intercept + handgood) / 2&quot;, &quot;handneutral &gt; 4&quot;) hyp = hypothesis(fit.brm1, hypothesis = h, alpha = 0.025) hyp plot(hyp) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (Intercept+handne... &lt; 0 0.85 0.49 -0.11 1.82 0.04 2 (handneutral)-(4) &gt; 0 0.38 0.59 -0.76 1.56 2.89 Post.Prob Star 1 0.04 2 0.74 --- &#39;CI&#39;: 95%-CI for one-sided and 97.5%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 97.5%; for two-sided hypotheses, the value tested against lies outside the 97.5%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. Here, I’ve adjusted the alpha value which affects what credible interval is used to evaluate each hypothesis. An alternative way of accounting for multiple comparisons within the Baysian framework is by constructing a joint posterior based on the hypotheses of interest, and looking at whether the credible interval of the joint posterior excludes 0 (Gelman and Tuerlinckx 2000). fit.brm1 %&gt;% posterior_samples() %&gt;% clean_names() %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mutate(hypothesis_1 = b_intercept &gt; 2, hypothesis_2 = b_handneutral &gt; 3, hypotheses_joint = hypothesis_1 * hypothesis_2 ) %&gt;% summarize(hyp1 = sum(hypothesis_1)/n(), hyp2 = sum(hypothesis_2)/n(), hyp_joint = sum(hypotheses_joint)/n()) hyp1 hyp2 hyp_joint 1 1 0.9925 0.9925 23.4.3.5 Bayes factor Another way of testing hypothesis is via the Bayes factor. Let’s fit the two models we are interested in comparing with each other: fit.brm2 = brm(formula = balance ~ 1 + hand, data = df.poker, save_all_pars = T, file = &quot;cache/brm2&quot;) fit.brm3 = brm(formula = balance ~ 1 + hand + skill, data = df.poker, save_all_pars = T, file = &quot;cache/brm3&quot;) And then compare the models useing the bayes_factor() function: bayes_factor(fit.brm3, fit.brm2) Iteration: 1 Iteration: 2 Iteration: 3 Iteration: 4 Iteration: 5 Iteration: 1 Iteration: 2 Iteration: 3 Iteration: 4 Iteration: 5 Estimated Bayes factor in favor of bridge1 over bridge2: 3.80160 23.4.3.6 Full specification So far, we have used the defaults that brm() comes with and not bothered about specifiying the priors, etc. 23.4.3.6.1 Getting the priors Notice that we didn’t specify any priors in the model. By default, “brms” assigns weakly informative priors to the parameters in the model. We can see what these are by running the following command: fit.brm1 %&gt;% prior_summary() prior class coef group resp dpar nlpar bound 1 b 2 b handgood 3 b handneutral 4 student_t(3, 10, 10) Intercept 5 student_t(3, 0, 10) sigma We can also get information about which priors need to be specified before fitting a model: get_prior(formula = balance ~ 1 + hand, family = &quot;gaussian&quot;, data = df.poker) prior class coef group resp dpar nlpar bound 1 b 2 b handgood 3 b handneutral 4 student_t(3, 10, 10) Intercept 5 student_t(3, 0, 10) sigma Here is an example for what a more complete model specification could look like: fit.brm4 = brm( formula = balance ~ 1 + hand, family = &quot;gaussian&quot;, data = df.poker, prior = c( prior(normal(0, 10), class = &quot;b&quot;, coef = &quot;handgood&quot;), prior(normal(0, 10), class = &quot;b&quot;, coef = &quot;handneutral&quot;), prior(student_t(3, 3, 10), class = &quot;Intercept&quot;), prior(student_t(3, 0, 10), class = &quot;sigma&quot;) ), inits = list( list(Intercept = 0, sigma = 1, handgood = 5, handneutral = 5), list(Intercept = -5, sigma = 3, handgood = 2, handneutral = 2), list(Intercept = 2, sigma = 1, handgood = -1, handneutral = 1), list(Intercept = 1, sigma = 2, handgood = 2, handneutral = -2) ), iter = 4000, warmup = 1000, chains = 4, file = &quot;cache/brm4&quot;, seed = 1 ) fit.brm4 %&gt;% summary() Family: gaussian Links: mu = identity; sigma = identity Formula: balance ~ 1 + hand Data: df.poker (Number of observations: 300) Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; total post-warmup samples = 12000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 5.96 0.41 5.15 6.76 1.00 9139 8795 handneutral 4.37 0.58 3.23 5.53 1.00 9638 8040 handgood 7.05 0.58 5.93 8.19 1.00 9782 8367 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 4.13 0.17 3.81 4.49 1.00 12982 9331 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). We can also take a look at the Stan code that the brm() function creates: fit.brm4 %&gt;% stancode() // generated with brms 2.7.0 functions { } data { int&lt;lower=1&gt; N; // total number of observations vector[N] Y; // response variable int&lt;lower=1&gt; K; // number of population-level effects matrix[N, K] X; // population-level design matrix int prior_only; // should the likelihood be ignored? } transformed data { int Kc = K - 1; matrix[N, K - 1] Xc; // centered version of X vector[K - 1] means_X; // column means of X before centering for (i in 2:K) { means_X[i - 1] = mean(X[, i]); Xc[, i - 1] = X[, i] - means_X[i - 1]; } } parameters { vector[Kc] b; // population-level effects real temp_Intercept; // temporary intercept real&lt;lower=0&gt; sigma; // residual SD } transformed parameters { } model { vector[N] mu = temp_Intercept + Xc * b; // priors including all constants target += normal_lpdf(b[1] | 0, 10); target += normal_lpdf(b[2] | 0, 10); target += student_t_lpdf(temp_Intercept | 3, 3, 10); target += student_t_lpdf(sigma | 3, 0, 10) - 1 * student_t_lccdf(0 | 3, 0, 10); // likelihood including all constants if (!prior_only) { target += normal_lpdf(Y | mu, sigma); } } generated quantities { // actual population-level intercept real b_Intercept = temp_Intercept - dot_product(means_X, b); } One thing worth noticing: by default, “brms” centers the predictors which makes it easier to assign a default prior over the intercept. 23.4.3.7 Inference diagnostics So far, we’ve assumed that the inference has worked out. We can check this by running plot() on our brm object: plot(fit.brm1) Let’s make our own version of a trace plot for one parameter in the model: fit.brm1 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() We can also take a look at the auto-correlation plot. Ideally, we want to generate independent samples from the posterior. So we don’t want subsequent samples to be strongly correlated with each other. Let’s take a look: variables = fit.brm1 %&gt;% get_variables() %&gt;% .[1:4] fit.brm1 %&gt;% posterior_samples() %&gt;% mcmc_acf(pars = variables, lags = 4) Looking good! The autocorrelation should become very small as the lag increases (indicating that we are getting independent samples from the posterior). 23.4.3.7.1 When things go wrong Let’s try to fit a model to very little data (just two observations) with extremely uninformative priors: df.data = tibble(y = c(-1, 1)) fit.brm5 = brm( data = df.data, family = gaussian, formula = y ~ 1, prior = c( prior(uniform(-1e10, 1e10), class = Intercept), prior(uniform(0, 1e10), class = sigma) ), inits = list( list(Intercept = 0, sigma = 1), list(Intercept = 0, sigma = 1) ), iter = 4000, warmup = 1000, chains = 2, file = &quot;cache/brm5&quot; ) Let’s take a look at the posterior distributions of the model parameters: summary(fit.brm5) Warning: The model has not converged (some Rhats are &gt; 1.1). Do not analyse the results! We recommend running more iterations and/or setting stronger priors. Warning: There were 225 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup Family: gaussian Links: mu = identity; sigma = identity Formula: y ~ 1 Data: df.data (Number of observations: 2) Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; total post-warmup samples = 6000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Intercept -51502175.28 484211604.68 -1258318917.93 524975203.26 1.63 Bulk_ESS Tail_ESS Intercept 106 63 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS sigma 398611802.92 1217973385.49 852.93 4284866883.47 1.58 3 Tail_ESS sigma 87 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Not looking good – The estimates and credible intervals are off the charts. And the effective samples sizes in the chains are very small. Let’s visualize the trace plots: plot(fit.brm5) fit.brm5 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() Given that we have so little data in this case, we need to help the model a little bit by providing some slighlty more specific priors. fit.brm6 = brm( data = df.data, family = gaussian, formula = y ~ 1, prior = c( prior(normal(0, 10), class = Intercept), # more reasonable priors prior(cauchy(0, 1), class = sigma) ), iter = 4000, warmup = 1000, chains = 2, seed = 1, file = &quot;cache/brm6&quot; ) Let’s take a look at the posterior distributions of the model parameters: summary(fit.brm6) Warning: There were 3 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup Family: gaussian Links: mu = identity; sigma = identity Formula: y ~ 1 Data: df.data (Number of observations: 2) Samples: 2 chains, each with iter = 4000; warmup = 1000; thin = 1; total post-warmup samples = 6000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept -0.13 1.69 -4.10 3.06 1.00 1352 909 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 2.04 1.88 0.61 6.95 1.00 1144 1426 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). This looks much better. There is still quite a bit of uncertainty in our paremeter estimates, but it has reduced dramatically. Let’s visualize the trace plots: plot(fit.brm6) fit.brm6 %&gt;% spread_draws(b_Intercept) %&gt;% clean_names() %&gt;% mutate(chain = as.factor(chain)) %&gt;% ggplot(aes(x = iteration, y = b_intercept, group = chain, color = chain)) + geom_line() Looking mostly good – except for one hiccup on sigma … 23.5 Dealing with heteroscedasticity Let’s generate some fake developmental data where the variance in the data is greatest for young children, smaller for older children, and even smaller for adults: # make example reproducible set.seed(0) df.variance = tibble( group = rep(c(&quot;3yo&quot;, &quot;5yo&quot;, &quot;adults&quot;), each = 20), response = rnorm(60, mean = rep(c(0, 5, 8), each = 20), sd = rep(c(3, 1.5, 0.3), each = 20)) ) df.variance %&gt;% ggplot(aes(x = group, y = response)) + geom_jitter(height = 0, width = 0.1, alpha = 0.7) While frequentist models (such as a linear regression) assume equality of variance, Baysian models afford us with the flexibility of inferring both the parameter estimates of the groups (i.e. the means and differences between the means), as well as the variances. We simply define a multivariate model which tries to fit both the response as well as the variance sigma: fit.brm7 = brm( formula = bf(response ~ group, sigma ~ group), data = df.variance, file = &quot;cache/brm7&quot; ) Let’s take a look at the model output: summary(fit.brm7) Family: gaussian Links: mu = identity; sigma = log Formula: response ~ group sigma ~ group Data: df.variance (Number of observations: 60) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Intercept 0.00 0.72 -1.38 1.45 1.00 1298 sigma_Intercept 1.15 0.17 0.85 1.51 1.00 2171 group5yo 5.16 0.77 3.60 6.62 1.00 1450 groupadults 7.96 0.72 6.49 9.37 1.00 1300 sigma_group5yo -1.05 0.23 -1.51 -0.59 1.00 2362 sigma_groupadults -2.19 0.23 -2.65 -1.74 1.00 2264 Tail_ESS Intercept 1797 sigma_Intercept 1953 group5yo 1929 groupadults 1710 sigma_group5yo 2489 sigma_groupadults 2598 Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). And let’s visualize the results: df.variance %&gt;% expand(group) %&gt;% add_fitted_draws(fit.brm7, dpar = TRUE) %&gt;% select(group, .row, .draw, posterior = .value, mu, sigma) %&gt;% gather(&quot;index&quot;, &quot;value&quot;, c(mu, sigma)) %&gt;% ggplot(aes(x = value, y = group)) + geom_halfeyeh() + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + facet_grid(cols = vars(index)) This plot shows what the posterior looks like for both mu (the inferred means), and for sigma (the inferred variances) for the different groups. 23.6 Ordinal regression For more information, see this tutorial. While running an ordinal regression is far from trivial in frequentist world, it’s easy to do using “brms”. Let’s load the cars data and turn the number of cylinders into an ordered factor: df.cars = mtcars %&gt;% mutate(cyl = ordered(cyl)) # creates an ordered factor Let’s check that the cylinders are indeed ordered now: df.cars %&gt;% str() &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : Ord.factor w/ 3 levels &quot;4&quot;&lt;&quot;6&quot;&lt;&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... fit.brm8 = brm(formula = cyl ~ mpg, data = df.cars, family = &quot;cumulative&quot;, file = &quot;cache/brm8&quot;, seed = 1) Visualize the results: data_plot = df.cars %&gt;% ggplot(aes(x = mpg, y = cyl, color = cyl)) + geom_point() + scale_color_brewer(palette = &quot;Dark2&quot;, name = &quot;cyl&quot;) fit_plot = df.cars %&gt;% data_grid(mpg = seq_range(mpg, n = 101)) %&gt;% add_fitted_draws(fit.brm8, value = &quot;P(cyl | mpg)&quot;, category = &quot;cyl&quot;) %&gt;% ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) + stat_lineribbon(aes(fill = cyl), alpha = 1/5, .width = c(0.95)) + scale_color_brewer(palette = &quot;Dark2&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;) plot_grid(ncol = 1, align = &quot;v&quot;, data_plot, fit_plot ) Posterior predictive check: df.cars %&gt;% select(mpg) %&gt;% add_predicted_draws(fit.brm8, prediction = &quot;cyl&quot;, seed = 1234) %&gt;% ggplot(aes(x = mpg, y = cyl)) + geom_count(color = &quot;gray75&quot;) + geom_point(aes(fill = cyl), data = df.cars, shape = 21, size = 2) + scale_fill_brewer(palette = &quot;Dark2&quot;) + geom_label_repel( data = . %&gt;% ungroup() %&gt;% filter(cyl == &quot;8&quot;) %&gt;% filter(mpg == max(mpg)) %&gt;% dplyr::slice(1), label = &quot;posterior predictions&quot;, xlim = c(26, NA), ylim = c(NA, 2.8), point.padding = 0.3, label.size = NA, color = &quot;gray50&quot;, segment.color = &quot;gray75&quot;) + geom_label_repel( data = df.cars %&gt;% filter(cyl == &quot;6&quot;) %&gt;% filter(mpg == max(mpg)) %&gt;% dplyr::slice(1), label = &quot;observed data&quot;, xlim = c(26, NA), ylim = c(2.2, NA), point.padding = 0.2, label.size = NA, segment.color = &quot;gray35&quot;) 23.7 Additional resources Tutorial on visualizing brms posteriors with tidybayes Hypothetical outcome plots Visual MCMC diagnostics How to model slider data the Baysian way Tutorial on constrasts in Bayes Bayesian regression modeling (for factorial designs):A tutorial 23.8 Session info sessionInfo() R version 3.6.1 (2019-07-05) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.3 [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 tidyverse_1.2.1 [9] bayesplot_1.7.0 GGally_1.4.0 gganimate_1.0.3 ggrepel_0.8.1 [13] ggplot2_3.2.1 cowplot_1.0.0 rstanarm_2.18.2 brms_2.10.0 [17] Rcpp_1.0.3 tidybayes_1.1.0 modelr_0.1.5 janitor_1.2.0 [21] kableExtra_1.1.0 knitr_1.25 loaded via a namespace (and not attached): [1] readxl_1.3.1 backports_1.1.5 [3] Hmisc_4.2-0 plyr_1.8.4 [5] igraph_1.2.4.1 lazyeval_0.2.2 [7] splines_3.6.1 svUnit_0.7-12 [9] crosstalk_1.0.0 rstantools_2.0.0 [11] inline_0.3.15 digest_0.6.22 [13] htmltools_0.3.6 rsconnect_0.8.15 [15] checkmate_1.9.4 magrittr_1.5 [17] cluster_2.1.0 matrixStats_0.55.0 [19] xts_0.11-2 prettyunits_1.0.2 [21] colorspace_1.4-1 rvest_0.3.4 [23] haven_2.1.1 xfun_0.9 [25] callr_3.3.2 crayon_1.3.4 [27] jsonlite_1.6 lme4_1.1-21 [29] zeallot_0.1.0 survival_2.44-1.1 [31] zoo_1.8-6 glue_1.3.1 [33] gtable_0.3.0 webshot_0.5.1 [35] pkgbuild_1.0.5 rstan_2.19.2 [37] abind_1.4-5 scales_1.0.0 [39] miniUI_0.1.1.1 htmlTable_1.13.2 [41] viridisLite_0.3.0 xtable_1.8-4 [43] progress_1.2.2 HDInterval_0.2.0 [45] ggstance_0.3.3 foreign_0.8-72 [47] Formula_1.2-3 stats4_3.6.1 [49] StanHeaders_2.19.0 DT_0.9 [51] htmlwidgets_1.3 httr_1.4.1 [53] threejs_0.3.1 arrayhelpers_1.0-20160527 [55] RColorBrewer_1.1-2 ellipsis_0.3.0 [57] acepack_1.4.1 pkgconfig_2.0.3 [59] reshape_0.8.8 loo_2.1.0 [61] farver_2.0.1 nnet_7.3-12 [63] labeling_0.3 tidyselect_0.2.5 [65] rlang_0.4.1 reshape2_1.4.3 [67] later_0.8.0 munsell_0.5.0 [69] cellranger_1.1.0 tools_3.6.1 [71] cli_1.1.0 generics_0.0.2 [73] gifski_0.8.6 broom_0.5.2 [75] ggridges_0.5.1 evaluate_0.14 [77] yaml_2.2.0 processx_3.4.1 [79] nlme_3.1-141 mime_0.7 [81] mvnfast_0.2.5 xml2_1.2.2 [83] compiler_3.6.1 shinythemes_1.1.2 [85] rstudioapi_0.10 png_0.1-7 [87] tweenr_1.0.1 stringi_1.4.3 [89] highr_0.8 ps_1.3.0 [91] Brobdingnag_1.2-6 lattice_0.20-38 [93] Matrix_1.2-17 nloptr_1.2.1 [95] markdown_1.1 shinyjs_1.0 [97] vctrs_0.2.0 pillar_1.4.2 [99] lifecycle_0.1.0 bridgesampling_0.7-2 [101] data.table_1.12.2 httpuv_1.5.2 [103] R6_2.4.1 latticeExtra_0.6-28 [105] bookdown_0.13 promises_1.0.1 [107] gridExtra_2.3 codetools_0.2-16 [109] boot_1.3-23 colourpicker_1.0 [111] MASS_7.3-51.4 gtools_3.8.1 [113] assertthat_0.2.1 withr_2.1.2 [115] shinystan_2.5.0 parallel_3.6.1 [117] hms_0.5.2 rpart_4.1-15 [119] grid_3.6.1 coda_0.19-3 [121] minqa_1.2.4 snakecase_0.11.0 [123] rmarkdown_1.15 shiny_1.3.2 [125] lubridate_1.7.4 base64enc_0.1-3 [127] dygraphs_1.1.1.6 References "]
]
