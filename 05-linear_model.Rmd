# The linear model 

## Resources 

- [modeling data in the tidyverse](https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse)
- [simulating linear models](https://aosmith.rbind.io/2018/01/09/simulate-simulate-part1/)
- [great homework template](http://www.andrew.cmu.edu/user/achoulde/94842/homework/homework5.html)
- [linear regression tutorial](https://rpubs.com/aaronsc32/simple-linear-regression)
- [multiple regression tutorial](https://rpubs.com/aaronsc32/multiple-regression)

## Learning goals 

- general approach: ordinal least squares 
- two-level predictor
- multi-level predictor
- one continuous predictor
  + correlation
- one continuous predictor, and one two-level predictor 
- two continuous predictors 

- x predictors 
- model comparison: 
  + the problem of overfitting
  + likelihood test
  + F-test 
  + AIC and BIC
  + cross-validation 
- model assumptions
  + breaking assumptions and seeing how much it matters
  + QQ plot for assessing normality of residuals (see nice explanation in Julian's book)

## Todo 

- understand the different columns returned by `augment()`
- start with continuous predictor or binary predictor? 
- ordinal predictors as numerical (assumes that differences between categories are the same) vs. dummy-coded 
- centered predictors make it easier to interpret intercept 
- z-scored predictors make it easy to compare how much each predictor mattered in a multiple regression
  + give some intuition on what z-scoring does 
- interpreting interactions:
  + a separate line for each categorical predictor
  + "infinite separate lines" when the other predictor is continuous
- go through anscombe's quartet to illustrate correlations 
- how to deal with highly correlated predictors (fit a model with one predictor first, and then see whether the residual variance can be explained by another predictor)
- interpretation of sum of squares in ANOVA (regression with multiple predictors) --> nice explanation in Julian's book
- change the base category in the regression
- for the QQ plot section: maybe keep the theoretical distribution the same and change the empirical distribution? 

## General points to make 

- statistical routine: 
  1. explore data 
  2. visualize data 
  3. fit model 
  4. interpret model 
  5. evaluate model 
  (repeat 3.-5.)
  
- why squared error? 
  - makes all errors positive (so they don't cancel out when summing)
  - punishes one larger error more than several smaller errors

```{r echo=FALSE, message=FALSE}
# packages 
library("knitr") #for markdown things 
library("broom") #for getting tidy model summaries
library("lme4") #for mixed effects models 
library("patchwork") #for making figure panels
library("janitor") #for cleaning variable names 
library("emmeans") #for comparing estimated marginal means 
library("boot") #for bootstrapping
library("GGally") #for ggpairs() function
library("modelr") #for cross-validation
library("tictoc") #for timing things
library("corrr") #for timing things
library("tidyverse") #for everything else

# knitr options 
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>")

# display options 
options(scipen = 4) # don't use scientific notation in output 

# set ggplot theme
theme_set(
  theme_classic()+
    theme(text = element_text(size = 20)) #text size
  )
```

## General approach 

1. Define a statistical model 
2. Check assumptions of the model 
3. Assess the fit of the model as a whole 
4. Assess direction and magnitude of individual predictors 

## Model fitting 

[Wickham book chapter on modeling basics](https://r4ds.had.co.nz/model-basics.html#a-simple-model)

- find the line that best captures the data (i.e. that minimizes the sum of squared errors)
- use the Wickham example as a first instruction

In general, statistical models take the following form: 

$$
data = model + error 
$$

Here, we try to predict the dependent variable $Y$ as a function of the independent variable $X$ (I'll sometimes refer to $X$ as _predictor_).

$$
Y = f(X) + \epsilon
$$
The linear model. 

$$
Y = \beta_0 + \beta_1 X + \epsilon 
$$




### OLS vs. MLE 

- OLS: 
  - method for approximately determining unknown parameters in a linear regression model
  - minimize sums of squared errors 
- MLE: 
  - estimate parameters of a statistical model 
  - inlcudes least-absolute deviations (L1-norm) and least squares (l2-norm)

- minimizing squared error is equivalent to maximizing the likelihood 

## One binary predictor

$$
y = \beta_0 + \beta_1 \cdot condition + \epsilon
$$

- regression predicts the mean value for each group (mean minimizes the sum of squared errors)
- t-test then tests whether the two averages are different from each other 
- dummy coding for factors as predictors


```{r}
set.seed(0) #to make example reproducible

df.data = data_frame(
  condition = rep(c("1","2"), each = 40),
  response = c(rnorm(40, mean = 8, sd = 2),
              rnorm(40, mean = 7, sd = 2))
)

# show top five and bottom five rows of data 
head(df.data, 5) %>% 
  bind_rows(tail(df.data, 5))

```


```{r}
# alternative way that generates data using the regression formula

# set.seed(0) #to make example reproducible
# n_participants = 80 
# 
# alpha = 5
# beta = 2
# 
# df.data = data_frame(
#   participant = 1:n_participants,
#   condition = rep(0:1, each = n_participants/2),
#   error = rnorm(n = n_participants, sd = 1),
#   response = alpha + beta * condition + error
# ) %>% 
#   mutate(condition = factor(condition, levels = 0:1, labels = c("condition1", "condition2")))
# 
# # show top five and bottom five rows of data 
# rbind(head(df.data),
#       tail(df.data)
#       )
```

Here, I've generated samples from two hypothetical conditions. For Condition 1, I've sampled from a normal distribution with a mean of 8 and a standard deviation of two. For Condition 2, I've used the same standard deviation but a mean of 7.

This is what the two distributions look like: 

```{r normal-overlap, fig.cap="Two normal distributions with different means but the same variance."}
ggplot(data_frame(x = c(0, 15)), aes(x))+
  stat_function(fun = dnorm, args = list(mean = 8, sd = 2), geom = "area", fill = "red", color = "black", alpha = 0.1)+
  stat_function(fun = dnorm, args = list(mean = 7, sd = 2), geom = "area", fill = "blue", color = "black", alpha = 0.1)
```

As we see, the overlap a lot. Because both of the distributions have the same standard deviation, we can calculate the overlap in the following way: 
```{r}
mu1 = 7
mu2 = 8
sd = 2
d = abs((7-8)/sd)
2 * pnorm(-d/2)
```

So, 80% of the two distributions overlap. To double check, let's get the result through simulation. 

```{r}
set.seed(0)

# parameters 
mu1 = 7
sd1 = 2 

mu2 = 8
sd2 = 2 

nsamples = 1000

#generate samples 
samples1 = rnorm(n = nsamples, mean = mu1, sd = sd1)
samples2 = rnorm(n = nsamples, mean = mu2, sd = sd2)

df.histogram = data_frame(
  hist1 = hist(samples1, plot = F, breaks = -5:15)$count,
  hist2 = hist(samples2, plot = F, breaks = -5:15)$count) %>% 
  mutate_all(funs(./sum(.))) %>% #normalize each distribution
  rowwise() %>% #group by rows 
  mutate(overlap = min(hist1, hist2)) %>% #we calculate the overlap by getting the minimum of both distributions
  ungroup()

#overlap 
sum(df.histogram$overlap) 
```

Pretty close! Here, I've sampled 1000 values from each distribution, build a histogram based on the samples for each distribution, normalized each distribution, and then calculated the overlap of both distributions by taking the minimum value for each bin and then summing up. Why the minimum? Take a look at \@ref(fig:normal-overlap). Can you see that if we go along the x-axis from left to right, and take the minimum of both distributions at each point, we calculate the overlap of both distributions (i.e. the purple part). 

Let's see what our hypothetical data for the two conditions looks like. 

```{r}
set.seed(0)

ggplot(data = df.data, aes(x = condition, y = response))+
  stat_summary(fun.y = mean, geom = 'bar', color = 'black', fill = "gray80")+
  stat_summary(fun.data = mean_cl_boot, geom = 'linerange', size = 1)+
  geom_point(position = position_jitter(height = 0, width = 0.1), alpha = 0.4)+
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) #removes padding at bottom 
```

As expected, the mean for Condition 1 is higher than that for Condition 2. However, is that difference significant? There is quite a bit of variance in each condition. One way to find out would be to run the permutation test that we covered earlier. Here, we will use the linear model instead. 

In short, the linear model fits a line to this data that goes through the means of each condition. It then tests whether the slope of the line is significantly different from 0. 

```{r}
fit1 = lm(formula = response ~ condition, data = df.data)
```

I've used the `lm()` (linear model) function to fit the data. This function takes a `formula` and the `data` as input. In the formula, we specify the dependent variable on the left side of the tilde `~` and the independent variable(s) on the right side of the tilde.

A line is defined by an intercept and a slope. Let's see what coefficients the linear model came up with to fit our data.

```{r}
fit1 %>% print()
```

The `print()` function (which can be omitted here) outputs the call as well as the coefficients. The intercept is `r fit1$coef[1]` and the slope is `r fit1$coef[2]`. Note that the `lm()` function used `condition1` as the reference category. Let's plot this line on top of our data. 

```{r}
set.seed(0)

df.plot = df.data %>% 
  mutate(condition = as.numeric(condition) - 1)

ggplot(data = df.plot, aes(x = condition, y = response))+
  stat_summary(fun.y = mean, geom = 'bar', color = 'black', fill = "gray80")+
  stat_summary(fun.data = mean_cl_boot, geom = 'linerange', size = 1)+
  geom_point(position = position_jitter(height = 0, width = 0.1), alpha = 0.4)+
  geom_abline(intercept = fit1$coef[1], slope = fit1$coef[2], color = "red", size = 2)+
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1)))+ #removes padding at bottom 
  scale_x_continuous(breaks = 0:1, labels = 1:2)

```

The line goes right through the means of each condition. Note that to display this right, I've had to move the bars over to the left (so that the middle of the left bar is centered at x = 0). 

```{r}
fit1 %>% summary()
```

```{r}
fit2 = lm(response ~ 1, data = df.data)
fit2 %>% print()
```

We contrast this with a model that tries to fit the data with a single parameter. The best fitting parameter `r fit2$coef[1]` corresponds to the overall mean of the data. 

```{r}
mean(df.data$response)
```

```{r}
tmp.anova = anova(fit1, fit2)
print(tmp.anova)
```

The ANOVA test compares the residual sums of squares (RSS), and takes into account the difference in degrees of freedom (which is a function of the number of parameters in each model). 

We can calculate the residual sums of squares like so: 

```{r}
df.residuals = df.data %>% 
  mutate(mean.overall = mean(response)) %>% 
  group_by(condition) %>% 
  mutate(mean.condition = mean(response)) %>% 
  ungroup() %>% 
  mutate(residual.overall = (response - mean.overall)^2,
         residual.condition = (response - mean.condition)^2)

print(str_c("RSS (overall): ", df.residuals$residual.overall %>% sum() %>% round(2)))
print(str_c("RSS (condition): ", df.residuals$residual.condition %>% sum() %>% round(2)))
```


The difference in the sum of squares follows an F-distribution. 

```{r}
ggplot(data_frame(x = c(0, 15)), aes(x))+
  stat_function(fun = df, args = list(df1 = 1, df2 = 78))+
  geom_vline(xintercept = tmp.anova$F[2], linetype = 2)
```


To get a better sense for what's going on, let's illustrate the residuals for the two different models. 

Here is the model that only considers a global intercept. 

```{r}
set.seed(1)

df.plot = df.data %>% 
  mutate(condition = as.numeric(condition), #make condition numeric 
         condition.jittered = condition + rnorm(nrow(.), sd = 0.2) #add random jitter
         )
  
ggplot(data = df.plot, aes(x = condition.jittered, y = rating))+
  geom_point(aes(color = as.factor(condition)), alpha = 0.4)+
  geom_hline(yintercept = fit2$coef[1], color = "black", size = 1)+
  geom_segment(aes(xend = condition.jittered, yend = fit2$coef[1]), alpha = .2)+
  labs(title = "RSS for intercept only model")+
  scale_color_manual(values = c("blue", "red"))+
  scale_x_continuous(breaks = 1:2, labels = 1:2)+
  scale_y_continuous(breaks = seq(0, 10, 5), labels  = seq(0, 10, 5), limits = c(0, 15))+
  theme(legend.position = "none",
        axis.title.x = element_blank())
```

And here is a model that accounts for a difference between the conditions. 

```{r}
set.seed(1)

df.plot = fit1 %>% 
  augment() %>% #the augment function from the broom package returns data and residuals (and some other things)
  mutate(condition = as.numeric(condition), #make condition numeric 
         condition.jittered = condition + rnorm(nrow(.), sd = 0.2) #add random jitter
         )
  
ggplot(data = df.plot, aes(x = condition.jittered, y = rating))+
  geom_point(aes(color = as.factor(condition)), alpha = 0.4)+
  geom_segment(aes(x = 0.5, xend = 1.5, y = fit1$coef[1], yend = fit1$coef[1]), color = "blue")+
  geom_segment(aes(x = 1.5, xend = 2.5, y = fit1$coef[1] + fit1$coef[2], yend = fit1$coef[1] + fit1$coef[2]), color = "red")+
  geom_segment(aes(xend = condition.jittered, yend = .fitted), alpha = .2)+
  labs(title = "RSS for intercept + predictor model")+
  scale_color_manual(values = c("blue", "red"))+
  scale_x_continuous(breaks = 1:2, labels = 1:2)+
  scale_y_continuous(breaks = seq(0, 10, 5), labels  = seq(0, 10, 5), limits = c(0, 15))+
  theme(legend.position = "none",
        axis.title.x = element_blank())
```

Just to be clear, there are really only two values here on the x-axis. I've just moved the points so as to better illustrate how the residuals are calculated. 

So, the basic question here is: how much are we reducing the error of the model, if we assume allow for a difference between conditions, compared to when we don't allow for that difference. Is the reduction of error statistically significant? 

### Likelihood 

```{r}
# https://rdrr.io/r/stats/sigma.html

# fit = lm(formula = response ~ condition, data = df.data)
fit = lm(formula = response ~ 1, data = df.data)

sigma = sigma(fit)
# sqrt(sum((fit$residuals - mean(fit$residuals))^2) / (nobs(fit) - sum(!is.na(coef(fit)))))

dnorm(fit$residuals, mean = mean(fit$residuals), sd = sigma)  %>% 
  log() %>% 
  sum()

logLik(fit)
```




### Interpreting `summary.lm()` output

`summary()` gives: 
- the call 
- information about how the residuals are distributed 
- coefficients:
  + point estimate
  + standard error 
  + t-value 
  + p-value: test whether a coefficient is different from 0 
- model fit measures: 
  + residual standard error
  + R^2 and adjusted R^2 
  + F-statistic (which tests to what extent the whole model accounts for variation in the data) with p-value 

[interpreting lm summary](https://www.quora.com/How-do-I-interpret-the-summary-of-a-linear-model-in-R)

Point estimate: 

- correlation coefficients (non-standardized)

- interpretation of intercept: 
  + predicted value when all predictors are 0 (sometimes doesn't make sense: in that case, center the predictors to interpret the intercept)
  + when predictors are centered, then intercept is the average value

Coefficient standard error: 

- [this post](https://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression) explains how these standard errors are calculated
- [tutorial on confidence intervals](https://rpubs.com/aaronsc32/regression-confidence-prediction-intervals)

- can be used to calculate confidence intervals 
- and test hypotheses 

```{r}

# how to go from standard error to confidence intervals 
n_obs = nrow(df.data)
fit1 %>%
  tidy() %>%
  cbind(fit1 %>%
          confint_tidy()) %>%
  mutate(ci_low = estimate + std.error * qt(0.025, df = n_obs-2),
         ci_high = estimate + std.error * qt(0.975, df = n_obs-2))


# how to go to confidence interval for each data point 

# tmp = fit1 %>% 
#   augment() %>% 
#   cbind(fit1 %>% 
#           predict(interval = "confidence")) %>% 
#   clean_names() %>% 
#   select(response, condition, fitted, se_fit, lwr, upr) %>% 
#   mutate(ci_low = fitted + se_fit * qt(0.025, df = n()-2),
#          ci_high = fitted + se_fit * qt(0.975, df = n()-2)
#          )

  

```


TODO: illustrate how these can be simulated 

t-value: 

Wald test of the hypothesis that the corresponding regression coefficient is equal to 0. If the errors are normally distributed or the sample size is large enough, then these t statistics are distributed under the null hypothesis as t random variables with degrees of freedom (df ) equal to the residual df under the model.

p-value: 

two-sided p-values that are not adjusted for multiple comparisons

```{r}
# get p-values with Bonferroni correction
tmp = fit1 %>% 
  tidy() %>% 
  pull(p.value) %>% 
  p.adjust(method = "bonferroni")
```

- relationship to other measures 

```{r}
fit1 %>%
  tidy() %>%
  clean_names() %>%
  cbind(confint_tidy(fit1)) %>% 
  mutate(cf_low = estimate + std_error * qt(0.025, df = nobs(fit1) - 2),
         cf_high = estimate + std_error * qt(0.975, df = nobs(fit1) - 2),
         stat = estimate / std_error,
         p = 2 * pt(stat, nobs(fit1) - 2, lower = FALSE) #two tailed test 
         )
```

- [how to calculate the likelihood of a model](https://stats.stackexchange.com/questions/73196/recalculate-log-likelihood-from-a-simple-r-lm-model)

```{r}
# binom.test(1, 20, p = 0.5)
# 
# pbinom(1, 20, p = 0.5)*2

df.data = data_frame(
  x = 0:20,
  y = dbinom(x, 20, p = 0.5))

ggplot(data = df.data, aes(x = x, y = y))+
  geom_bar(stat = "identity")+
  geom_vline(xintercept = c(qbinom(0.025, size = 20, prob = 0.5),
                            qbinom(0.975, size = 20, prob = 0.5)),
             linetype = 2)

ggplot(data_frame(x = 0:1), aes(x))+
  stat_function(fun = "qbinom", args = list(size = 20, prob = 0.5))+
  scale_y_continuous(breaks = 0:20)+
  theme(panel.grid.major.y = element_line(linetype = 2))



```



Text in this section is taken from here: [interpreting lm summary output](http://www.learnbymarketing.com/tutorials/explaining-the-lm-summary-in-r/)

```{r}
# Residual Standard error (Like Standard Deviation)
model = fit1

k = length(model$coefficients) - 1 #Subtract one to ignore intercept
SSE = sum(model$residuals^2)
n = length(model$residuals)
sqrt(SSE/(n - (1 + k))) #Residual Standard Error

```

In R, the lm summary produces the standard deviation of the error  with a slight twist.  Standard deviation is the square root of variance.  Standard Error is very similar.  The only difference is that instead of dividing by n-1, you subtract n minus 1 + # of variables involved.

```{r}
#Multiple R-Squared (Coefficient of Determination)
y = df.data$rating
model = fit1

SST = sum((y - mean(y))^2)
SSE = sum((y - model$fitted.values)^2)
1 - SSE / SST
```

Also called the coefficient of determination, this is an oft-cited measurement of how well your model fits to the data.  While there are many issues with using it alone (see Anscombe's quartet), it's a quick and pre-computed check for your model.

R-Squared subtracts the residual error from the variance in Y.  The bigger the error, the worse the remaining variance will appear. If you notice, numerator doesn't have to be positive. If the model is so bad, you can actually end up with a negative R-Squared.

```{r}
#Adjusted R-Squared
n = length(y)
k = length(model$coefficients) - 1 #Subtract one to ignore intercept
SSE = sum(model$residuals^2)
SSyy = sum((y - mean(y))^2)
1 - (SSE / SSyy)*(n - 1)/(n - (k + 1))
```

Multiple R-Squared works great for simple linear (one variable) regression.  However, in most cases, the model has multiple variables. The more variables you add, the more variance you're going to explain. So you have to control for the extra variables.

Adjusted R-Squared normalizes Multiple R-Squared by taking into account how many samples you have and how many variables you're using.

Notice how k is in the denominator.  If you have 100 observations (n) and 5 variables, you'll be dividing by 100-5-1 = 94.  If you have 20 variables instead, you're dividing by 100-20-1 = 79.  As the denominator gets smaller, the results get larger: 99/94 = 1.05; 79/94 = 1.25.

A larger normalizing value is going to make the Adjusted R-Squared worse since we're subtracting its product from one.

```{r}
#F-Statistic
#Ho: All coefficients are zero
#Ha: At least one coefficient is nonzero
#Compare test statistic to F Distribution table
y = df.data$rating
model = fit1

n = length(y)
SSE = sum(model$residuals^2)
SSyy = sum((y - mean(y))^2)
k = length(model$coefficients) - 1
((SSyy - SSE)/k) / (SSE/(n - (k + 1)))
```

Finally, the F-Statistic. Including the t-tests, this is the second "test" that the summary function produces for lm models. The F-Statistic is a "global" test that checks if at least one of your coefficients are nonzero.



```{r}
fit1 %>% confint_tidy()
```

Confidence intervals (95%) on the coefficients. 

```{r}
fit1 %>% fitted()
```

Fitted values. 

```{r}
fit2 = lm(rating ~ 1, data = df.data)
fit2 %>% summary()
```
```{r}
anova(fit1, fit2)
```


The grand mean is `r mean(df.data$rating)` 

(df.data$rating %>% sd()) / sqrt((nrow(df.data))) 

### Meaningful intercepts 

- [Bodo Winter tutorial](http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf)

- center variables 


### Simulating from the statistical model 

CONTINUE HERE 

[simulation example](https://aosmith.rbind.io/2018/01/09/simulate-simulate-part1/)


## One categorical predictor

Let's say we ran a study with three different groups. 

```{r}
set.seed(0)

n_samples = 50

df.data = data_frame(
  condition = rep(c("beaver", "tree", "control"), each = n_samples),
  performance = c(rnorm(n_samples, mean = 55, sd = 10),
                  rnorm(n_samples, mean = 65, sd = 5),
                  rnorm(n_samples, mean = 60, sd = 20))
  ) %>% 
  mutate(condition = factor(condition, levels = c("control", "beaver", "tree")))

```


Let's take a look at the data first:

```{r}
df.plot = df.data 

ggplot(data = df.plot, aes(x = condition, y = performance))+
  geom_point(position = position_jitter(height = 0, width = 0.1), alpha = 0.2)+
  stat_summary(fun.data = "mean_cl_boot", geom = "linerange")+
  stat_summary(fun.y = "mean", geom = "point", size = 3)
```

The plot nicely shows the difference in means between conditions, as well as the difference in variance. Let's first test whether performance scores were overall affected by our manipulation. To do so, we first fit the linear model. 

```{r}
fit1 = lm(performance~condition, data = df.data)
fit2 = lm(performance~1, data = df.data)
```

Now we can use the `anova()` function to test whether a significant proportion of the variance in the data can be explained by our manipulation. 

```{r}
anova(fit1)
```

```{r}
# comparing to a model without any predictors
anova(fit1, fit2)
```

Here, that's the case. 

But let's say what we actually interested in are the differences between conditions. For example, was there any difference between the "control" condition and the "beaver" condition? Let's take a look at the summary of the model: 

```{r}
summary(fit1)
```

<!-- Note: global ANOVA test is reported at the bottom -->

We see that the model now has three coefficients: one for the intercept (`(Intercept`), and two for the condition predictor (`conditionbeaver` and `conditiontree`). 

<!-- write down model formula -->

Let's say that we wanted to compare whether the results in the "beaver" condition differ significantly from the results in the "tree" condition. Logically, this should be the case: the "tree" condition is significantly different from the "control" condition, and the mean in the "control" condition is greater than the mean in the "beaver" condition. But let's check anyhow. 

To do so, we need to change the default value of the condition factor. 

```{r}

df.data = df.data %>%
  mutate(condition = fct_relevel(condition, "beaver")) #move beaver to the front

fit3 = lm(performance ~ condition, data = df.data)

summary(fit3)

```

```{r}
fit3 %>% 
  tidy() %>% 
  mutate(p.value_adjusted = p.adjust(p.value, method = "bonferroni", n = 3)) #adjust p-values for multiple comparisons
```


As expected, the difference between the "beaver" condition and the "tree condition" was significant. 

### Post-hoc tests 

Let's say we would like to have all pairwise comparisons between our three conditions. The `emmeans` package helps us to do so ("emm" stands for estimated marginal means aka least-squares means).

<!-- https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html -->

```{r}
# library("emmeans")

# emmeans(fit3)

```

see this example http://thinkstats.org/practical-example.html

```{r}
# code from: http://thinkstats.org/practical-example.html 
leastsquare <- emmeans(lmResult, 
                      pairwise ~ diet,
                      adjust="tukey")
 
# display the results by grouping using letters
CLD(leastsquare$emmeans, 
    alpha=.05,  
    Letters=letters)
```



### Multiple testing 

- correct for the number of tests

### Contrasts 

- [tutorial](http://www.flutterbys.com.au/stats/tut/tut7.1.html)
- [manual](https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/)
- [course slides](https://www.andrew.cmu.edu/user/achoulde/94842/lectures/lecture10/lecture10-94842.html)
- a way of avoiding multiple comparisons 
- the emphasis is on "planned": so these comparisons should be pre-registered 

```{r}
# dummy coding 

df.dummy = data_frame(
  condition = rep(c("A", "B"), each = 20),
  response = ifelse(condition == "A",
                    rnorm(20, mean = 10, sd = 2),
                    rnorm(20, mean = 20, sd = 2))
)

fit.dummy = lm(response ~ condition, data = df.dummy)
fit.dummy %>% summary()

df.plot = df.dummy %>% 
  mutate(condition = ifelse(condition == "A", 0, 1))

ggplot(data = df.plot, aes(x = condition, y = response))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_smooth(method = "lm", se = F, color = "black")+
  geom_point(position = position_jitter(width = 0.01, height = 0))+
  stat_summary(fun.y = "mean", geom = "point", shape = 21, color = "black", fill = "yellow", size = 4)

```

```{r}
# contrast coding 

fit.contrast = lm(response ~ condition, data = df.dummy)
fit.contrast %>% summary()

df.plot = df.dummy %>% 
  mutate(condition = ifelse(condition == "A", -1, 1))

ggplot(data = df.plot, aes(x = condition, y = response))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_smooth(method = "lm", se = F, color = "black")+
  geom_point(position = position_jitter(width = 0.01, height = 0))+
  stat_summary(fun.y = "mean", geom = "point", shape = 21, color = "black", fill = "yellow", size = 4)
```

```{r}
# proportinal contrast coding 

df.dummy = df.dummy %>% 
  mutate(condition = factor(condition, levels = c("B", "A")))

fit.contrast = lm(response ~ condition, data = df.dummy)
fit.contrast %>% summary()

df.plot = df.dummy %>% 
  mutate(condition = ifelse(condition == "A", -0.5, 0.5))

ggplot(data = df.plot, aes(x = condition, y = response))+
  geom_vline(xintercept = 0, linetype = 2)+
  geom_smooth(method = "lm", se = F, color = "black")+
  geom_point(position = position_jitter(width = 0.01, height = 0))+
  stat_summary(fun.y = "mean", geom = "point", shape = 21, color = "black", fill = "yellow", size = 4)
```


#### Tutorial

- [tutorial](https://pagepiccinini.com/2016/01/08/introduction-and-linear-models-part-1/)
- [tutorial 2](https://pagepiccinini.com/2016/03/18/contrast-coding-with-three-level-variables/)

```{r}
data_threelevel = data_frame(group = c(rep("A", 10), rep("B", 10), rep("C", 10)),
                  score = c(rnorm(10, 40, 5), rnorm(10, 50, 5), rnorm(10, 60, 5)))

data_twolevel = data_threelevel %>%
  filter(group != "C") %>%
  mutate(group = factor(group))

data_twolevel %>% 
  group_by(group) %>% 
  summarize(mean = mean(score),
            sd = sd(score))

# dummy coding
ab_dummy.lm = lm(score ~ group, data_twolevel)
ab_dummy.lm %>% summary()

# contrast coding 
data_twolevel_contrast = data_twolevel %>%
  mutate(contrast_AvB = ifelse(group == "A", -0.5, 0.5))

ab_contrast.lm = lm(score ~ contrast_AvB, data_twolevel_contrast)
ab_contrast.lm %>% summary()


# data_twolevel %>% 
#   summarize(mean = mean(score))

ab_ac_dummy.lm = lm(score ~ group, data_threelevel)
kable(tidy(ab_ac_dummy.lm))

```


## One continuous predictor

Now, let's say that we are interested in the relationship between two continuous variables. 

```{r}
set.seed(0)

# let's make a dataset
n_samples = 100
df.data = data_frame(
  intelligence = rnorm(n_samples, mean = 100, sd = 10),
  score = intelligence/10 + rnorm(n_samples, mean = 0, sd = 2)
)
df.data %>% head()
```

Let's visualize the relationship between intelligence and result: 

```{r}
df.plot = df.data

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point()

```

It looks like intelligence is somewhat predictive of the test score. People with higher intelligence values tend to have higher test scores. Let's fit a linear model again.  

```{r}
fit4 = lm(score ~ intelligence, data = df.data)
fit4 %>% summary()
```

The intercept gives the predicted score when intelligence is 0. This doesn't really make sense here. First, it predicts a test score that is out of the range, and second nobody has an intelligence score of 0. The coefficient means that for each intelligence point, the model predicts an increase of the test score by `r fit4 %>% coef() %>% .[2] %>% round(2)`.  

One thing we can do to make the intercept more meaningful is to center our predictor variable. That is, we subtract the mean 

```{r}

fit4b = lm(score ~ intelligence, 
           data = df.data %>% 
             mutate(intelligence = intelligence - mean(intelligence)))
fit4b %>% summary()
```

The average test score is `r fit5 %>% coef() %>% .[1] %>% round(2)`. Note that the estimate for the "intelligence" coefficient hasn't changed. 

Let's draw the regression line: 

```{r}
df.plot = df.data

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point()+
  geom_abline(intercept = coef(fit4)[1], slope = coef(fit4)[2])

```

Like we have seen when 


```{r}
df.plot = fit4 %>% 
  augment() %>% 
  clean_names()

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point()+
  geom_abline(intercept = coef(fit4)[1], slope = coef(fit4)[2])+
  geom_segment(aes(xend = intelligence, yend = fitted), alpha = .2)
```

Let's compare this with a model that only has a single parameter. 

```{r}
fit5 = lm(score ~ 1, data = df.data)
```

We already know that the single value that minimizes the least squared error is the mean. 

```{r}
df.plot = df.data

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point()+
  geom_hline(yintercept = mean(df.data$score))+
  geom_segment(aes(xend = intelligence, yend = mean(df.data$score)), alpha = .2)
```

Let's compare whether the model with two parameters explains the data significantly better than a model that only has a single parameter. 

```{r}
anova(fit4, fit5)
```

It does! The residual sum of squares is significantly lower for the model that includes "intelligence" as a predictor. 


## Checking model assumptions

### Bodo Winter Tutorial 1 

```{r}
df.pitch = data_frame(
  pitch = c(233, 204, 242, 130, 112, 142),
  sex = rep(c("female", "male"), each = 3)
)
```

```{r}
xmdl = lm(pitch ~ sex, data = df.pitch)
xmdl %>% summary()
```

- "intercept": estimate for the female category 
- "sexmale": estimate for the difference between the female and male category
- what comes first in the alphabet will be chosen as the reference category 


### Linearity

What to do if residual plot indicates non-linearity? 

- maybe you've missed a factor? 
- non-linear transformation of the response variable (e.g. log-transform)
- add non-linear transformation to fixed effects (e.g. predictor^2)
- if there are stripes in the residual plot, the dv might not be continuous --> use a logistic model instead

### Absence of collinearity 

- predictors should not be highly correlated --> interpretation of the model becomes unstable
- don't include highly correlated predictors in the model 
- only include the most meaningful predictor (drop the others) --> don't base this decision on significance! 
- use PCA to combine multiple predictors into one 
- ggpairs 
- corrr

#### `correlate()`

```{r}
df.data = diamonds

df.data %>% 
  filter(row_number() < 100) %>% 
  select(carat, depth, x:z) %>%
  correlate() %>% 
  shave()
```


#### `ggpairs()`

```{r}
df.data = diamonds

df.data %>% 
  filter(row_number() < 100) %>% 
  select(carat, depth, x:z) %>%
  ggpairs(
    lower = list(continuous = wrap("points", alpha = 0.1))
  )+
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )
```



### Homoskedasticity 

- equal variance across the range of predicted values
- residuals should have the same amount of deviation from the predicted values across the whole range
- again, log-transform might help 

### Normality of residuals 

- least important assumption 
- linear models are robust against this assumption 

### Absence of influential data points (outliers)

- can drastically change the interpretation of the results 
- check via the `dfbeta()` function
  - leave-one-out diagnostics 
- run analysis with and without influential points and report both analysis (ideally, the main interpretation does not change)
- only exclude points if: 
  - clear criteria for outliers have been defined a-priori (ideally, pre-registered)
  - it's an obvious error (e.g. transcribed incorrectly)
  
```{r}
dfbeta(xmdl)
```
  
### Independence 

- by far the most important assumption
- violating independence may greatly inflate your chance of finding a spurious result 
- results in p-values that are meaningless 
- independence is determined by the experimental design 
- this is where mixed effects models will come in: 
  - in most of our experiments we have participants make multiple judgments


#### Analyzing residuals 

QQ plots, or quantile-quantile plots, are a good way of visually comparing two distributions. One common usage in statistics is to assess whether a variable is normally distributed. For example, let's say that we fit a regression model and want to now assess whether the residuals (i.e. the model errors) are normally distributed. Let's first just plot the residuals from the model we fit above. 

```{r empirical-theoretical-distribution, fig.cap="Empirical distribution of residuals, and theoretical distribution."}

df.plot = fit1 %>% 
  augment() %>% 
  clean_names() %>% #handy function that cleans up messy column names
  select(condition, rating, fitted, residual = resid) #I've selected only some of the columns

theoretical.params = as.list(MASS::fitdistr(df.plot$residual, "normal")$estimate) #fit a normal distribution to the residuals 

ggplot(data = df.plot, aes(x = residual))+
  stat_density(geom = "line", aes(color = "green"), size = 1.5)+
  stat_function(fun = "dnorm", args = params, aes(color = "black"), size = 1.5)+
  scale_color_manual(values = c("black", "green"), labels = c("theoretical", "empirical"))+
  theme(legend.title = element_blank(),
        legend.position = c(0.9, 0.9))

```

Here, the empirical distribution of the errors and the theoretical normal distribution with a mean of 0 and a SD of 2 correspond very closely. Let's take a look at the corresponding QQ plot. 


```{r}
ggplot(data = df.plot, aes(sample = residual))+
  geom_abline(intercept = 0, slope = 1, linetype = 2)+
  geom_qq(distribution = "qnorm", dparams = theoretical.params)+
  geom_qq_line(distribution = "qnorm", dparams = theoretical.params)
```

Note that the QQ plot is sensitive to the general shape of the distribution. 

I've used the `geom_qq()` and `geom_qq_line()` functions that are part of ggplot. By default, these functions assume a normal distribution as the theoretical distribution. This plot is just another way of showing the information in Figure \@ref(fig:empirical-theoretical-distribution). Intuitively, a QQ plot is built in the following way: imagine going with your finger from left to right along the x-axis on Figure \@ref(fig:empirical-theoretical-distribution), and then add a point on the QQ plot which captures the cumulative density for each distribution. 

Here are some more examples for what these plots would look like when comparing different theoretical distributions to the same empirical distribution. 

```{r}
# data frame with parameters saved in a list column 
df.parameters = data_frame(
  parameters = list(
    theoretical.params,
    list(mean = -2, sd = 2),
    list(mean = 2, sd = 2),
    list(mean = 0, sd = 5)
  )
)

# list container for plots
l.plots = list()

for (i in 1:nrow(df.parameters)){
  p1 = ggplot(data = df.plot, aes(x = residual))+
    stat_density(geom = "line", color = "green", size = 1.5)+
    stat_function(fun = "dnorm", args = df.parameters$parameters[[i]], color = "black", size = 1.5)
  
  p2 = ggplot(data = df.plot, aes(sample = residual))+
    geom_abline(intercept = 0, slope = 1, linetype = 2)+
    geom_qq(dparams = df.parameters$parameters[[i]])+
    geom_qq_line(dparams = df.parameters$parameters[[i]])
  
  l.plots[[length(l.plots) + 1]] = p1
  l.plots[[length(l.plots) + 1]] = p2
}

# use patchwork for plotting 
l.plots[[1]] + 
l.plots[[2]] +
l.plots[[3]] +
l.plots[[4]] +
l.plots[[5]] +
l.plots[[6]] +
l.plots[[7]] +
l.plots[[8]] +
  plot_layout(ncol = 4, byrow = F) &
  theme(text = element_text(size = 10))
```

The line changes, but it's still a line. So the QQ plot helps us detect what kind of distribution the data follows. 

Now, let's see what happens if distributions don't have the same shape. 

```{r}

#let's generate some "empirical" data from a beta distribution 
set.seed(0)

df.plot = data_frame(
  residual = rbeta(1000, shape1 = 5, shape2 = 5)
)

# data frame with parameters saved in a list column 
df.parameters = data_frame(
  parameters = list(
    list(shape1 = 1, shape2 = 5),
    list(shape1 = 2, shape2 = 5),
    list(shape1 = 5, shape2 = 2),
    list(shape1 = 5, shape2 = 1)
  )
)

# list container for plots
l.plots = list()

for (i in 1:nrow(df.parameters)){
  p1 = ggplot(data = df.plot, aes(x = residual))+
    stat_density(geom = "line", color = "green", size = 1.5)+
    stat_function(fun = "dbeta", args = df.parameters$parameters[[i]], color = "black", size = 1.5)
  
  p2 = ggplot(data = df.plot, aes(sample = residual))+
    geom_abline(intercept = 0, slope = 1, linetype = 2)+
    geom_qq(distribution = "qbeta", dparams = df.parameters$parameters[[i]])
  
  l.plots[[length(l.plots) + 1]] = p1
  l.plots[[length(l.plots) + 1]] = p2
}

# use patchwork for plotting
l.plots[[1]] + 
l.plots[[2]] +
l.plots[[3]] +
l.plots[[4]] +
l.plots[[5]] +
l.plots[[6]] +
l.plots[[7]] +
l.plots[[8]] +
  plot_layout(ncol = 4, byrow = F) &
  theme(text = element_text(size = 10))

```

The qqplot on the 


### Confidence intervals 

[difference between confidence interval and prediction interval](https://www.graphpad.com/support/faq/the-distinction-between-confidence-intervals-prediction-intervals-and-tolerance-intervals/)

> Confidence intervals tell you about how well you have determined the mean. Assume that the data really are randomly sampled from a Gaussian distribution. If you do this many times, and calculate a confidence interval of the mean from each sample, you'd expect about 95 % of those intervals to include  the true value of the population mean. The key point is that the confidence interval tells you about the likely location of the true population parameter.

> Prediction intervals tell you where you can expect to see the next data point sampled. Assume that the data really are randomly sampled from a Gaussian distribution. Collect a sample of data and calculate a prediction interval. Then sample one more value from the population. If you do this many times, you'd expect that next value to lie within that prediction interval in 95% of the samples.The key point is that the prediction interval tells you about the distribution of values, not the uncertainty in determining the population mean.  
Prediction intervals must account for both the uncertainty in knowing the value of the population mean, plus data scatter. So a prediction interval is always wider than a confidence interval.  


How confident are we in the predictions that the model makes? 

```{r}
df.plot = fit4 %>% 
  augment() %>% 
  clean_names()

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point()+
  geom_smooth(method = "lm", level = 0.95, color = "black")+
  geom_segment(aes(xend = intelligence, yend = fitted), alpha = .2)

```

Notice the peculiar shape of the confidence interval. It's narrower in the center and wider on the outsides. 

- confidence interval indicates a range of possible straight lines that describe the relationship between intelligence and score
- use bootstrapping to get this interval 

```{r}
conf_predictors = confint(fit4)

# confidence interval on the predictions
prediction = predict(fit4, interval = "confidence", level = .95) %>% 
  as_data_frame()

df.plot = fit4 %>% 
  augment() %>% 
  clean_names() %>% 
  left_join(prediction %>% rename(fitted = fit), by = "fitted")

ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point(aes(y = fitted), color = "black")+
  geom_linerange(aes(ymin = lwr, ymax = upr))+
  geom_smooth(method = "lm", level = 0.95, color = "black")

```

- show that this is is equivalent to using "mean_cl_normal"

### Bootstrapped confidence intervals 

```{r}
set.seed(0)

# create two bootstrap samples 
df.boot1 = df.data %>% 
    sample_n(size = nrow(.), replace = T) %>% 
    add_count(intelligence)

df.boot2 = df.data %>% 
    sample_n(size = nrow(.), replace = T) %>% 
    add_count(intelligence)

df.boot1 %>% head() %>% print()

```

```{r bootstrap-example, fig.cap="The black lines show the best fitting regression line for the dataset, and the blue lines show the best fitting regression line for the other data set."}

fit.boot1 = lm(score ~ intelligence, data = df.boot1)
fit.boot2 = lm(score ~ intelligence, data = df.boot2)
  

p1 = ggplot(data = df.boot1, aes(x = intelligence, y = score))+
  geom_abline(intercept = coef(fit.boot2)[1],
              slope = coef(fit.boot2)[2],
              color = "blue")+
  geom_point(aes(color = n))+
  geom_smooth(method = "lm", se = F, color = "black")+
  labs(title = "Bootstrap 1")+
  scale_color_gradient(low = "gray60", high = "gray0")

p2 = ggplot(data = df.boot2, aes(x = intelligence, y = score))+
  geom_abline(intercept = coef(fit.boot1)[1],
              slope = coef(fit.boot1)[2],
              color = "blue")+
  geom_point(aes(color = n))+
  geom_smooth(method = "lm", se = F, color = "black")+
  labs(title = "Bootstrap 2")+
  scale_color_gradient(low = "gray60", high = "gray0")

p1 + p2 &
  theme(legend.position = "none")

```

Notice how the data points are different for each bootstrap. And notice how some points are darker than others. The darkness indicates how often a particular data point has been sampled.

Now let's do this many times and save the coefficients. 

```{r}

# set number of boostrapped samples
n_bootstraps = 100 

# set up data frame to store coefficients 
df.coefficients = data_frame(
  sample = 1:n_bootstraps, 
  intercept = NA,
  slope = NA
)

# estimate parameters for each bootstrapped sample 
for (i in 1:n_bootstraps){
  df.boot = df.data %>% 
    sample_n(size = nrow(.), replace = T) %>% 
    add_count(intelligence) #column indicating how often a particular value was sampled
  
  fit = lm(score ~ intelligence, data = df.boot)
  df.coefficients$intercept[i] = coef(fit)[1]
  df.coefficients$slope[i] = coef(fit)[2]
}
  
df.coefficients %>% head() %>% print

```

And then plot the regression lines from our bootstrapped samples on top of the fitted model. 

```{r}
# scatter plot with linear regression 
p = ggplot(data = df.plot, aes(x = intelligence, y = score))+
  geom_point(color = "black")+
  geom_smooth(method = "lm", level = 0.95, color = "black")

# add bootstrapped regression lines
for (i in 1:nrow(df.coefficients)){
  p = p + 
    geom_abline(intercept = df.coefficients$intercept[i],
                slope = df.coefficients$slope[i],
                alpha = 0.1,
                color = "blue")
}
p

```

- show that this is equivalent to using `mean_cl_boot()`

You can also make use of the `boot` package. This package makes it easy to get estimates and confidence intervals of choice from bootstrapped samples. 

```{r}
tic() #start timer 

# function to calculate desired statistic 
func_boot_mean = function(data, variable, indices){
  d = data[[variable]][indices]
  out = mean(d)
  return(out)
}

boot.mean = boot(data = df.data,
     statistic = func_boot_mean,
     variable = "intelligence",
     R = 100)

boot.mean %>% boot.ci(type = "perc") %>% .$perc %>% .[4:5]
toc() #stop timer 

# the same result using a for loop 

tic() #start timer
result = numeric(100)
for (i in 1:100){
  result[i] = df.data %>% 
    sample_n(size = nrow(.), replace = T) %>% 
    pull(intelligence) %>% 
    mean()
}
quantile(result, prob = c(0.05, 0.95))
toc() #stop timer

```


```{r}
# using the boostrap package 

fun_boot = function(formula, data, indices){
  d = data[indices, ]
  fit = lm(formula, data = d)
  out = coef(fit)
  return(out)
}

results = boot(data = df.data, 
               statistic = fun_boot,
               formula = score ~ intelligence,
               R = 1000)

quantile(results$t[,1], prob = c(.05, .95))
quantile(results$t[,2], prob = c(.05, .95))

results %>% boot.ci(type = "perc", index = 1)
results %>% boot.ci(type = "bca", index = 2)
```

### Prediction intervals 

```{r}
# predict(fit, interval = "prediction", level = 0.95)
# predict(fit8, interval = "confidence", level = 0.95)

# fit8 %>% summary()

fit8 %>% 
  tidy() %>% 
  clean_names() %>% 
  left_join(confint(fit8, level = 0.95) %>% 
              as.data.frame() %>% 
              rownames_to_column("term"),
            by = "term") %>% 
  mutate(cf_low = estimate + std_error * qt(0.025, df = nobs(fit8)-1),
         cf_high = estimate + std_error * qt(0.975, df = nobs(fit8)-1),
         stat = estimate / std_error,
         p = dt(stat, df = nobs(fit8))*2
         )

```




## One continuous and one discrete predictor 

- with multiple predictors: 
  - explain one predictor holding all the others constant 
  
$$
y = \alpha + \beta_1 \cdot condition + \beta_2 \cdot x + \epsilon
$$



```{r}
set.seed(0)

alpha = 10 
beta1 = 3
beta2 = 0.4

n_participants = 100

df.data = data_frame(
  participant = 1:n_participants,
  condition = rep(0:1, each = n_participants/2),
  x = runif(n = n_participants, min = 0, max = 20),
  error = rnorm(n = n_participants, sd = 1),
  response = alpha + beta1 * condition + beta2 * x + error
) %>% 
  mutate(condition = factor(condition, levels = 0:1, labels = 1:2))

df.plot = df.data

ggplot(data = df.plot, aes(x = x, y = response, color = condition))+
  geom_point(size = 2, show.legend = F)
```

```{r}
fit.categorical = lm(response ~ condition, data = df.data)
fit.categorical %>% summary()

ggplot(data = df.data, aes(x = x, y = response, color = condition))+
  geom_point(size = 2)+
  geom_hline(yintercept = coef(fit.categorical)[1], color = "#F8766D", size = 1.5)+
  geom_hline(yintercept = coef(fit.categorical)[1] + coef(fit.categorical)[2], color = "#00BFC4", size = 1.5)

```

```{r}
fit.continuous = lm(response ~ x, data = df.data)
fit.continuous %>% summary()

ggplot(data = df.data, aes(x = x, y = response, color = condition))+
  geom_point(size = 2)+
  geom_smooth(method = "lm", se = F, aes(group = 1), color = "black")
  # geom_abline(intercept = coef(fit.continuous)[1], slope = coef(fit.continuous)[2], color = "red", size = 1.5)
  
```


```{r}
fit.combined = lm(response ~ x + condition, data = df.data)

ggplot(data = df.data, aes(x = x, y = response, color = condition, group = condition))+
  geom_point(size = 2)+
  geom_smooth(method = "lm", se = F)
  
```

```{r}
fit.combined %>% summary()
```

- explain how the linear model expresses the different results

### Interactions 

- be extra careful when interpreting regressions with interactions [see](https://pagepiccinini.com/2016/01/08/introduction-and-linear-models-part-1/)

Let's first create a dataset in which a binary predictor interacts with a continuous one. 

$$
y = \alpha + \beta_1 \cdot condition + \beta_2 \cdot x + beta_3 \cdot condition \cdot x + \epsilon
$$

```{r}
set.seed(0)

alpha = 10 
beta1 = 2
beta2 = 0.4
beta3 = 0.5

n_participants = 100

df.data.interaction = data_frame(
  participant = 1:n_participants,
  condition = rep(0:1, each = n_participants/2),
  x = runif(n = n_participants, min = 0, max = 20),
  error = rnorm(n = n_participants, sd = 1),
  response = alpha + beta1 * condition + beta2 * x + beta3 * condition * x + error
) %>% 
  mutate(condition = factor(condition, levels = 0:1, labels = 1:2))

df.plot = df.data.interaction

ggplot(data = df.plot, aes(x = x, y = response, color = condition))+
  geom_point(size = 2, show.legend = F)
```

```{r}
fit6 = lm(response ~ condition + x, data = df.data.interaction)
summary(fit6)

```

```{r}
df.plot = df.data.interaction
df.fit = fit6 %>% 
  tidy()

ggplot(data = df.plot, aes(x = x, y = response, color = condition))+
  geom_point(size = 2, show.legend = F)+
  geom_abline(intercept = df.fit$estimate[1], slope = df.fit$estimate[3], color = "#F8766D")+
  geom_abline(intercept = df.fit$estimate[1] + df.fit$estimate[2], slope = df.fit$estimate[3], color = "#00BFC4")
```

Let's take a look at the residuals.

```{r}
df.plot = fit6 %>% 
  augment() %>% 
  clean_names()
  
ggplot(data = df.plot, aes(x = fitted, y = resid))+
  geom_hline(yintercept = 0)+
  geom_point()+
  geom_smooth(method = "loess")
```

We can see from the residuals that we're missing something important in the data.^[For more information about the "loess" kernel, see @cleveland1979robust.]

Let's fit a model with an interaction and visualize the model.

```{r}
fit7 = lm(response ~ 1 + condition * x, data = df.data.interaction)

df.fit = fit7 %>% 
  tidy()

df.plot = df.data.interaction

ggplot(data = df.plot, aes(x = x, y = response, color = condition))+
  geom_point(size = 2, show.legend = F)+
  geom_abline(intercept = df.fit$estimate[1], slope = df.fit$estimate[3] , color = "#F8766D")+
  geom_abline(intercept = df.fit$estimate[1] + df.fit$estimate[2], slope = df.fit$estimate[3] + df.fit$estimate[4], color = "#00BFC4")
```

Let's take a look at the residuals. 

```{r}
df.plot = fit7 %>% 
  augment() %>% 
  clean_names()
  
ggplot(data = df.plot, aes(x = fitted, y = resid))+
  geom_hline(yintercept = 0)+
  geom_point()+
  geom_smooth(method = "loess")
```

That looks better! Of course, this is not particularly surprising since we generated the data assuming an interaction. Let's see how well the regression recovers the parameters that we've used to generated the data. 


```{r regression-recovery}
fit7 = lm(response ~ 1 + condition * x, data = df.data.interaction)

df.fit = fit7 %>% 
  tidy() %>%
  left_join( # add confidence intervals 
    confint(fit7) %>% 
      as.data.frame() %>% 
      rownames_to_column("term"),
    by = "term"
  ) %>% 
  mutate(truth = c(alpha, beta1, beta2, beta3)) %>% 
  mutate(parameter = c("alpha", "beta1", "beta2", "beta3")) %>% 
  select(term, parameter, truth, estimate, contains("%"))

kable(df.fit, digits = 2, caption = "Ground truth and inferred parameters values.")
```

As Table \@ref(tab:regression-recovery) shows, we were able to recover the ground truth parameters fairly accurately. 


Note that the following two ways are equivalent for specifying the model: 

```{r}
fit.1 = lm(response ~ 1 + condition * x, data = df.data.interaction)
fit.1 %>% print()

fit.2 = lm(response ~ 1 + condition + x + condition:x, data = df.data.interaction)
fit.2 %>% print()
  
# let's add the interaction as a predictor to the data frame
df.tmp = df.data.interaction %>% 
  mutate(condition = condition %>% as.character() %>% as.numeric() - 1,
    interaction = x * condition)

fit.3 = lm(response ~ 1 + condition + x + interaction, df.tmp)
fit.3 %>% print()
```


## Interpretation of coefficiencts 

p. 34 in Gelman book: 

- *predictive interpretation*: 
  - how does the outcome variable differ (on average), when comparing two groups who differ on the predictive variable 
- *counterfactual interpretation*: 
  - how would an individual differ if something had been different --> causal interpretation; changing this variable would result in such a change


## Crossvalidation 

- [tutorial on cross validation](http://ijlyttle.github.io/model_cv_selection.html)
- [another tutorial](http://sjspielman.org/bio5312_fall2017/files/kfold_supplement.pdf)

We start by generating the data. 

```{r}
set.seed(0)

# parameters 
alpha = 1 
beta1 = 2 
beta2 = -1
n_participants = 100

# data frame 
df.data = data_frame(
  participant = 1:n_participants,
  x = runif(n = n_participants, min = 0, max = 1),
  error = rnorm(n = n_participants, sd = 0.1),
  y = alpha + beta1 * x + beta2 * x^2 + error
)

# print  
df.data %>% head()

```

The true generating function is a quadratic function $y = 1 + 2 \cdot x - x^2$. Let's visualize the data. 

```{r}
df.plot = df.data 

ggplot(df.plot, aes(x = x, y = y)) +
  stat_function(fun = function(x){alpha + beta1 * x + beta2 * x^2}, color = "black", alpha = 0.7, linetype = "dashed") +
  geom_point(alpha = 0.6)
```

Here, I've plotted the data together with the true generating function shown as a dotted line. Now let's first fit a simple linear model. 

```{r}
fit8 = lm(y ~ 1 + x, data = df.data)
fit8 %>% summary()
```

The summary looks pretty good. The simple linear model accounts for `r summary(fit8)$r.squared %>% round(2)`% of the variance. But that's not the only criterion we should consider. Let's take a look at how well the model captures the data. 

```{r}
# data and model predictions 
df.plot = fit8 %>% 
  augment() %>% 
  clean_names() %>% 
  cbind(predict(fit8, interval = "confidence") %>% 
          as_data_frame() %>% 
          select(cf_low = lwr, cf_high = upr)) %>% # add confidence intervals
  cbind(predict(fit8, interval = "prediction") %>% 
          as_data_frame() %>% 
          select(pred_low = lwr, pred_high = upr)) # add prediction intervals

df.fit = fit8 %>% 
  tidy() %>% 
  clean_names()

ggplot(df.plot, aes(x = x, y = y))+
  geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha = 0.1)+ # prediction interval
  geom_ribbon(aes(ymin = cf_low, ymax = cf_high), alpha = 0.2)+ # confidence interval 
  stat_function(fun = function(x){df.fit$estimate[1] + df.fit$estimate[2] * x},
                geom = "line")+
  geom_point()
```

This looks pretty good. Notice however, how the model overpredicts values at the lower and upper range, and underpredicts in the middle range. To see this better, let's take a look at the residuals.

```{r}
df.plot = fit8 %>% 
  augment() %>% 
  clean_names()

ggplot(data = df.plot, aes(x = fitted, y = resid))+
  geom_hline(yintercept = 0)+
  geom_point()+
  geom_smooth(method = "loess")
```

This is the signature for a residual plot when the model misses a quadratic relationship. The residulas aren't just normally distributed around 0 (which would be true for a model that fits the data well), but residuals are larger for low and high values. Of course, this is not surprising given that we know the true data generating model.  

Let's now fit a model that includes a quadratic predictor.

```{r}
df.data = df.data %>% 
  mutate(x_squared = x^2)

fit9 = lm(y ~ 1 + x + x_squared, data = df.data)
fit9 %>% summary()
```

The quadratic model accounts for `r summary(fit9)$r.squared %>% round(2)`% of the variance. Let's take a look at the model: 

```{r}
# data and model predictions 
df.plot = fit9 %>% 
  augment() %>% 
  clean_names() %>% 
  cbind(predict(fit9, interval = "confidence") %>% 
          as_data_frame() %>% 
          select(cf_low = lwr, cf_high = upr)) %>% # add confidence intervals
  cbind(predict(fit9, interval = "prediction") %>% 
          as_data_frame() %>% 
          select(pred_low = lwr, pred_high = upr)) # add prediction intervals

df.fit = fit9 %>% 
  tidy() %>% 
  clean_names()

ggplot(df.plot, aes(x = x, y = y))+
  geom_ribbon(aes(ymin = pred_low, ymax = pred_high), alpha = 0.1)+ # prediction interval
  geom_ribbon(aes(ymin = cf_low, ymax = cf_high), alpha = 0.2)+ # confidence interval 
  stat_function(fun = function(x){df.fit$estimate[1] + df.fit$estimate[2] * x + df.fit$estimate[3] * x^2},
                geom = "line")+
  geom_point()
```

Let's take a look at the residuals. 

```{r}
df.plot = fit9 %>% 
  augment() %>% 
  clean_names()

ggplot(data = df.plot, aes(x = fitted, y = resid))+
  geom_hline(yintercept = 0)+
  geom_point()+
  geom_smooth(method = "loess")
```

The residuals look much better here. 

Which model provides a better explanation of the data? Which model is more likely to yield correct predictions for new data that is generated from the ground truth?  

So far, we have seen how we can use model selection criteria (AIC, BIC), and a likelihood test 

```{r}
# Likelihood test 
anova(fit9, fit8)
```

The likelihood test is significant. The residual sum of squares is significantly lower for the more complex model. Would a more complex model fit even better? Let's fit a model that includes a third order polynomial. 

```{r}
fit10  = lm(y ~ x + I(x^2) + I(x^3), data = df.data)
fit10 %>% summary()
```

Notice how now, neither the quadratic nor the cubic predictor are significant. Let's plot the model's predictions. 

```{r}
df.plot = df.data
  
df.fit = fit10 %>% 
  tidy() %>% 
  clean_names()

ggplot(data = df.plot, aes(x = x, y = y))+
  geom_point()+
  stat_function(fun = function(x){df.fit$estimate[1] + 
                  df.fit$estimate[2] * x +
                  df.fit$estimate[3] * x^2 +
                  df.fit$estimate[4] * x^3},
                geom = "line"
                  )
```

And compare the cubic model to the best-fitting model so far. 

```{r}
# Likelihood test 
anova(fit10, fit9)
```

TODO: 
- explain the `I()` function formula
- `^` has special meaning within a formula; see `help(formula)`

Good -- it doesn't explain more of the variance. Remember that we can only use the likelihood test if the models are nested. That is, if the more complex model "includes" the simpler one. 

```{r}
# Selection criteria 
fit8 %>% 
  glance() %>% 
  rbind(
    fit9 %>% 
      glance()) %>% 
  rbind(
    fit10 %>% 
      glance()) %>% 
  mutate(model = c("linear", "quadratic", "cubic")) %>% 
  clean_names() %>% 
  select(model, r_squared, log_lik, aic, bic, deviance) %>% 
  kable(digits = 2, caption = "Model selection criteria.")
```

So far, we have a pretty clear winner. The quadratic model best trades of model fit and complexity. Let's see whether it also generalizes best.  

To do so, we will use cross-validation. In cross-validation, we first split the dataset into multiple pairs of training and test sets. We then fit the model to the training set and evaluate how well it does on the held-out test set. 

We will use functions in the `modelr` package to help us with this task. 

```{r}
df.split = df.data %>% 
  crossv_mc(n = 50, test = 0.5) %>% 
  print()
```

I used the `crossv_mc()` function to create n = 50 separate trainin-test data sets. By setting `test = 0.5`, we ensure that the dataset is split approximately in half. 

Let's take a look at one of the splits. 

```{r}
df.split[1,] %>% str()
```

Both the `train` and `test` column contain the data, but they differ in the index `idx`. Also, the training set contains n = `r length(df.split$train[[1]]$idx)` observations, whereas the test set contains n = `r length(df.split$test[[1]]$idx)`observations. 

Now, let's fit our three models to all the different training sets. To do so, let's first write a simple function for each of our models. 

```{r}
fun.linear = function(df){
  lm(y ~ x, data = df)
}

fun.quadratic = function(df){
  lm(y ~ x + I(x^2), data = df)
}

fun.cubic = function(df){
  lm(y ~ x + I(x^2) + I(x^3), data = df)
}

fun.quartic = function(df){
  lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = df)
}

# an alternative way of specifying polynomial models
# fun.quartic = function(df){
#   lm(y ~ poly(x, degree = 4, raw = T), data = df)
# }

# see this post for more information: https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do

```

Now, let's train each of the models on the training sets. 

```{r}
df.split = df.split %>% 
  mutate(fit_linear = map(train, fun.linear),
         fit_quadratic = map(train, fun.quadratic),
         fit_cubic = map(train, fun.cubic),
         fit_quartic = map(train, fun.quartic)
         )
```

And calculate the root mean squared error on each training set. 

```{r}
df.split = df.split %>% 
  mutate(
    rmse_training_linear = map2(fit_linear, train, rmse),
    rmse_training_quadratic = map2(fit_quadratic, train, rmse),
    rmse_training_cubic = map2(fit_cubic, train, rmse),
    rmse_training_quartic = map2(fit_quartic, train, rmse)
    )
```

Let's compare the models in terms of their root mean squared error on the training set.  

```{r}
df.plot = df.split %>% 
  select(contains("rmse_training")) %>% 
  unnest() %>% 
  gather(key = "model", value = "error") %>% 
  mutate(model = factor(model, levels = str_c("rmse_training_",c("linear", "quadratic", "cubic", "quartic")),
                        labels = c("linear", "quadratic", "cubic", "quartic")))

ggplot(data = df.plot, aes(x = model, y = error))+
  geom_point(position = position_jitter(width = 0.1, height = 0), alpha = 0.2)+
  stat_summary(fun.data = "mean_cl_boot", geom = "linerange")+
  stat_summary(fun.y = "mean", geom = "line", aes(group = 1))+
  stat_summary(fun.y = "mean", geom = "point", shape = 21, color = "black", fill = "red", size = 3)+
  labs(title = "Error on training set")
```

Now let's see how the models perform on the held out test set. 

```{r}
df.split = df.split %>% 
  mutate(
    rmse_test_linear = map2(fit_linear, test, rmse),
    rmse_test_quadratic = map2(fit_quadratic, test, rmse),
    rmse_test_cubic = map2(fit_cubic, test, rmse),
    rmse_test_quartic = map2(fit_quartic, test, rmse)
    )
```

```{r}
df.plot = df.split %>% 
  select(contains("rmse_test")) %>% 
  unnest() %>% 
  gather(key = "model", value = "error") %>% 
  mutate(model = factor(model, levels = str_c("rmse_test_",c("linear", "quadratic", "cubic", "quartic")),
                        labels = c("linear", "quadratic", "cubic", "quartic")))

ggplot(data = df.plot, aes(x = model, y = error))+
  geom_point(position = position_jitter(width = 0.1, height = 0), alpha = 0.2)+
  stat_summary(fun.data = "mean_cl_boot", geom = "linerange")+
  stat_summary(fun.y = "mean", geom = "line", aes(group = 1))+
  stat_summary(fun.y = "mean", geom = "point", shape = 21, color = "black", fill = "red", size = 3)+
  labs(title = "Error on test set")
```

```{r}
df.split %>% 
  select(contains("rmse_test")) %>% 
  unnest() %>% 
  gather("model", "value") %>% 
  mutate(model = factor(model, levels = str_c("rmse_test_",c("linear", "quadratic", "cubic", "quartic")),
                        labels = c("linear", "quadratic", "cubic", "quartic"))) %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(value),
            sd_rmse = sd(value)) %>% 
  mutate_at(vars(contains("rmse")), funs(round(., 3))) %>% 
  kable()
```

The quadratic model has the lowest error on the test data. It's simpler than the cubic and quartic models and also more robust. 

### Visualize model predictions 

The cross-validation results show that the quadratic model results in the smallest average root mean squared error. To get a better sense for why this is happening, let's visualize the model predictions. 

Let's first generate a data frame that contains the parameter estimates for each model and training set. 

```{r}

df.estimates = df.split %>% 
  select(contains("fit"), train, test, .id) %>% 
  mutate(
    coef_linear = map(fit_linear, tidy),
    coef_quadratic = map(fit_quadratic, tidy),
    coef_cubic = map(fit_cubic, tidy),
    coef_quartic = map(fit_quartic, tidy)
    ) %>% 
  select(contains("coef")) %>% 
  gather("model", "value") %>% 
  unnest(.id = "id") %>% 
  clean_names() %>% 
  select(model, id, term, estimate, std_error)

df.estimates %>% 
  filter(row_number() <= 5 | row_number() >= (nrow(.)-5)) %>% 
  kable()
```

The `df.estimates` data frame contains the parameter estimates from each model for the 50 split-half crossvalidation runs. Let's visualize what these model predictions look like. 

```{r}

function_types = c("linear", "quadratic", "cubic", "quartic")

# list with functions 
l.functions = list()
l.functions$linear = function(x, parameters){parameters[1] + parameters[2] * x}
l.functions$quadratic = function(x, parameters){parameters[1] + parameters[2] * x + parameters[3] * x^2}
l.functions$cubic = function(x, parameters){parameters[1] + parameters[2] * x + parameters[3] * x^2 + parameters[4] * x^3}
l.functions$quartic = function(x, parameters){parameters[1] + parameters[2] * x + parameters[3] * x^2 + parameters[4] * x^3 + parameters[5] * x^4}

# list container for plots
l.plots = list()

df.plot = df.data

# iterating over plots 
for (i in 1:length(function_types)){
  
  function_type = function_types[i]
  
  df.fit = df.estimates %>% 
  filter(str_detect(string = model, pattern = function_type))
    
  # data plot 
  p = ggplot(df.plot, aes(x = x, y = y)) +
    stat_function(fun = function(x){alpha + beta1 * x + beta2 * x^2}, color = "black", alpha = 0.7, linetype = "dashed") +
    geom_point(alpha = 0.6)
  
  # add predictions
  for(j in df.fit$id %>% unique()){
    
    estimates = df.fit %>% 
      filter(id == j) %>% 
      select(estimate) %>% 
      unlist()
    
    p = p + 
      stat_function(fun = l.functions[[function_type]], args = list(parameters = estimates), alpha = 0.1)
  }
  
  l.plots[[i]] = p + 
    labs(title = str_c(function_type, " functions"))
}

l.plots[[1]] + 
  l.plots[[2]] + 
  l.plots[[3]] + 
  l.plots[[4]] + 
  plot_layout(ncol = 2) & 
  theme(plot.tag = element_text(face = "bold", size = 20))
```


- Reading: assign some Gigerenzer paper on prediction vs. fitting

## Parameter estimation via bootstrap 

- [tutorial on bootstrap](http://ijlyttle.github.io/model_bootstrap_parameter.html)

The crossvalidation revealed that the quadratic model provided the best account of the data in that it minimized the root mean squared error on the test sets. 

Let's first summarize the model again here. 

```{r}
fit.quadratic = lm(y ~ 1 + x + I(x^2), data = df.data)

fit.quadratic %>% 
  tidy() %>% 
  clean_names() %>% 
  left_join(confint(fit.quadratic, level = 0.95) %>% 
              as.data.frame() %>% 
              rownames_to_column("term"),
            by = "term") %>% 
  select(term:std_error, contains("%")) %>% 
  # mutate(cf_low = estimate + std_error * qt(0.025, df = nrow(df.data) - 3),
  #   cf_high = estimate + std_error * qt(0.975, df = nrow(df.data) - 3))
  kable(digits = 2)
```

Here, for the confidence intervals, we assume a t-distribution with df = n - 3 (taking into account the three parameters that we fit). An alternative way of getting confidence intervals that does not rely on parametric assumptions about is via bootstrap. 

First, we generate a data frame with bootstrapped samples. Remember that for bootstrapping, we draw samples with replacement. Again, the `modelr` package makes this easy. 

```{r}
df.bootstrap = df.data %>%
  bootstrap(100) %>% 
  print()
```

Now, let's apply the model to each bootstrapped sample. 

```{r}

# just to reiterate
# fun.quadratic = function(df){
#   lm(y ~ x + I(x^2), data = df)
# }

df.bootstrap = df.bootstrap %>%
  mutate(fit = map(strap, fun.quadratic))

```

Now, let's summarize the model fits. 

```{r}
df.estimates = df.bootstrap %>% 
  mutate(parameters = map(fit, tidy)) %>% # extract coefficients
  select(parameters) %>% 
  unnest(.id = "id") %>% 
  clean_names() %>% 
  mutate(term = factor(term, levels = c("(Intercept)", "x", "I(x^2)")))
```

And let's visualize the parameters. 

```{r}
ggplot(data = df.estimates, aes(x = estimate))+
  stat_density(geom = "line", position = "identity")+
  facet_wrap(~term, ncol = 1)
```

Let's see what the summary statistics of our parameters look like based on our bootstrapped procedure. 

```{r}
df.estimates %>% 
  group_by(term) %>% 
  summarize(mean = mean(estimate),
            ci_low = quantile(estimate, probs = 0.025),
            ci_high = quantile(estimate, probs = 0.975)
            ) %>% 
  kable()
```

In this case, the results are comparable to what we found above. In this case, all of the confidence intervals contain the true values that were used to generate the data. 

Let's visualize the confidence intervals on top of our original data set. 

```{r}
df.plot = df.data

df.predictions = df.bootstrap %>% 
  select(fit) %>% 
  transmute(predictions = map(fit, augment)) %>% 
  unnest(.id = "id") %>% 
  clean_names()

ggplot(df.plot, aes(x = x, y = y)) +
  stat_function(fun = function(x){alpha + beta1 * x + beta2 * x^2}, color = "black", alpha = 0.7, linetype = "dashed") +
  geom_point(alpha = 0.6)+
  geom_line(data = df.predictions %>% 
              select(-y) %>% 
              rename(y = fitted),
            aes(group = id),
            color = "blue",
            alpha = 0.05)
```


TODO: 
- come up with an example for which boostrapped and parametric confidence intervals yield quite results
- show how to do the same computation with the `boot` package

## Bootstrap vs. crossvalidation? 

- [crossvalidated blog post](https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio))
- [tutorial](https://www.burns-stat.com/documents/tutorials/the-statistical-bootstrap-and-other-resampling-methods-2/)


- __cross-validation__:
  - model selection 

- __bootstrap__:
  - confidence in estimates  

## Power analysis 

- [power analysis with dplyr](http://www.imachordata.com/linear-model-power-analysis-with-dplyr/)

```{r}
df.seals = read_csv("data/ShrinkingSeals.csv")

ggplot(df.seals, aes(x=age, y=length)) +
  geom_point()

fit.seals = lm(formula = length ~ age,
               data = df.seals)
```


```{r}
# get parameters 

df.parameters = fit.seals %>% 
  tidy() %>% 
  select(term, estimate) %>% 
  bind_rows(
    fit.seals %>% 
    glance() %>% 
    select(estimate = sigma) %>% 
    mutate(term = "sigma")
  ) %>% 
  spread(term, estimate) %>% 
  clean_names() %>% 
  rename(slope = age)

df.simulation = df.parameters %>% 
  crossing(n = 10:100) %>% 
  group_by(intercept, slope, sigma, n) %>% 
  expand(reps = 1:n) %>% 
  ungroup() %>% 
  crossing(sim = 1:100) %>% 
  mutate(age = runif(n(), min = 1000, max = 8500),
         length = rnorm(n(), mean = intercept + slope * age, sd = sigma))

df.models = df.simulation %>% 
  group_by(intercept, slope, sigma, n, sim) %>% 
  nest() %>% 
  mutate(mod = map(data, ~lm(length ~ age, data = .)),
         coefs = map(mod, ~tidy(.)))

df.coefficients = df.models %>% 
  unnest(coefs, .id = "id") %>% 
  ungroup() %>% 
  filter(term == "age")

df.power = df.coefficients %>% 
  group_by(n) %>% 
  summarise(power = 1-sum(p.value > .05)/n()) %>% 
  ungroup()

ggplot(data = df.power, aes(x = n, y = power))+
  geom_line()+
  geom_point()
```

```{r}
# get parameters 

df.parameters = fit.seals %>% 
  tidy() %>% 
  select(term, estimate) %>% 
  bind_rows(
    fit.seals %>% 
    glance() %>% 
    select(estimate = sigma) %>% 
    mutate(term = "sigma")
  ) %>% 
  spread(term, estimate) %>% 
  clean_names() %>% 
  rename(slope = age)

df.simulation = df.parameters %>% 
  select(-sigma) %>% 
  crossing(n = seq(10, 100, 10)) %>% 
  group_by(intercept, slope, n) %>% 
  expand(reps = 1:n) %>% 
  ungroup() %>% 
  crossing(sim = 1:100,
           sigma = seq(1, 10, 1)) %>% 
  crossing() %>% 
  mutate(age = runif(n(), min = 1000, max = 8500),
         length = rnorm(n(), mean = intercept + slope * age, sd = sigma))

df.models = df.simulation %>% 
  group_by(intercept, slope, sigma, n, sim) %>% 
  nest() %>% 
  mutate(mod = map(data, ~lm(length ~ age, data = .)),
         coefs = map(mod, ~tidy(.)))

df.coefficients = df.models %>% 
  unnest(coefs, .id = "id") %>% 
  ungroup() %>% 
  filter(term == "age")

df.power = df.coefficients %>% 
  group_by(n, sigma) %>% 
  summarise(power = 1 - sum(p.value > .05) / n()) %>% 
  ungroup() %>% 
  mutate(sigma = as.factor(sigma))

ggplot(data = df.power, aes(x = n, y = power, color = sigma))+
  geom_line()+
  geom_point()
```


```{r}
print(sessionInfo(), locale = FALSE)
```


